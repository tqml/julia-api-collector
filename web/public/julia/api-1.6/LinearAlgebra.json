{
  "julia": "1.6.7",
  "methods": [
    {
      "name": "/",
      "arg_names": ["A", "D"],
      "arg_types": [
        "Union{LinearAlgebra.AbstractTriangular, StridedMatrix{T} where T}",
        "Diagonal"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["D", "x"],
      "arg_types": ["Diagonal", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["Da", "Db"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["x", "v"],
      "arg_types": ["Number", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["Bidiagonal", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["Hermitian", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["LowerTriangular", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["LowerTriangular", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["LowerTriangular", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["Symmetric", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["SymTridiagonal", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["J1", "J2"],
      "arg_types": ["UniformScaling", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["J", "A"],
      "arg_types": ["UniformScaling", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["J", "x"],
      "arg_types": ["UniformScaling", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UnitUpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UnitUpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["trA", "F"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVector{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "adjF"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["trA", "F"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UnitLowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "UnitLowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{UnitLowerTriangular, UnitUpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Factorization{var\"#s812\"} where var\"#s812\"<:Real)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{LowerTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}",
        "Factorization{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{LowerTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{UnitLowerTriangular, UnitUpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Factorization{var\"#s812\"} where var\"#s812\"<:Real)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["adjA", "F"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVector{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["adjB", "adjF"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["u", "A"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["adjA", "F"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": ["AbstractMatrix{T} where T", "Factorization"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Factorization{var\"#s812\"} where var\"#s812\"<:Real)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "F"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "F"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["B", "adjF"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "J"],
      "arg_types": ["AbstractMatrix{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["v", "J"],
      "arg_types": ["AbstractVector{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["UnitUpperTriangular", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitUpperTriangular", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitUpperTriangular", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["UpperTriangular", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UpperTriangular", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UpperTriangular", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "x"],
      "arg_types": ["UnitLowerTriangular", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitLowerTriangular", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{LowerTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{LowerTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitLowerTriangular", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{UnitLowerTriangular, UnitUpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{UnitLowerTriangular, UnitUpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "/",
      "arg_names": ["A", "B"],
      "arg_types": ["Tridiagonal", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "BIn"],
      "arg_types": [
        "Union{QR{T, S} where S<:AbstractMatrix{T}, LinearAlgebra.QRCompactWY{T, M} where M<:AbstractMatrix{T}, QRPivoted{T, S} where S<:AbstractMatrix{T}}",
        "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{QR, LinearAlgebra.QRCompactWY, QRPivoted}",
        "Diagonal"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{QR{TA, S} where S<:AbstractMatrix{TA}, LinearAlgebra.QRCompactWY{TA, M} where M<:AbstractMatrix{TA}, QRPivoted{TA, S} where S<:AbstractMatrix{TA}}",
        "AbstractVecOrMat{TB}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["Da", "Db"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["D", "A"],
      "arg_types": ["Diagonal", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["D", "b"],
      "arg_types": ["Diagonal", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["x", "A"],
      "arg_types": ["Number", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["x", "A"],
      "arg_types": ["Number", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["x", "A"],
      "arg_types": ["Number", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["x", "A"],
      "arg_types": ["Number", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["x", "J"],
      "arg_types": ["Number", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["LowerTriangular", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["LowerTriangular", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "D"],
      "arg_types": ["Factorization", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "J"],
      "arg_types": ["Factorization", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Bidiagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}",
        "AbstractVecOrMat{var\"#s813\"} where var\"#s813\"<:Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["Bidiagonal", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["T", "B"],
      "arg_types": ["SymTridiagonal", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["J1", "J2"],
      "arg_types": ["UniformScaling", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["J", "A"],
      "arg_types": ["UniformScaling", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where var\"#s814\"<:(LU{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T))",
        "Transpose{T, var\"#s812\"} where var\"#s812\"<:StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Number, var\"#s813\"<:(Bidiagonal{var\"#s812\", V} where {var\"#s812\"<:Number, V<:AbstractVector{var\"#s812\"}})}",
        "AbstractVecOrMat{var\"#s811\"} where var\"#s811\"<:Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Factorization{var\"#s812\"} where var\"#s812\"<:Real)}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Bidiagonal}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "B"],
      "arg_types": [
        "LQ{T, S} where S<:AbstractMatrix{T}",
        "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LQ{TA, S} where S<:AbstractMatrix{TA}",
        "StridedVecOrMat{TB}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "B"],
      "arg_types": [
        "Factorization{T}",
        "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["F", "B"],
      "arg_types": ["Factorization", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitUpperTriangular", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitUpperTriangular", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "J"],
      "arg_types": ["AbstractMatrix{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:(LU{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T))",
        "Adjoint{T, var\"#s812\"} where var\"#s812\"<:StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Tridiagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["u", "v"],
      "arg_types": [
        "Union{Adjoint{T, var\"#s814\"}, Transpose{T, var\"#s814\"}} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{Adjoint{T, var\"#s814\"}, Transpose{T, var\"#s814\"}} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["adjF", "D"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}",
        "Diagonal"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Number, var\"#s813\"<:(Bidiagonal{var\"#s812\", V} where {var\"#s812\"<:Number, V<:AbstractVector{var\"#s812\"}})}",
        "AbstractVecOrMat{var\"#s811\"} where var\"#s811\"<:Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Bidiagonal}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["adjF", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Factorization}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UpperTriangular", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UpperTriangular", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{LowerTriangular, UpperTriangular}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{LowerTriangular, UpperTriangular}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitLowerTriangular", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitLowerTriangular", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{UnitLowerTriangular, UnitUpperTriangular}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{UnitLowerTriangular, UnitUpperTriangular}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
    },
    {
      "name": "\\",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVector{T} where T", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["F"],
      "arg_types": ["Hessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": [
        "Symmetric{var\"#s814\", S} where {var\"#s814\"<:Real, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["B"],
      "arg_types": [
        "Bidiagonal{var\"#s814\", V} where {var\"#s814\"<:Real, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["B"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["S"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Real, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["S"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["B"],
      "arg_types": ["Union{BitMatrix, BitVector}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["F"],
      "arg_types": ["LU"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["Transpose{var\"#s814\", S} where {var\"#s814\"<:Real, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["R"],
      "arg_types": ["LinearAlgebra.Rotation"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["LQ"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["G"],
      "arg_types": ["LinearAlgebra.Givens"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["Adjoint"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["S"],
      "arg_types": [
        "Tridiagonal{var\"#s814\", V} where {var\"#s814\"<:Real, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["S"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["A"],
      "arg_types": ["AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint",
      "arg_names": ["a"],
      "arg_types": ["AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> adjoint(A)\n22 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'x\n25 + 0im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractVector{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "adjoint!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "axpby!",
      "arg_names": ["", "x", "", "y"],
      "arg_types": ["", "AbstractArray", "", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\naxpby!(a, X, b, Y)\n```\n\nOverwrite `Y` with `X*a + Y*b`, where `a` and `b` are scalars. Return `Y`.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 2, 3];\n\njulia> y = [4., 5, 6];\n\njulia> BLAS.axpby!(2., x, 3., y)\n3-element Vector{Float64}:\n 14.0\n 19.0\n 24.0\n```\n"
    },
    {
      "name": "axpy!",
      "arg_names": ["", "x", "rx", "y", "ry"],
      "arg_types": [
        "",
        "AbstractArray",
        "AbstractArray{var\"#s814\", N} where {var\"#s814\"<:Integer, N}",
        "AbstractArray",
        "AbstractArray{var\"#s813\", N} where {var\"#s813\"<:Integer, N}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\naxpy!(a, X, Y)\n```\n\nOverwrite `Y` with `X*a + Y`, where `a` is a scalar. Return `Y`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> BLAS.axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
    },
    {
      "name": "axpy!",
      "arg_names": ["", "x", "y"],
      "arg_types": ["", "AbstractArray", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\naxpy!(a, X, Y)\n```\n\nOverwrite `Y` with `X*a + Y`, where `a` is a scalar. Return `Y`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> BLAS.axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
    },
    {
      "name": "bunchkaufman",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman(A, rook::Bool=false; check = true) -> S::BunchKaufman\n```\n\nCompute the Bunch-Kaufman [^Bunch1977] factorization of a symmetric or Hermitian matrix `A` as `P'*U*D*U'*P` or `P'*L*D*L'*P`, depending on which triangle is stored in `A`, and return a [`BunchKaufman`](@ref) object. Note that if `A` is complex symmetric then `U'` and `L'` denote the unconjugated transposes, i.e. `transpose(U)` and `transpose(L)`.\n\nIterating the decomposition produces the components `S.D`, `S.U` or `S.L` as appropriate given `S.uplo`, and `S.p`.\n\nIf `rook` is `true`, rook pivoting is used. If `rook` is false, rook pivoting is not used.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe following functions are available for `BunchKaufman` objects: [`size`](@ref), `\\`, [`inv`](@ref), [`issymmetric`](@ref), [`ishermitian`](@ref), [`getindex`](@ref).\n\n[^Bunch1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179. [url](http://www.ams.org/journals/mcom/1977-31-137/S0025-5718-1977-0428694-0/).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 3]\n22 Matrix{Int64}:\n 1  2\n 2  3\n\njulia> S = bunchkaufman(A) # A gets wrapped internally by Symmetric(A)\nBunchKaufman{Float64, Matrix{Float64}}\nD factor:\n22 Tridiagonal{Float64, Vector{Float64}}:\n -0.333333  0.0\n  0.0       3.0\nU factor:\n22 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.666667\n     1.0\npermutation:\n2-element Vector{Int64}:\n 1\n 2\n\njulia> d, u, p = S; # destructuring via iteration\n\njulia> d == S.D && u == S.U && p == S.p\ntrue\n\njulia> S = bunchkaufman(Symmetric(A, :L))\nBunchKaufman{Float64, Matrix{Float64}}\nD factor:\n22 Tridiagonal{Float64, Vector{Float64}}:\n 3.0   0.0\n 0.0  -0.333333\nL factor:\n22 UnitLowerTriangular{Float64, Matrix{Float64}}:\n 1.0        \n 0.666667  1.0\npermutation:\n2-element Vector{Int64}:\n 2\n 1\n```\n"
    },
    {
      "name": "bunchkaufman",
      "arg_names": ["A", "rook"],
      "arg_types": ["AbstractMatrix{T}", "Bool"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman(A, rook::Bool=false; check = true) -> S::BunchKaufman\n```\n\nCompute the Bunch-Kaufman [^Bunch1977] factorization of a symmetric or Hermitian matrix `A` as `P'*U*D*U'*P` or `P'*L*D*L'*P`, depending on which triangle is stored in `A`, and return a [`BunchKaufman`](@ref) object. Note that if `A` is complex symmetric then `U'` and `L'` denote the unconjugated transposes, i.e. `transpose(U)` and `transpose(L)`.\n\nIterating the decomposition produces the components `S.D`, `S.U` or `S.L` as appropriate given `S.uplo`, and `S.p`.\n\nIf `rook` is `true`, rook pivoting is used. If `rook` is false, rook pivoting is not used.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe following functions are available for `BunchKaufman` objects: [`size`](@ref), `\\`, [`inv`](@ref), [`issymmetric`](@ref), [`ishermitian`](@ref), [`getindex`](@ref).\n\n[^Bunch1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179. [url](http://www.ams.org/journals/mcom/1977-31-137/S0025-5718-1977-0428694-0/).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 3]\n22 Matrix{Int64}:\n 1  2\n 2  3\n\njulia> S = bunchkaufman(A) # A gets wrapped internally by Symmetric(A)\nBunchKaufman{Float64, Matrix{Float64}}\nD factor:\n22 Tridiagonal{Float64, Vector{Float64}}:\n -0.333333  0.0\n  0.0       3.0\nU factor:\n22 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.666667\n     1.0\npermutation:\n2-element Vector{Int64}:\n 1\n 2\n\njulia> d, u, p = S; # destructuring via iteration\n\njulia> d == S.D && u == S.U && p == S.p\ntrue\n\njulia> S = bunchkaufman(Symmetric(A, :L))\nBunchKaufman{Float64, Matrix{Float64}}\nD factor:\n22 Tridiagonal{Float64, Vector{Float64}}:\n 3.0   0.0\n 0.0  -0.333333\nL factor:\n22 UnitLowerTriangular{Float64, Matrix{Float64}}:\n 1.0        \n 0.666667  1.0\npermutation:\n2-element Vector{Int64}:\n 2\n 1\n```\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A"],
      "arg_types": [
        "Hermitian{T, S} where {T<:Union{ComplexF32, ComplexF64}, S<:StridedMatrix{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A", "rook"],
      "arg_types": [
        "Hermitian{T, S} where {T<:Union{ComplexF32, ComplexF64}, S<:StridedMatrix{T}}",
        "Bool"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Symmetric{T, S}, Symmetric{Complex{T}, S}} where {T<:Union{Float32, Float64}, S<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A", "rook"],
      "arg_types": [
        "Union{Hermitian{T, S}, Symmetric{T, S}, Symmetric{Complex{T}, S}} where {T<:Union{Float32, Float64}, S<:(StridedMatrix{T} where T)}",
        "Bool"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s811\"} where var\"#s811\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "bunchkaufman!",
      "arg_names": ["A", "rook"],
      "arg_types": [
        "StridedMatrix{var\"#s810\"} where var\"#s810\"<:Union{Float32, Float64, ComplexF32, ComplexF64}",
        "Bool"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref). The argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n33 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L'  A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n33 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n         1.0\n     1.0   \n 1.0       \n\njulia> P' * L * L' * P  A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L'  A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["x", "uplo"],
      "arg_types": ["Number", "Symbol"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref). The argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n33 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L'  A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n33 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n         1.0\n     1.0   \n 1.0       \n\njulia> P' * L * L' * P  A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L'  A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["A"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref). The argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n33 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L'  A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n33 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n         1.0\n     1.0   \n 1.0       \n\njulia> P' * L * L' * P  A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L'  A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["A", ""],
      "arg_types": ["Diagonal", "Val{false}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref). The argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n33 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L'  A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n33 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n         1.0\n     1.0   \n 1.0       \n\njulia> P' * L * L' * P  A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSuiteSparse.CHOLMOD.Factor{Float64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n33 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L'  A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Union{Hermitian{var\"#s809\", var\"#s808\"}, Hermitian{Complex{var\"#s809\"}, var\"#s808\"}, Symmetric{var\"#s809\", var\"#s808\"}} where {var\"#s809\"<:Real, var\"#s808\"<:(StridedMatrix{T} where T)}, StridedMatrix{T} where T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["A", ""],
      "arg_types": [
        "Union{Union{Hermitian{var\"#s807\", var\"#s806\"}, Hermitian{Complex{var\"#s807\"}, var\"#s806\"}, Symmetric{var\"#s807\", var\"#s806\"}} where {var\"#s807\"<:Real, var\"#s806\"<:(StridedMatrix{T} where T)}, StridedMatrix{T} where T}",
        "Val{false}"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(false); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n33 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.U\n33 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n     1.0   5.0\n          3.0\n\njulia> C.L\n33 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0       \n  6.0  1.0   \n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
    },
    {
      "name": "cholesky",
      "arg_names": ["A", ""],
      "arg_types": [
        "Union{Union{Hermitian{var\"#s810\", var\"#s809\"}, Hermitian{Complex{var\"#s810\"}, var\"#s809\"}, Symmetric{var\"#s810\", var\"#s809\"}} where {var\"#s810\"<:Real, var\"#s809\"<:(StridedMatrix{T} where T)}, StridedMatrix{T} where T}",
        "Val{true}"
      ],
      "kwarg_names": ["tol", "check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky(A, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`StridedMatrix`](@ref) or a *perfectly* symmetric or Hermitian `StridedMatrix`. The triangular Cholesky factor can be obtained from the factorization `F` with: `F.L` and `F.U`. The following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref). The argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "Val{false}"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": ["StridedMatrix{T} where T", "Val{false}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": [
        "Union{Hermitian{var\"#s809\", var\"#s808\"}, Hermitian{Complex{var\"#s809\"}, var\"#s808\"}, Symmetric{var\"#s809\", var\"#s808\"}} where {var\"#s809\"<:Union{Float32, Float64}, var\"#s808\"<:(StridedMatrix{T} where T)}",
        "Val{true}"
      ],
      "kwarg_names": ["tol", "check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": [
        "Union{Hermitian{var\"#s811\", S}, Hermitian{Complex{var\"#s811\"}, S}, Symmetric{var\"#s811\", S}} where {var\"#s811\"<:Real, S}",
        "Val{true}"
      ],
      "kwarg_names": ["tol", "check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": ["StridedMatrix{T} where T", "Val{true}"],
      "kwarg_names": ["tol", "check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cholesky!",
      "arg_names": ["A", ""],
      "arg_types": ["Diagonal", "Val{false}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\ncholesky!(A::StridedMatrix, Val(false); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::StridedMatrix, Val(true); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
    },
    {
      "name": "cond",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["x", "p"],
      "arg_types": ["Number", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A"],
      "arg_types": [
        "UpperTriangular{var\"#s808\", S} where {var\"#s808\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s808\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A", "p"],
      "arg_types": [
        "UpperTriangular{var\"#s807\", S} where {var\"#s807\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s807\"}}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s808\", S} where {var\"#s808\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s808\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A", "p"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s807\", S} where {var\"#s807\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s807\"}}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s808\", S} where {var\"#s808\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s808\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A", "p"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s807\", S} where {var\"#s807\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s807\"}}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A"],
      "arg_types": [
        "LowerTriangular{var\"#s808\", S} where {var\"#s808\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s808\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A", "p"],
      "arg_types": [
        "LowerTriangular{var\"#s807\", S} where {var\"#s807\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:AbstractMatrix{var\"#s807\"}}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "cond",
      "arg_names": ["A", "p"],
      "arg_types": ["AbstractMatrix{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
    },
    {
      "name": "condskeel",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
    },
    {
      "name": "condskeel",
      "arg_names": ["A", "p"],
      "arg_types": ["AbstractMatrix{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
    },
    {
      "name": "condskeel",
      "arg_names": ["A", "x"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
    },
    {
      "name": "condskeel",
      "arg_names": ["A", "x", "p"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractVector{T} where T",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
    },
    {
      "name": "copy_transpose!",
      "arg_names": ["B", "ir_dest", "jr_dest", "A", "ir_src", "jr_src"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "AbstractRange{Int64}",
        "AbstractRange{Int64}",
        "AbstractVecOrMat{T} where T",
        "AbstractRange{Int64}",
        "AbstractRange{Int64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "No documentation found.\n\n`LinearAlgebra.copy_transpose!` is a `Function`.\n\n```\n# 2 methods for generic function \"copy_transpose!\":\n[1] copy_transpose!(B::AbstractVecOrMat{T} where T, ir_dest::AbstractRange{Int64}, jr_dest::AbstractRange{Int64}, A::AbstractVecOrMat{T} where T, ir_src::AbstractRange{Int64}, jr_src::AbstractRange{Int64}) in LinearAlgebra at /opt/hostedtoolcache/julia/1.6.7/x64/share/julia/stdlib/v1.6/LinearAlgebra/src/transpose.jl:181\n[2] copy_transpose!(B::AbstractMatrix{T} where T, ir_dest::UnitRange{Int64}, jr_dest::UnitRange{Int64}, tM::AbstractChar, M::AbstractVecOrMat{T} where T, ir_src::UnitRange{Int64}, jr_src::UnitRange{Int64}) in LinearAlgebra at /opt/hostedtoolcache/julia/1.6.7/x64/share/julia/stdlib/v1.6/LinearAlgebra/src/matmul.jl:691\n```\n"
    },
    {
      "name": "copy_transpose!",
      "arg_names": ["B", "ir_dest", "jr_dest", "tM", "M", "ir_src", "jr_src"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "UnitRange{Int64}",
        "UnitRange{Int64}",
        "AbstractChar",
        "AbstractVecOrMat{T} where T",
        "UnitRange{Int64}",
        "UnitRange{Int64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "No documentation found.\n\n`LinearAlgebra.copy_transpose!` is a `Function`.\n\n```\n# 2 methods for generic function \"copy_transpose!\":\n[1] copy_transpose!(B::AbstractVecOrMat{T} where T, ir_dest::AbstractRange{Int64}, jr_dest::AbstractRange{Int64}, A::AbstractVecOrMat{T} where T, ir_src::AbstractRange{Int64}, jr_src::AbstractRange{Int64}) in LinearAlgebra at /opt/hostedtoolcache/julia/1.6.7/x64/share/julia/stdlib/v1.6/LinearAlgebra/src/transpose.jl:181\n[2] copy_transpose!(B::AbstractMatrix{T} where T, ir_dest::UnitRange{Int64}, jr_dest::UnitRange{Int64}, tM::AbstractChar, M::AbstractVecOrMat{T} where T, ir_src::UnitRange{Int64}, jr_src::UnitRange{Int64}) in LinearAlgebra at /opt/hostedtoolcache/julia/1.6.7/x64/share/julia/stdlib/v1.6/LinearAlgebra/src/matmul.jl:691\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["A", "B"],
      "arg_types": ["T", "T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["A", "B"],
      "arg_types": ["T", "T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["D1", "D2"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "Diagonal",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "LowerTriangular",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "src"],
      "arg_types": ["Hermitian", "Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "Bidiagonal",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "src"],
      "arg_types": ["Symmetric", "Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "SymTridiagonal",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["A", "J"],
      "arg_types": ["AbstractMatrix{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["B", "ir_dest", "jr_dest", "tM", "M", "ir_src", "jr_src"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "UnitRange{Int64}",
        "UnitRange{Int64}",
        "AbstractChar",
        "AbstractVecOrMat{T} where T",
        "UnitRange{Int64}",
        "UnitRange{Int64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "UpperTriangular",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "rdest", "src", "rsrc"],
      "arg_types": [
        "Array{T, N} where N",
        "Union{AbstractRange{Ti}, UnitRange{Ti}}",
        "Array{T, N} where N",
        "Union{AbstractRange{Ti}, UnitRange{Ti}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "src"],
      "arg_types": ["Tridiagonal", "Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
    },
    {
      "name": "copyto!",
      "arg_names": ["dest", "bc"],
      "arg_types": [
        "Tridiagonal",
        "Base.Broadcast.Broadcasted{var\"#s814\", Axes, F, Args} where {var\"#s814\"<:LinearAlgebra.StructuredMatrixStyle, Axes, F, Args<:Tuple}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at offset `so`, to array `dest` starting at offset `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
    },
    {
      "name": "cross",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVector{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncross(x, y)\n(x,y)\n```\n\nCompute the cross product of two 3-vectors.\n\n# Examples\n\n```jldoctest\njulia> a = [0;1;0]\n3-element Vector{Int64}:\n 0\n 1\n 0\n\njulia> b = [0;0;1]\n3-element Vector{Int64}:\n 0\n 0\n 1\n\njulia> cross(a,b)\n3-element Vector{Int64}:\n 1\n 0\n 0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["F"],
      "arg_types": ["Hessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["Q"],
      "arg_types": ["LinearAlgebra.LQPackedQ"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": [
        "Symmetric{var\"#s814\", S} where {var\"#s814\"<:Real, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["Eigen"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["F"],
      "arg_types": ["LU{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["F"],
      "arg_types": ["UpperHessenberg"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["C"],
      "arg_types": ["CholeskyPivoted"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["C"],
      "arg_types": ["Cholesky"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["Q"],
      "arg_types": ["LinearAlgebra.QRPackedQ"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["Q"],
      "arg_types": ["LinearAlgebra.QRCompactWYQ"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "det",
      "arg_names": ["F"],
      "arg_types": ["Factorization"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["D", "k"],
      "arg_types": ["Diagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M", "n"],
      "arg_types": ["Bidiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M", "n"],
      "arg_types": ["Tridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M", "n"],
      "arg_types": [
        "SymTridiagonal{var\"#s813\", V} where {var\"#s813\"<:Number, V<:AbstractVector{var\"#s813\"}}",
        "Integer"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["M", "n"],
      "arg_types": ["SymTridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["B"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diag",
      "arg_names": ["A", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also: [`diagm`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
    },
    {
      "name": "diagind",
      "arg_names": ["m", "n"],
      "arg_types": ["Integer", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
    },
    {
      "name": "diagind",
      "arg_names": ["m", "n", "k"],
      "arg_types": ["Integer", "Integer", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
    },
    {
      "name": "diagind",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
    },
    {
      "name": "diagind",
      "arg_names": ["A", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n33 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
    },
    {
      "name": "diagm",
      "arg_names": ["kv"],
      "arg_types": [
        "Pair{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Integer, var\"#s813\"<:(AbstractVector{T} where T)}..."
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m``n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n44 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n44 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n```\n"
    },
    {
      "name": "diagm",
      "arg_names": ["m", "n", "kv"],
      "arg_types": [
        "Integer",
        "Integer",
        "Pair{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Integer, var\"#s813\"<:(AbstractVector{T} where T)}..."
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m``n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n44 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n44 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n```\n\n```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m``n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n33 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
    },
    {
      "name": "diagm",
      "arg_names": ["v"],
      "arg_types": ["AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m``n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n33 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
    },
    {
      "name": "diagm",
      "arg_names": ["m", "n", "v"],
      "arg_types": ["Integer", "Integer", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m``n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n44 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n44 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n```\n\n```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m``n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n33 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": [
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}",
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": [
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}",
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "a", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Real, Complex}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "a", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Number",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "transA", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Transpose{var\"#s814\", S} where {var\"#s814\"<:Real, S}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "B", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Bidiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LowerTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UnitUpperTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UpperTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UnitLowerTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Tridiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "S", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "SymTridiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "J", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UniformScaling",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["Number", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Symmetric", "Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "H", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UpperHessenberg",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "adjA", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Adjoint",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "rx", "y", "ry"],
      "arg_types": [
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}",
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "rx", "y", "ry"],
      "arg_types": [
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}",
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{T} where T", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "D", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Diagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Hermitian", "Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["BitVector", "BitVector"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractMatrix{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["AbstractArray", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": ["", "", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "vl", "vh"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "B"],
      "arg_types": ["Number", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
    },
    {
      "name": "eigen",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "SymTridiagonal{T, V} where V<:AbstractVector{T}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "vl", "vu"],
      "arg_types": [
        "SymTridiagonal{T, V} where V<:AbstractVector{T}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nComputes the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(),imag())`. A different comparison function `by()` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{TA}", "AbstractMatrix{TB}"],
      "kwarg_names": ["kws..."],
      "module": "LinearAlgebra",
      "doc": "```\neigen(A, B) -> GeneralizedEigen\n```\n\nComputes the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nAny keyword arguments passed to `eigen` are passed through to the lower-level [`eigen!`](@ref) function.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "vl", "vu"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Hermitian{Complex{var\"#s814\"}, var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\"<:Union{Float32, Float64}, var\"#s813\"<:(StridedMatrix{T} where T)}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "vl", "vh"],
      "arg_types": [
        "Union{Hermitian{T, var\"#s814\"}, Hermitian{Complex{T}, var\"#s814\"}, Symmetric{T, var\"#s814\"}} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{Hermitian{T, S}, Symmetric{T, S}}",
        "Union{Hermitian{T, S}, Symmetric{T, S}}"
      ],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A", "B"],
      "arg_types": ["Hermitian{T, S}", "Hermitian{T, S}"],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigen!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{var\"#s811\", var\"#s810\"}, Hermitian{Complex{var\"#s811\"}, var\"#s810\"}, Symmetric{var\"#s811\", var\"#s810\"}} where {var\"#s811\"<:Union{Float32, Float64}, var\"#s810\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigen!(A, [B]; permute, scale, sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
    },
    {
      "name": "eigmax",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigmax",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Hermitian{Complex{var\"#s814\"}, var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\"<:Real, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigmax",
      "arg_names": ["A"],
      "arg_types": ["Union{Number, AbstractMatrix{T} where T}"],
      "kwarg_names": ["permute", "scale"],
      "module": "LinearAlgebra",
      "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigmin",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigmin",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Hermitian{Complex{var\"#s814\"}, var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\"<:Real, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigmin",
      "arg_names": ["A"],
      "arg_types": ["Union{Number, AbstractMatrix{T} where T}"],
      "kwarg_names": ["permute", "scale"],
      "module": "LinearAlgebra",
      "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n22 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n22 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["F"],
      "arg_types": ["Union{Eigen, GeneralizedEigen}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A", "vl", "vh"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": ["kwargs..."],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s812\", V} where {var\"#s812\"<:Number, V<:AbstractVector{var\"#s812\"}}"
      ],
      "kwarg_names": ["permute", "scale"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": ["permute", "scale"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "SymTridiagonal{T, V} where V<:AbstractVector{T}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A", "vl", "vu"],
      "arg_types": [
        "SymTridiagonal{T, V} where V<:AbstractVector{T}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturns the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": ["kws..."],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n22 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{TA}", "AbstractMatrix{TB}"],
      "kwarg_names": ["kws..."],
      "module": "LinearAlgebra",
      "doc": "```\neigvals(A, B) -> values\n```\n\nComputes the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "vl", "vu"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s811\"} where var\"#s811\"<:Union{Float32, Float64}"
      ],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s812\"} where var\"#s812\"<:Union{ComplexF32, ComplexF64}"
      ],
      "kwarg_names": ["permute", "scale", "sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": ["sortby"],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "irange"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Hermitian{Complex{var\"#s814\"}, var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\"<:Union{Float32, Float64}, var\"#s813\"<:(StridedMatrix{T} where T)}",
        "UnitRange"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "vl", "vh"],
      "arg_types": [
        "Union{Hermitian{T, var\"#s814\"}, Hermitian{Complex{T}, var\"#s814\"}, Symmetric{T, var\"#s814\"}} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{Hermitian{T, S}, Symmetric{T, S}}",
        "Union{Hermitian{T, S}, Symmetric{T, S}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A", "B"],
      "arg_types": ["Hermitian{T, S}", "Hermitian{T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvals!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{var\"#s814\", var\"#s813\"}, Hermitian{Complex{var\"#s814\"}, var\"#s813\"}, Symmetric{var\"#s814\", var\"#s813\"}} where {var\"#s814\"<:Union{Float32, Float64}, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n22 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n22 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n22 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A", "eigvals"],
      "arg_types": [
        "SymTridiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, V<:AbstractVector{var\"#s814\"}}",
        "Vector{var\"#s813\"} where var\"#s813\"<:Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A::SymTridiagonal[, eigvals]) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\nIf the optional vector of eigenvalues `eigvals` is specified, `eigvecs` returns the specific corresponding eigenvectors.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n\njulia> eigvecs(A)\n33 Matrix{Float64}:\n  0.418304  -0.83205      0.364299\n -0.656749  -7.39009e-16  0.754109\n  0.627457   0.5547       0.546448\n\njulia> eigvecs(A, [1.])\n31 Matrix{Float64}:\n  0.8320502943378438\n  4.263514128092366e-17\n -0.5547001962252291\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": [
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": [
        "LinearAlgebra.AbstractTriangular{T, S} where S<:(AbstractMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["F"],
      "arg_types": ["Union{Eigen, GeneralizedEigen}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A::SymTridiagonal[, eigvals]) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\nIf the optional vector of eigenvalues `eigvals` is specified, `eigvecs` returns the specific corresponding eigenvectors.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   \n 2.0  2.0  3.0\n     3.0  1.0\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n\njulia> eigvecs(A)\n33 Matrix{Float64}:\n  0.418304  -0.83205      0.364299\n -0.656749  -7.39009e-16  0.754109\n  0.627457   0.5547       0.546448\n\njulia> eigvecs(A, [1.])\n31 Matrix{Float64}:\n  0.8320502943378438\n  4.263514128092366e-17\n -0.5547001962252291\n```\n\n```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n\n```\neigvecs(A, B) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the generalized eigenvectors of `A` and `B`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvecs(A, B)\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": ["Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A"],
      "arg_types": ["Union{Number, AbstractMatrix{T} where T}"],
      "kwarg_names": ["kws..."],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n33 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
    },
    {
      "name": "eigvecs",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": ["kws..."],
      "module": "LinearAlgebra",
      "doc": "```\neigvecs(A, B) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the generalized eigenvectors of `A` and `B`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n22 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n22 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvecs(A, B)\n22 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n```\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["Adjoint"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["Transpose"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["S"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "factorize",
      "arg_names": ["A"],
      "arg_types": ["Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n55 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n55 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0           \n     1.0  1.0       \n         1.0  1.0   \n             1.0  1.0\n                 1.0\n```\n\nThis returns a `55 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
    },
    {
      "name": "givens",
      "arg_names": ["A", "i1", "i2", "j"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Integer",
        "Integer",
        "Integer"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ngivens(A::AbstractArray, i1::Integer, i2::Integer, j::Integer) -> (G::Givens, r)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that the result of the multiplication\n\n```\nB = G*A\n```\n\nhas the property that\n\n```\nB[i1,j] = r\nB[i2,j] = 0\n```\n\nSee also: [`LinearAlgebra.Givens`](@ref)\n"
    },
    {
      "name": "givens",
      "arg_names": ["x", "i1", "i2"],
      "arg_types": ["AbstractVector{T} where T", "Integer", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ngivens(x::AbstractVector, i1::Integer, i2::Integer) -> (G::Givens, r)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that the result of the multiplication\n\n```\nB = G*x\n```\n\nhas the property that\n\n```\nB[i1] = r\nB[i2] = 0\n```\n\nSee also: [`LinearAlgebra.Givens`](@ref)\n"
    },
    {
      "name": "givens",
      "arg_names": ["f", "g", "i1", "i2"],
      "arg_types": ["T", "T", "Integer", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ngivens(f::T, g::T, i1::Integer, i2::Integer) where {T} -> (G::Givens, r::T)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that for any vector `x` where\n\n```\nx[i1] = f\nx[i2] = g\n```\n\nthe result of the multiplication\n\n```\ny = G*x\n```\n\nhas the property that\n\n```\ny[i1] = r\ny[i2] = 0\n```\n\nSee also: [`LinearAlgebra.Givens`](@ref)\n"
    },
    {
      "name": "hessenberg",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nhessenberg(A) -> Hessenberg\n```\n\nCompute the Hessenberg decomposition of `A` and return a `Hessenberg` object. If `F` is the factorization object, the unitary matrix can be accessed with `F.Q` (of type `LinearAlgebra.HessenbergQ`) and the Hessenberg matrix with `F.H` (of type [`UpperHessenberg`](@ref)), either of which may be converted to a regular matrix with `Matrix(F.H)` or `Matrix(F.Q)`.\n\nIf `A` is [`Hermitian`](@ref) or real-[`Symmetric`](@ref), then the Hessenberg decomposition produces a real-symmetric tridiagonal matrix and `F.H` is of type [`SymTridiagonal`](@ref).\n\nNote that the shifted factorization `A+I = Q (H+I) Q'` can be constructed efficiently by `F + *I` using the [`UniformScaling`](@ref) object [`I`](@ref), which creates a new `Hessenberg` object with shared storage and a modified shift.   The shift of a given `F` is obtained by `F.`. This is useful because multiple shifted solves `(F + *I) \\ b` (for different `` and/or `b`) can be performed efficiently once `F` is created.\n\nIterating the decomposition produces the factors `F.Q, F.H, F.`.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 9. 7.; 4. 4. 1.; 4. 3. 2.]\n33 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> F = hessenberg(A)\nHessenberg{Float64, UpperHessenberg{Float64, Matrix{Float64}}, Matrix{Float64}, Vector{Float64}, Bool}\nQ factor:\n33 LinearAlgebra.HessenbergQ{Float64, Matrix{Float64}, Vector{Float64}, false}:\n 1.0   0.0        0.0\n 0.0  -0.707107  -0.707107\n 0.0  -0.707107   0.707107\nH factor:\n33 UpperHessenberg{Float64, Matrix{Float64}}:\n  4.0      -11.3137       -1.41421\n -5.65685    5.0           2.0\n           -8.88178e-16   1.0\n\njulia> F.Q * F.H * F.Q'\n33 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> q, h = F; # destructuring via iteration\n\njulia> q == F.Q && h == F.H\ntrue\n```\n"
    },
    {
      "name": "hessenberg!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nhessenberg!(A) -> Hessenberg\n```\n\n`hessenberg!` is the same as [`hessenberg`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "hessenberg!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{var\"#s813\", S} where {var\"#s813\"<:Union{Float32, Float64, ComplexF32, ComplexF64}, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s813\")}, Symmetric{var\"#s814\", S} where {var\"#s814\"<:Union{Float32, Float64}, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nhessenberg!(A) -> Hessenberg\n```\n\n`hessenberg!` is the same as [`hessenberg`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "isdiag",
      "arg_names": [""],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["A"],
      "arg_types": [
        "Symmetric{var\"#s814\", S} where {var\"#s814\"<:Real, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["A"],
      "arg_types": [
        "Symmetric{var\"#s814\", S} where {var\"#s814\"<:Complex, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Real, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["B"],
      "arg_types": ["BunchKaufman"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "ishermitian",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["A"],
      "arg_types": ["Union{Eigen, GeneralizedEigen}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["C"],
      "arg_types": ["Union{Cholesky, CholeskyPivoted}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`. See also [`isposdef!`](@ref)\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n22 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
    },
    {
      "name": "isposdef!",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nisposdef!(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`, overwriting `A` in the process. See also [`isposdef`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 2. 50.];\n\njulia> isposdef!(A)\ntrue\n\njulia> A\n22 Matrix{Float64}:\n 1.0  2.0\n 2.0  6.78233\n```\n"
    },
    {
      "name": "issuccess",
      "arg_names": ["C"],
      "arg_types": ["Union{Cholesky, CholeskyPivoted}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> LinearAlgebra.issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> LinearAlgebra.issuccess(F)\nfalse\n```\n"
    },
    {
      "name": "issuccess",
      "arg_names": ["F"],
      "arg_types": ["LU"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> LinearAlgebra.issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> LinearAlgebra.issuccess(F)\nfalse\n```\n"
    },
    {
      "name": "issuccess",
      "arg_names": ["B"],
      "arg_types": ["BunchKaufman"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> LinearAlgebra.issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> LinearAlgebra.issuccess(F)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": [
        "Hermitian{var\"#s814\", S} where {var\"#s814\"<:Real, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": [
        "Hermitian{var\"#s814\", S} where {var\"#s814\"<:Complex, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["B"],
      "arg_types": ["BunchKaufman"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": [""],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{var\"#s814\"} where var\"#s814\"<:Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "issymmetric",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n22 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A"],
      "arg_types": ["Union{LowerTriangular, UnitLowerTriangular}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A", "k"],
      "arg_types": ["Union{LowerTriangular, UnitLowerTriangular}", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["D", "k"],
      "arg_types": ["Diagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A"],
      "arg_types": ["Transpose"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A"],
      "arg_types": ["Adjoint"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["M", "k"],
      "arg_types": ["Bidiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["M", "k"],
      "arg_types": ["Tridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["M", "k"],
      "arg_types": ["SymTridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": [""],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istril",
      "arg_names": ["A", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A"],
      "arg_types": ["Union{UnitUpperTriangular, UpperTriangular}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A", "k"],
      "arg_types": ["Union{UnitUpperTriangular, UpperTriangular}", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["D", "k"],
      "arg_types": ["Diagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A"],
      "arg_types": ["Transpose"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A"],
      "arg_types": ["Adjoint"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M", "k"],
      "arg_types": ["Bidiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M", "k"],
      "arg_types": ["Tridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["M", "k"],
      "arg_types": ["SymTridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": [""],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "istriu",
      "arg_names": ["A", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n22 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n22 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVecOrMat{T} where T", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["Number", "Union{Number, AbstractVecOrMat{T} where T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Diagonal{T1, V} where V<:AbstractVector{T1}",
        "Diagonal{T2, V} where V<:AbstractVector{T2}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": [
        "Union{Adjoint{T, var\"#s814\"}, Transpose{T, var\"#s814\"}} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Union{Adjoint{T, var\"#s814\"}, Transpose{T, var\"#s814\"}} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["BitMatrix", "BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["BitVector", "BitVector"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractMatrix{T}", "AbstractMatrix{S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVector{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "kron",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVector{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nkron(A, B)\n```\n\nKronecker tensor product of two vectors or two matrices.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n22 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n44 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n32 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "Union{UnitUpperTriangular, UpperTriangular}",
        "UpperTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "Union{LowerTriangular, UnitLowerTriangular}",
        "LowerTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["B", "R"],
      "arg_types": [
        "BunchKaufman{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["B", "R"],
      "arg_types": [
        "BunchKaufman{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["B", "R"],
      "arg_types": [
        "BunchKaufman{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedVecOrMat{S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["s", "X"],
      "arg_types": ["Number", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n\n```\nldiv!(a::Number, B::AbstractArray)\n```\n\nDivide each entry in an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rdiv!`](@ref) to divide scalar from right.\n\n# Examples\n\n```jldoctest\njulia> B = [1.0 2.0; 3.0 4.0]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> ldiv!(2.0, B)\n22 Matrix{Float64}:\n 0.5  1.0\n 1.5  2.0\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["F", "B"],
      "arg_types": [
        "Hessenberg{var\"#s814\", var\"#s813\", var\"#s812\", W, V} where {var\"#s814\"<:Complex, var\"#s813\", var\"#s812\"<:(AbstractMatrix{var\"#s811\"} where var\"#s811\"<:Real), W<:(AbstractVector{T} where T), V<:Number}",
        "AbstractVecOrMat{var\"#s810\"} where var\"#s810\"<:Complex"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["F", "B"],
      "arg_types": ["Hessenberg", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B", "rcond"],
      "arg_types": [
        "QRPivoted{T, S} where S<:AbstractMatrix{T}",
        "StridedMatrix{T}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "QRPivoted{T, S} where S<:AbstractMatrix{T}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "QRPivoted{T, S} where S<:AbstractMatrix{T}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "b"],
      "arg_types": ["QRPivoted", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["QRPivoted", "StridedMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{T, var\"#s811\"} where var\"#s811\"<:(StridedMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s787\", var\"#s786\"} where {var\"#s787\", var\"#s786\"<:(Transpose{T, var\"#s785\"} where var\"#s785\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s784\", var\"#s783\"} where {var\"#s784\", var\"#s783\"<:(Adjoint{T, var\"#s717\"} where var\"#s717\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s716\", var\"#s715\"} where {var\"#s716\", var\"#s715\"<:(Adjoint{T, var\"#s714\"} where var\"#s714\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "LowerTriangular{var\"#s798\", var\"#s797\"} where {var\"#s798\", var\"#s797\"<:Transpose}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "LowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Transpose}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "LowerTriangular{var\"#s798\", var\"#s797\"} where {var\"#s798\", var\"#s797\"<:Adjoint}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "LowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Adjoint}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["S", "B"],
      "arg_types": [
        "LDLt{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:SymTridiagonal}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["F", "B"],
      "arg_types": ["UpperHessenberg", "AbstractVecOrMat{T} where T"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["SymTridiagonal", "AbstractVecOrMat{T} where T"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["J", "B"],
      "arg_types": ["UniformScaling", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "LinearAlgebra.QRCompactWY{T, M} where M<:AbstractMatrix{T}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LinearAlgebra.QRCompactWY{T, M} where M<:AbstractMatrix{T}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LU{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LU{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["LU{T, Tridiagonal{T, V}}", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where var\"#s814\"<:(LU{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T))",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LU{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)})}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU{T, Tridiagonal{T, V}}}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["transD", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Diagonal{T, V} where V<:AbstractVector{T})}",
        "AbstractVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Bidiagonal}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Union{LinearAlgebra.AbstractTriangular, Bidiagonal}}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:(LU{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T))",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjF", "B"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:(LU{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T))",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LU{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)})}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", LU{T, Tridiagonal{T, V}}} where var\"#s814\"",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjD", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Diagonal{T, V} where V<:AbstractVector{T})}",
        "AbstractVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Bidiagonal}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Union{LinearAlgebra.AbstractTriangular, Bidiagonal}}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["F", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Hessenberg}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LQ{T, S} where S<:AbstractMatrix{T}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": [
        "CholeskyPivoted{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": [
        "CholeskyPivoted{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": ["CholeskyPivoted", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": ["CholeskyPivoted", "StridedMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": [
        "Cholesky{T, var\"#s814\"} where var\"#s814\"<:(AbstractMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["C", "B"],
      "arg_types": [
        "Cholesky{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["QR{T, S} where S<:AbstractMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["QR", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["Y", "s", "X"],
      "arg_types": ["AbstractArray", "Number", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["Y", "A", "B"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "Factorization",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["x", "A", "b"],
      "arg_types": ["AbstractArray", "Diagonal", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["Y", "J", "B"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "UniformScaling",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{T, var\"#s811\"} where var\"#s811\"<:(StridedMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s787\", var\"#s786\"} where {var\"#s787\", var\"#s786\"<:(Transpose{T, var\"#s785\"} where var\"#s785\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s784\", var\"#s783\"} where {var\"#s784\", var\"#s783\"<:(Adjoint{T, var\"#s717\"} where var\"#s717\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s716\", var\"#s715\"} where {var\"#s716\", var\"#s715\"<:(Adjoint{T, var\"#s714\"} where var\"#s714\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s804\", var\"#s803\"} where {var\"#s804\", var\"#s803\"<:Transpose}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Transpose}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s804\", var\"#s803\"} where {var\"#s804\", var\"#s803\"<:Adjoint}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Adjoint}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{T, var\"#s811\"} where var\"#s811\"<:(StridedMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s787\", var\"#s786\"} where {var\"#s787\", var\"#s786\"<:(Transpose{T, var\"#s785\"} where var\"#s785\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s784\", var\"#s783\"} where {var\"#s784\", var\"#s783\"<:(Adjoint{T, var\"#s717\"} where var\"#s717\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s716\", var\"#s715\"} where {var\"#s716\", var\"#s715\"<:(Adjoint{T, var\"#s714\"} where var\"#s714\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UpperTriangular{var\"#s810\", var\"#s809\"} where {var\"#s810\", var\"#s809\"<:Transpose}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UpperTriangular{var\"#s810\", var\"#s809\"} where {var\"#s810\", var\"#s809\"<:Adjoint}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{T, var\"#s811\"} where var\"#s811\"<:(StridedMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s787\", var\"#s786\"} where {var\"#s787\", var\"#s786\"<:(Transpose{T, var\"#s785\"} where var\"#s785\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s784\", var\"#s783\"} where {var\"#s784\", var\"#s783\"<:(Adjoint{T, var\"#s717\"} where var\"#s717\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s716\", var\"#s715\"} where {var\"#s716\", var\"#s715\"<:(Adjoint{T, var\"#s714\"} where var\"#s714\"<:(StridedMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s792\", var\"#s791\"} where {var\"#s792\", var\"#s791\"<:Transpose}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Transpose}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{LinearAlgebra.AbstractTriangular, Bidiagonal}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s792\", var\"#s791\"} where {var\"#s792\", var\"#s791\"<:Adjoint}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["xA", "b", "x"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Adjoint}",
        "AbstractVector{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "Union{LinearAlgebra.AbstractTriangular, Bidiagonal}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "SVD{T, Tr, M} where {Tr, M<:(AbstractArray{T, N} where N)}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["D", "A"],
      "arg_types": ["Diagonal", "Union{LowerTriangular, UpperTriangular}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["D", "v"],
      "arg_types": [
        "Diagonal{T, V} where V<:AbstractVector{T}",
        "AbstractVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["D", "V"],
      "arg_types": [
        "Diagonal{T, V} where V<:AbstractVector{T}",
        "AbstractMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldiv!",
      "arg_names": ["Y", "A", "B"],
      "arg_types": [
        "AbstractArray",
        "AbstractMatrix{T} where T",
        "AbstractArray"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.051652892561983674\n  0.10020661157024757\n\njulia> A\\X\n3-element Vector{Float64}:\n  0.7128099173553719\n -0.05165289256198333\n  0.10020661157024785\n```\n"
    },
    {
      "name": "ldlt",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nldlt(S::SymTridiagonal) -> LDLt\n```\n\nCompute an `LDLt` factorization of the real symmetric tridiagonal matrix `S` such that `S = L*Diagonal(d)*L'` where `L` is a unit lower triangular matrix and `d` is a vector. The main use of an `LDLt` factorization `F = ldlt(S)` is to solve the linear system of equations `Sx = b` with `F\\b`.\n\n# Examples\n\n```jldoctest\njulia> S = SymTridiagonal([3., 4., 5.], [1., 2.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0  1.0   \n 1.0  4.0  2.0\n     2.0  5.0\n\njulia> ldltS = ldlt(S);\n\njulia> b = [6., 7., 8.];\n\njulia> ldltS \\ b\n3-element Vector{Float64}:\n 1.7906976744186047\n 0.627906976744186\n 1.3488372093023255\n\njulia> S \\ b\n3-element Vector{Float64}:\n 1.7906976744186047\n 0.627906976744186\n 1.3488372093023255\n```\n"
    },
    {
      "name": "ldlt!",
      "arg_names": ["S"],
      "arg_types": ["SymTridiagonal{T, V}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nldlt!(S::SymTridiagonal) -> LDLt\n```\n\nSame as [`ldlt`](@ref), but saves space by overwriting the input `S`, instead of creating a copy.\n\n# Examples\n\n```jldoctest\njulia> S = SymTridiagonal([3., 4., 5.], [1., 2.])\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0  1.0   \n 1.0  4.0  2.0\n     2.0  5.0\n\njulia> ldltS = ldlt!(S);\n\njulia> ldltS === S\nfalse\n\njulia> S\n33 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0       0.333333   \n 0.333333  3.66667   0.545455\n          0.545455  3.90909\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{UnitUpperTriangular, UpperTriangular}",
        "UpperTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "Union{LowerTriangular, UnitLowerTriangular}",
        "LowerTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["c", "A"],
      "arg_types": ["Number", "Union{LowerTriangular, UpperTriangular}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["x", "H"],
      "arg_types": ["Number", "UpperHessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["x", "F"],
      "arg_types": [
        "T",
        "Hessenberg{var\"#s814\", var\"#s813\", S, W, V} where {var\"#s814\", var\"#s813\"<:(UpperHessenberg{T, S} where S<:AbstractMatrix{T}), S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), V<:Number}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["x", "F"],
      "arg_types": [
        "T",
        "Hessenberg{var\"#s814\", var\"#s813\", S, W, V} where {var\"#s814\", var\"#s813\"<:(SymTridiagonal{T, V} where V<:AbstractVector{T}), S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), V<:Number}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "LowerTriangular{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Transpose{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "LowerTriangular{var\"#s811\", var\"#s810\"} where {var\"#s811\", var\"#s810\"<:(Adjoint{T, var\"#s809\"} where var\"#s809\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "LowerTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:(Adjoint{T, var\"#s806\"} where var\"#s806\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s805\", var\"#s804\"} where {var\"#s805\", var\"#s804\"<:(Transpose{T, var\"#s803\"} where var\"#s803\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:(Adjoint{T, var\"#s800\"} where var\"#s800\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:(Adjoint{T, var\"#s797\"} where var\"#s797\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s804\", var\"#s803\"} where {var\"#s804\", var\"#s803\"<:Adjoint}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "LowerTriangular{var\"#s804\", var\"#s803\"} where {var\"#s804\", var\"#s803\"<:Transpose}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["LowerTriangular", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T)",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["J", "B"],
      "arg_types": ["UniformScaling", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["transA", "B"],
      "arg_types": [
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["R", "A"],
      "arg_types": ["LinearAlgebra.Rotation", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["LQ", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["G", "A"],
      "arg_types": ["LinearAlgebra.Givens", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["G", "R"],
      "arg_types": ["LinearAlgebra.Givens", "LinearAlgebra.Rotation"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["s", "X"],
      "arg_types": ["Number", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["Q", "X"],
      "arg_types": [
        "LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", false} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["Q", "X"],
      "arg_types": [
        "LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", true} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["Q", "X"],
      "arg_types": [
        "LinearAlgebra.HessenbergQ{T, S, W, sym} where {S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), sym}",
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRCompactWYQ{T, S}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRCompactWYQ{T, S}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRPackedQ{T, S}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRPackedQ{T, S}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRPackedQ}",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T))}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjA", "B"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "AbstractMatrix{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjQ", "X"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", false} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}})}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjQ", "X"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", true} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}})}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["adjQ", "X"],
      "arg_types": [
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.HessenbergQ{T, S, W, sym} where {S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), sym})}",
        "Adjoint{T, var\"#s812\"} where var\"#s812\"<:StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitUpperTriangular{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Transpose{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s811\", var\"#s810\"} where {var\"#s811\", var\"#s810\"<:(Adjoint{T, var\"#s809\"} where var\"#s809\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:(Adjoint{T, var\"#s806\"} where var\"#s806\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s805\", var\"#s804\"} where {var\"#s805\", var\"#s804\"<:(Transpose{T, var\"#s803\"} where var\"#s803\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:(Adjoint{T, var\"#s800\"} where var\"#s800\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:(Adjoint{T, var\"#s797\"} where var\"#s797\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s809\", var\"#s808\"} where {var\"#s809\", var\"#s808\"<:Adjoint}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UnitUpperTriangular{var\"#s809\", var\"#s808\"} where {var\"#s809\", var\"#s808\"<:Transpose}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitUpperTriangular", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UpperTriangular{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Transpose{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UpperTriangular{var\"#s811\", var\"#s810\"} where {var\"#s811\", var\"#s810\"<:(Adjoint{T, var\"#s809\"} where var\"#s809\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:(Adjoint{T, var\"#s806\"} where var\"#s806\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s805\", var\"#s804\"} where {var\"#s805\", var\"#s804\"<:(Transpose{T, var\"#s803\"} where var\"#s803\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:(Adjoint{T, var\"#s800\"} where var\"#s800\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:(Adjoint{T, var\"#s797\"} where var\"#s797\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["UpperTriangular", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["LinearAlgebra.QRPackedQ{T, S}", "StridedVecOrMat{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["LinearAlgebra.QRPackedQ", "AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitLowerTriangular{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{T, var\"#s813\"} where var\"#s813\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Transpose{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s811\", var\"#s810\"} where {var\"#s811\", var\"#s810\"<:(Adjoint{T, var\"#s809\"} where var\"#s809\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "b"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:(Adjoint{T, var\"#s806\"} where var\"#s806\"<:(StridedMatrix{T} where T))}",
        "StridedVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s805\", var\"#s804\"} where {var\"#s805\", var\"#s804\"<:(Transpose{T, var\"#s803\"} where var\"#s803\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:(Adjoint{T, var\"#s800\"} where var\"#s800\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:(Adjoint{T, var\"#s797\"} where var\"#s797\"<:(StridedMatrix{T} where T))}",
        "StridedMatrix{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:Adjoint}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["xA", "B"],
      "arg_types": [
        "UnitLowerTriangular{var\"#s799\", var\"#s798\"} where {var\"#s799\", var\"#s798\"<:Transpose}",
        "StridedVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["UnitLowerTriangular", "StridedVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["Tridiagonal", "LinearAlgebra.AbstractTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "lmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["LinearAlgebra.QRCompactWYQ{T, S}", "StridedVecOrMat{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.lmul!(A, B);\n\njulia> B\n22 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n22 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": [
        "Union{LowerTriangular{T, S} where S<:AbstractMatrix{T}, UpperTriangular{T, S} where S<:AbstractMatrix{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["F"],
      "arg_types": ["LU{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["F"],
      "arg_types": ["BunchKaufman"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["F"],
      "arg_types": ["UpperHessenberg"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["F"],
      "arg_types": ["Hessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logabsdet",
      "arg_names": ["F"],
      "arg_types": [
        "LDLt{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:SymTridiagonal}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n22 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n22 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["C"],
      "arg_types": ["Cholesky"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["C"],
      "arg_types": ["CholeskyPivoted"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Real, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Complex, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["F"],
      "arg_types": ["Hessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["F"],
      "arg_types": ["Factorization"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "logdet",
      "arg_names": ["A"],
      "arg_types": [""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n22 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
    },
    {
      "name": "lowrankdowndate",
      "arg_names": ["C", "v"],
      "arg_types": ["Cholesky", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlowrankdowndate(C::Cholesky, v::StridedVector) -> CC::Cholesky\n```\n\nDowndate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U - v*v')` but the computation of `CC` only uses `O(n^2)` operations.\n"
    },
    {
      "name": "lowrankdowndate!",
      "arg_names": ["C", "v"],
      "arg_types": ["Cholesky", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlowrankdowndate!(C::Cholesky, v::StridedVector) -> CC::Cholesky\n```\n\nDowndate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U - v*v')` but the computation of `CC` only uses `O(n^2)` operations. The input factorization `C` is updated in place such that on exit `C == CC`. The vector `v` is destroyed during the computation.\n"
    },
    {
      "name": "lowrankupdate",
      "arg_names": ["C", "v"],
      "arg_types": ["Cholesky", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlowrankupdate(C::Cholesky, v::StridedVector) -> CC::Cholesky\n```\n\nUpdate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U + v*v')` but the computation of `CC` only uses `O(n^2)` operations.\n"
    },
    {
      "name": "lowrankupdate!",
      "arg_names": ["C", "v"],
      "arg_types": ["Cholesky", "StridedVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlowrankupdate!(C::Cholesky, v::StridedVector) -> CC::Cholesky\n```\n\nUpdate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U + v*v')` but the computation of `CC` only uses `O(n^2)` operations. The input factorization `C` is updated in place such that on exit `C == CC`. The vector `v` is destroyed during the computation.\n"
    },
    {
      "name": "lq",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlq(A) -> S::LQ\n```\n\nCompute the LQ decomposition of `A`. The decomposition's lower triangular component can be obtained from the [`LQ`](@ref) object `S` via `S.L`, and the orthogonal/unitary component via `S.Q`, such that `A  S.L*S.Q`.\n\nIterating the decomposition produces the components `S.L` and `S.Q`.\n\nThe LQ decomposition is the QR decomposition of `transpose(A)`, and it is useful in order to compute the minimum-norm solution `lq(A) \\ b` to an underdetermined system of equations (`A` has more columns than rows, but has full row rank).\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> S = lq(A)\nLQ{Float64, Matrix{Float64}} with factors L and Q:\n[-8.60233 0.0; 4.41741 -0.697486]\n[-0.581238 -0.813733; -0.813733 0.581238]\n\njulia> S.L * S.Q\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> l, q = S; # destructuring via iteration\n\njulia> l == S.L &&  q == S.Q\ntrue\n```\n"
    },
    {
      "name": "lq",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlq(A) -> S::LQ\n```\n\nCompute the LQ decomposition of `A`. The decomposition's lower triangular component can be obtained from the [`LQ`](@ref) object `S` via `S.L`, and the orthogonal/unitary component via `S.Q`, such that `A  S.L*S.Q`.\n\nIterating the decomposition produces the components `S.L` and `S.Q`.\n\nThe LQ decomposition is the QR decomposition of `transpose(A)`, and it is useful in order to compute the minimum-norm solution `lq(A) \\ b` to an underdetermined system of equations (`A` has more columns than rows, but has full row rank).\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> S = lq(A)\nLQ{Float64, Matrix{Float64}} with factors L and Q:\n[-8.60233 0.0; 4.41741 -0.697486]\n[-0.581238 -0.813733; -0.813733 0.581238]\n\njulia> S.L * S.Q\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> l, q = S; # destructuring via iteration\n\njulia> l == S.L &&  q == S.Q\ntrue\n```\n"
    },
    {
      "name": "lq!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlq!(A) -> LQ\n```\n\nCompute the [`LQ`](@ref) factorization of `A`, using the input matrix as a workspace. See also [`lq`](@ref).\n"
    },
    {
      "name": "lu",
      "arg_names": ["S"],
      "arg_types": ["LU"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu(A, pivot=Val(true); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`. If pivoting is chosen (default) the element type should also support [`abs`](@ref) and [`<`](@ref).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         |     |                        |\n| [`\\`](@ref)         |     |                       |\n| [`inv`](@ref)       |     |                       |\n| [`det`](@ref)       |     |                       |\n| [`logdet`](@ref)    |     |                       |\n| [`logabsdet`](@ref) |     |                       |\n| [`size`](@ref)      |     |                       |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
    },
    {
      "name": "lu",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu(A, pivot=Val(true); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`. If pivoting is chosen (default) the element type should also support [`abs`](@ref) and [`<`](@ref).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         |     |                        |\n| [`\\`](@ref)         |     |                       |\n| [`inv`](@ref)       |     |                       |\n| [`det`](@ref)       |     |                       |\n| [`logdet`](@ref)    |     |                       |\n| [`logabsdet`](@ref) |     |                       |\n| [`size`](@ref)      |     |                       |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
    },
    {
      "name": "lu",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu(A, pivot=Val(true); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`. If pivoting is chosen (default) the element type should also support [`abs`](@ref) and [`<`](@ref).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         |     |                        |\n| [`\\`](@ref)         |     |                       |\n| [`inv`](@ref)       |     |                       |\n| [`det`](@ref)       |     |                       |\n| [`logdet`](@ref)    |     |                       |\n| [`logabsdet`](@ref) |     |                       |\n| [`size`](@ref)      |     |                       |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
    },
    {
      "name": "lu",
      "arg_names": ["A", "pivot"],
      "arg_types": ["AbstractMatrix{T}", "Union{Val{false}, Val{true}}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu(A, pivot=Val(true); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`. If pivoting is chosen (default) the element type should also support [`abs`](@ref) and [`<`](@ref).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         |     |                        |\n| [`\\`](@ref)         |     |                       |\n| [`inv`](@ref)       |     |                       |\n| [`det`](@ref)       |     |                       |\n| [`logdet`](@ref)    |     |                       |\n| [`logabsdet`](@ref) |     |                       |\n| [`size`](@ref)      |     |                       |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A", "pivot"],
      "arg_types": ["StridedMatrix{T}", "Union{Val{false}, Val{true}}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A"],
      "arg_types": ["Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::SparseMatrixCSC; check=true) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. The sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n!!! note\n    `lu!(F::UmfpackLU, A::SparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A", "pivot"],
      "arg_types": [
        "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}",
        "Union{Val{false}, Val{true}}"
      ],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::SparseMatrixCSC; check=true) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. The sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n!!! note\n    `lu!(F::UmfpackLU, A::SparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A", "pivot"],
      "arg_types": ["StridedMatrix{T} where T", "Union{Val{false}, Val{true}}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A"],
      "arg_types": ["Tridiagonal{T, V}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::SparseMatrixCSC; check=true) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. The sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n!!! note\n    `lu!(F::UmfpackLU, A::SparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "lu!",
      "arg_names": ["A", "pivot"],
      "arg_types": ["Tridiagonal{T, V}", "Union{Val{false}, Val{true}}"],
      "kwarg_names": ["check"],
      "module": "LinearAlgebra",
      "doc": "```\nlu!(A, pivot=Val(true); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n22 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}}\nL factor:\n22 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n22 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n22 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::SparseMatrixCSC; check=true) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. The sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n!!! note\n    `lu!(F::UmfpackLU, A::SparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "lyap",
      "arg_names": ["A", "C"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n22 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' + B\n22 Matrix{Float64}:\n 0.0          6.66134e-16\n 6.66134e-16  8.88178e-16\n```\n"
    },
    {
      "name": "lyap",
      "arg_names": ["A", "C"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n22 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' + B\n22 Matrix{Float64}:\n 0.0          6.66134e-16\n 6.66134e-16  8.88178e-16\n```\n"
    },
    {
      "name": "lyap",
      "arg_names": ["a", "c"],
      "arg_types": ["T", "T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n22 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' + B\n22 Matrix{Float64}:\n 0.0          6.66134e-16\n 6.66134e-16  8.88178e-16\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{ComplexF32}",
        "StridedVecOrMat{ComplexF32}",
        "StridedVector{Float32}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{ComplexF64}",
        "StridedVecOrMat{ComplexF64}",
        "StridedVector{Float64}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{T}",
        "StridedVecOrMat{T}",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "transA", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "adjA", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "adjA", "x", "alpha", "beta"],
      "arg_types": [
        "StridedVector{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{ComplexF32}",
        "StridedVecOrMat{ComplexF32}",
        "StridedVecOrMat{Float32}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{ComplexF64}",
        "StridedVecOrMat{ComplexF64}",
        "StridedVecOrMat{Float64}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedVecOrMat{T}",
        "StridedVecOrMat{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVecOrMat{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{ComplexF32}",
        "StridedVecOrMat{ComplexF32}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{Float32}}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{ComplexF64}",
        "StridedVecOrMat{ComplexF64}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{Float64}}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedVecOrMat{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVecOrMat{T}",
        "Real",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "B", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "StridedVecOrMat{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedVecOrMat{var\"#s812\"} where var\"#s812\"<:Union{Float32, Float64})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "adjB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "transB", "alpha", "beta"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:StridedVecOrMat{T}}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:StridedVecOrMat{T}}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "StridedVecOrMat{T} where T",
        "SymTridiagonal",
        "StridedVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "Q", "B"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "LinearAlgebra.AbstractQ{T}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "Q"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "StridedVecOrMat{T}",
        "LinearAlgebra.AbstractQ{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjQ", "B"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.AbstractQ{T}}",
        "StridedVecOrMat{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjQ"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.AbstractQ{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "", ""],
      "arg_types": [
        "StridedVector{T}",
        "Symmetric{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "", ""],
      "arg_types": [
        "StridedVector{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "", ""],
      "arg_types": [
        "StridedVector{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedVector{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "Symmetric{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedMatrix{T}",
        "Symmetric{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedMatrix{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "StridedMatrix{T}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "", ""],
      "arg_types": [
        "StridedMatrix{T}",
        "StridedMatrix{T}",
        "Hermitian{T, var\"#s814\"} where var\"#s814\"<:(StridedMatrix{T} where T)",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "StridedMatrix{T} where T",
        "Diagonal",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "StridedMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "StridedMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Diagonal",
        "StridedMatrix{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Diagonal",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Diagonal",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Diagonal",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "StridedMatrix{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.AbstractTriangular}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Diagonal",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "transB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Symmetric{T, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Symmetric{T, S}, Symmetric{Complex{T}, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "transB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "transA", "x", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "transA", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "J", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractMatrix{T} where T",
        "UniformScaling",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Tridiagonal",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjB"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjB"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LinearAlgebra.AbstractTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LinearAlgebra.AbstractTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractVecOrMat{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "StridedMatrix{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.AbstractTriangular}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Diagonal",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Symmetric{T, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Symmetric{T, S}, Symmetric{Complex{T}, S}} where {T<:Real, S})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(Union{Hermitian{T, S}, Symmetric{T, S}, Symmetric{Complex{T}, S}} where {T<:Real, S})}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "adjB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Adjoint{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "adjB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractVecOrMat{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "in", "A", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:Diagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(StridedMatrix{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "transB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Transpose{var\"#s812\", var\"#s811\"} where {var\"#s812\", var\"#s811\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "transB", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractVecOrMat{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "adjA", "x", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "adjA", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractVecOrMat{T} where T)}",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Tridiagonal",
        "LinearAlgebra.AbstractTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "SymTridiagonal",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractMatrix{T} where T",
        "Union{Bidiagonal, SymTridiagonal, Tridiagonal}",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "J", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "UniformScaling",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "UnitUpperTriangular",
        "UnitUpperTriangular",
        "UnitUpperTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "UnitUpperTriangular",
        "UnitUpperTriangular",
        "UnitUpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "UpperTriangular",
        "Number",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "Number",
        "UpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "UnitUpperTriangular",
        "Number",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "Number",
        "UnitUpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": ["UpperTriangular", "UpperTriangular", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "UpperTriangular",
        "UpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "UpperTriangular",
        "UpperTriangular",
        "UnitUpperTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "UpperTriangular",
        "UnitUpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "UpperTriangular",
        "UnitUpperTriangular",
        "UpperTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "UpperTriangular",
        "UnitUpperTriangular",
        "UpperTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["out", "A", "in", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Diagonal",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "UnitLowerTriangular",
        "UnitLowerTriangular",
        "UnitLowerTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "UnitLowerTriangular",
        "UnitLowerTriangular",
        "UnitLowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "LowerTriangular",
        "Number",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "Number",
        "LowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "UnitLowerTriangular",
        "Number",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "X", "s", "alpha", "beta"],
      "arg_types": [
        "AbstractArray",
        "AbstractArray",
        "Number",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["A", "B", "C", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "Number",
        "UnitLowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "s", "X", "alpha", "beta"],
      "arg_types": [
        "AbstractArray",
        "Number",
        "AbstractArray",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": ["LowerTriangular", "LowerTriangular", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "LowerTriangular",
        "LowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "LowerTriangular",
        "LowerTriangular",
        "UnitLowerTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "LowerTriangular",
        "UnitLowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "LowerTriangular",
        "UnitLowerTriangular",
        "LowerTriangular"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "LowerTriangular",
        "UnitLowerTriangular",
        "LowerTriangular",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["y", "A", "x", "alpha", "beta"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractVecOrMat{T} where T",
        "AbstractVector{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B", "alpha", "beta"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "AbstractVecOrMat{T} where T",
        "AbstractVecOrMat{T} where T",
        "Number",
        "Number"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(C, A, B, , ) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B  + C $. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n22 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "LinearAlgebra.AbstractTriangular",
        "AbstractVecOrMat{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "mul!",
      "arg_names": ["C", "A", "B"],
      "arg_types": ["", "", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n22 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
    },
    {
      "name": "norm",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["x", "p"],
      "arg_types": ["Number", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
    },
    {
      "name": "norm",
      "arg_names": [""],
      "arg_types": ["Missing"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["", "p"],
      "arg_types": ["Missing", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["v", "p"],
      "arg_types": [
        "Union{Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}, Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["x", "rx"],
      "arg_types": [
        "StridedVector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["itr"],
      "arg_types": [""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
    },
    {
      "name": "norm",
      "arg_names": ["itr", "p"],
      "arg_types": ["", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
    },
    {
      "name": "normalize",
      "arg_names": ["a"],
      "arg_types": ["AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnormalize(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize!`](@ref) and [`norm`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n23 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n23 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\n```\n"
    },
    {
      "name": "normalize",
      "arg_names": ["a", "p"],
      "arg_types": ["AbstractArray", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnormalize(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize!`](@ref) and [`norm`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n23 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n23 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\n```\n"
    },
    {
      "name": "normalize!",
      "arg_names": ["a"],
      "arg_types": ["AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnormalize!(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` in-place so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize`](@ref) and [`norm`](@ref).\n"
    },
    {
      "name": "normalize!",
      "arg_names": ["a", "p"],
      "arg_types": ["AbstractArray", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnormalize!(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` in-place so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize`](@ref) and [`norm`](@ref).\n"
    },
    {
      "name": "nullspace",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": ["atol", "rtol"],
      "module": "LinearAlgebra",
      "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes greater than `max(atol, rtol*)`, where `` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n33 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n33 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "nullspace",
      "arg_names": ["A"],
      "arg_types": ["AbstractVector{T} where T"],
      "kwarg_names": ["atol", "rtol"],
      "module": "LinearAlgebra",
      "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes greater than `max(atol, rtol*)`, where `` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n33 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n33 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "nullspace",
      "arg_names": ["A", "tol"],
      "arg_types": ["AbstractVector{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes greater than `max(atol, rtol*)`, where `` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n33 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n33 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "nullspace",
      "arg_names": ["A", "tol"],
      "arg_types": ["AbstractMatrix{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes greater than `max(atol, rtol*)`, where `` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n33 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n33 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n31 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["x", "p"],
      "arg_types": ["Number", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["v"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["v", "q"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["v"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["v", "q"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1  n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1  n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["J", "p"],
      "arg_types": ["UniformScaling", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1  n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
    },
    {
      "name": "opnorm",
      "arg_names": ["A", "p"],
      "arg_types": ["AbstractMatrix{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1  j  n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1  i  m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n23 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
    },
    {
      "name": "ordschur",
      "arg_names": ["schur", "select"],
      "arg_types": ["Schur", "Union{BitVector, Vector{Bool}}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nordschur(F::Schur, select::Union{Vector{Bool},BitVector}) -> F::Schur\n```\n\nReorders the Schur factorization `F` of a matrix `A = Z*T*Z'` according to the logical array `select` returning the reordered factorization `F` object. The selected eigenvalues appear in the leading diagonal of `F.Schur` and the corresponding leading columns of `F.vectors` form an orthogonal/unitary basis of the corresponding right invariant subspace. In the real case, a complex conjugate pair of eigenvalues must be either both included or both excluded via `select`.\n"
    },
    {
      "name": "ordschur",
      "arg_names": ["gschur", "select"],
      "arg_types": ["GeneralizedSchur", "Union{BitVector, Vector{Bool}}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nordschur(F::GeneralizedSchur, select::Union{Vector{Bool},BitVector}) -> F::GeneralizedSchur\n```\n\nReorders the Generalized Schur factorization `F` of a matrix pair `(A, B) = (Q*S*Z', Q*T*Z')` according to the logical array `select` and returns a GeneralizedSchur object `F`. The selected eigenvalues appear in the leading diagonal of both `F.S` and `F.T`, and the left and right orthogonal/unitary Schur vectors are also reordered such that `(A, B) = F.Q*(F.S, F.T)*F.Z'` still holds and the generalized eigenvalues of `A` and `B` can still be obtained with `F../F.`.\n"
    },
    {
      "name": "ordschur!",
      "arg_names": ["schur", "select"],
      "arg_types": ["Schur", "Union{BitVector, Vector{Bool}}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nordschur!(F::Schur, select::Union{Vector{Bool},BitVector}) -> F::Schur\n```\n\nSame as [`ordschur`](@ref) but overwrites the factorization `F`.\n"
    },
    {
      "name": "ordschur!",
      "arg_names": ["gschur", "select"],
      "arg_types": ["GeneralizedSchur", "Union{BitVector, Vector{Bool}}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nordschur!(F::GeneralizedSchur, select::Union{Vector{Bool},BitVector}) -> F::GeneralizedSchur\n```\n\nSame as `ordschur` but overwrites the factorization `F`.\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v", "tol"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v", "tol"],
      "arg_types": [
        "Transpose{T, var\"#s814\"} where {T, var\"#s814\"<:(AbstractVector{T} where T)}",
        "Real"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["D"],
      "arg_types": ["Diagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["D", "tol"],
      "arg_types": ["Diagonal{T, V} where V<:AbstractVector{T}", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v"],
      "arg_types": ["AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v", "tol"],
      "arg_types": ["AbstractVector{T}", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v"],
      "arg_types": ["AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v", "tol"],
      "arg_types": ["AbstractVector{T}", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v"],
      "arg_types": ["AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["v", "tol"],
      "arg_types": ["AbstractVector{T}", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": ["atol", "rtol"],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["A", "tol"],
      "arg_types": ["AbstractMatrix{T}", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "pinv",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*)` where `` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `M`, and `` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(one(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n22 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n22 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n22 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: ke Bjrck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
    },
    {
      "name": "qr",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr(A, pivot=Val(false); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == Val(true)` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref).  This operation returns the \"thin\" Q factor, i.e., if `A` is `m``n` with `m>=n`, then `Matrix(F.Q)` yields an `m``n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m``m` orthogonal matrix, use `F.Q*Matrix(I,m,m)`.  If `m<=n`, then `Matrix(F.Q)` yields an `m``m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == Val(false)` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`.  See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n32 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n33 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.6   0.0   0.8\n -0.8   0.0  -0.6\n  0.0  -1.0   0.0\nR factor:\n22 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
    },
    {
      "name": "qr",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr(A, pivot=Val(false); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == Val(true)` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref).  This operation returns the \"thin\" Q factor, i.e., if `A` is `m``n` with `m>=n`, then `Matrix(F.Q)` yields an `m``n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m``m` orthogonal matrix, use `F.Q*Matrix(I,m,m)`.  If `m<=n`, then `Matrix(F.Q)` yields an `m``m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == Val(false)` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`.  See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n32 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n33 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.6   0.0   0.8\n -0.8   0.0  -0.6\n  0.0  -1.0   0.0\nR factor:\n22 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
    },
    {
      "name": "qr",
      "arg_names": ["A", "arg..."],
      "arg_types": ["AbstractMatrix{T}", ""],
      "kwarg_names": ["kwargs..."],
      "module": "LinearAlgebra",
      "doc": "```\nqr(A, pivot=Val(false); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == Val(true)` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref).  This operation returns the \"thin\" Q factor, i.e., if `A` is `m``n` with `m>=n`, then `Matrix(F.Q)` yields an `m``n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m``m` orthogonal matrix, use `F.Q*Matrix(I,m,m)`.  If `m<=n`, then `Matrix(F.Q)` yields an `m``m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == Val(false)` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`.  See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n32 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n33 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.6   0.0   0.8\n -0.8   0.0  -0.6\n  0.0  -1.0   0.0\nR factor:\n22 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
    },
    {
      "name": "qr",
      "arg_names": ["v"],
      "arg_types": ["AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr(A, pivot=Val(false); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == Val(true)` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref).  This operation returns the \"thin\" Q factor, i.e., if `A` is `m``n` with `m>=n`, then `Matrix(F.Q)` yields an `m``n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m``m` orthogonal matrix, use `F.Q*Matrix(I,m,m)`.  If `m<=n`, then `Matrix(F.Q)` yields an `m``m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == Val(false)` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`.  See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n32 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n33 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.6   0.0   0.8\n -0.8   0.0  -0.6\n  0.0  -1.0   0.0\nR factor:\n22 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s810\"} where var\"#s810\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A", ""],
      "arg_types": [
        "StridedMatrix{var\"#s809\"} where var\"#s809\"<:Union{Float32, Float64, ComplexF32, ComplexF64}",
        "Val{false}"
      ],
      "kwarg_names": ["blocksize"],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A", ""],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}",
        "Val{true}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A", ""],
      "arg_types": ["StridedMatrix{T} where T", "Val{false}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "qr!",
      "arg_names": ["A", ""],
      "arg_types": ["StridedMatrix{T} where T", "Val{true}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nqr!(A, pivot=Val(false); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`StridedMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}\nQ factor:\n22 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:\n -0.316228  -0.948683\n -0.948683   0.316228\nR factor:\n22 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(-3.1622776601683795)\nStacktrace:\n[...]\n```\n"
    },
    {
      "name": "rank",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the rank of a matrix by counting how many singular values of `A` have magnitude greater than `max(atol, rtol*)` where `` is `A`'s largest singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `A`, and `` is the [`eps`](@ref) of the element type of `A`.\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n"
    },
    {
      "name": "rank",
      "arg_names": ["C"],
      "arg_types": ["CholeskyPivoted"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the rank of a matrix by counting how many singular values of `A` have magnitude greater than `max(atol, rtol*)` where `` is `A`'s largest singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `A`, and `` is the [`eps`](@ref) of the element type of `A`.\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n"
    },
    {
      "name": "rank",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": ["atol", "rtol"],
      "module": "LinearAlgebra",
      "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the rank of a matrix by counting how many singular values of `A` have magnitude greater than `max(atol, rtol*)` where `` is `A`'s largest singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `A`, and `` is the [`eps`](@ref) of the element type of `A`.\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n"
    },
    {
      "name": "rank",
      "arg_names": ["A", "tol"],
      "arg_types": ["AbstractMatrix{T} where T", "Real"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the rank of a matrix by counting how many singular values of `A` have magnitude greater than `max(atol, rtol*)` where `` is `A`'s largest singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*`, where `n` is the size of the smallest dimension of `A`, and `` is the [`eps`](@ref) of the element type of `A`.\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{T, var\"#s810\"} where var\"#s810\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{T, var\"#s810\"} where var\"#s810\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{T, var\"#s810\"} where var\"#s810\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{T, var\"#s810\"} where var\"#s810\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s713\", var\"#s712\"} where {var\"#s713\", var\"#s712\"<:(Transpose{T, var\"#s711\"} where var\"#s711\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s710\", var\"#s709\"} where {var\"#s710\", var\"#s709\"<:(Adjoint{T, var\"#s708\"} where var\"#s708\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s707\", var\"#s77\"} where {var\"#s707\", var\"#s77\"<:(Adjoint{T, var\"#s76\"} where var\"#s76\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s713\", var\"#s712\"} where {var\"#s713\", var\"#s712\"<:(Transpose{T, var\"#s711\"} where var\"#s711\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s710\", var\"#s709\"} where {var\"#s710\", var\"#s709\"<:(Adjoint{T, var\"#s708\"} where var\"#s708\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s707\", var\"#s77\"} where {var\"#s707\", var\"#s77\"<:(Adjoint{T, var\"#s76\"} where var\"#s76\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s713\", var\"#s712\"} where {var\"#s713\", var\"#s712\"<:(Transpose{T, var\"#s711\"} where var\"#s711\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s710\", var\"#s709\"} where {var\"#s710\", var\"#s709\"<:(Adjoint{T, var\"#s708\"} where var\"#s708\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s707\", var\"#s77\"} where {var\"#s707\", var\"#s77\"<:(Adjoint{T, var\"#s76\"} where var\"#s76\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s713\", var\"#s712\"} where {var\"#s713\", var\"#s712\"<:(Transpose{T, var\"#s711\"} where var\"#s711\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s710\", var\"#s709\"} where {var\"#s710\", var\"#s709\"<:(Adjoint{T, var\"#s708\"} where var\"#s708\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s707\", var\"#s77\"} where {var\"#s707\", var\"#s77\"<:(Adjoint{T, var\"#s76\"} where var\"#s76\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitLowerTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UpperTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitUpperTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "LowerTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitLowerTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UpperTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "xB"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitUpperTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "C"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "Cholesky{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(AbstractMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "C"],
      "arg_types": ["StridedMatrix{T} where T", "CholeskyPivoted"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedVecOrMat{T} where T",
        "LU{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(StridedMatrix{T} where T)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "D"],
      "arg_types": ["Union{LowerTriangular, UpperTriangular}", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractMatrix{T} where T", "LU"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "A"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "F"],
      "arg_types": ["AbstractMatrix{T} where T", "UpperHessenberg"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "F"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Hessenberg}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "A"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LU}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "S"],
      "arg_types": [
        "AbstractVecOrMat{T} where T",
        "LDLt{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:SymTridiagonal}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["X", "s"],
      "arg_types": ["AbstractArray", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n```\nrdiv!(A::AbstractArray, b::Number)\n```\n\nDivide each entry in an array `A` by a scalar `b` overwriting `A` in-place.  Use [`ldiv!`](@ref) to divide scalar from left.\n\n# Examples\n\n```jldoctest\njulia> A = [1.0 2.0; 3.0 4.0]\n22 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> rdiv!(A, 2.0)\n22 Matrix{Float64}:\n 0.5  1.0\n 1.5  2.0\n```\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "F"],
      "arg_types": [
        "AbstractVecOrMat{var\"#s814\"} where var\"#s814\"<:Complex",
        "Hessenberg{var\"#s813\", var\"#s812\", var\"#s811\", W, V} where {var\"#s813\"<:Complex, var\"#s812\", var\"#s811\"<:(AbstractMatrix{var\"#s810\"} where var\"#s810\"<:Real), W<:(AbstractVector{T} where T), V<:Number}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "F"],
      "arg_types": ["AbstractMatrix{T} where T", "Hessenberg"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractVecOrMat{T} where T", "SymTridiagonal"],
      "kwarg_names": ["shift"],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "J"],
      "arg_types": ["AbstractMatrix{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular",
        "Union{UnitUpperTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "D"],
      "arg_types": [
        "AbstractMatrix{T}",
        "Diagonal{T, V} where V<:AbstractVector{T}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "adjD"],
      "arg_types": [
        "AbstractMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Diagonal{T, V} where V<:AbstractVector{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "transD"],
      "arg_types": [
        "AbstractMatrix{T}",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(Diagonal{T, V} where V<:AbstractVector{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "rdiv!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular",
        "Union{LowerTriangular, UnitLowerTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n"
    },
    {
      "name": "reflect!",
      "arg_names": ["x", "y", "c", "s"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractVector{T} where T",
        "",
        ""
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nreflect!(x, y, c, s)\n```\n\nOverwrite `x` with `c*x + s*y` and `y` with `conj(s)*x - c*y`. Returns `x` and `y`.\n\n!!! compat \"Julia 1.5\"\n    `reflect!` requires at least Julia 1.5.\n\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "c"],
      "arg_types": ["Union{LowerTriangular, UpperTriangular}", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{T, var\"#s812\"} where var\"#s812\"<:(StridedMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:(Transpose{T, var\"#s794\"} where var\"#s794\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s793\", var\"#s792\"} where {var\"#s793\", var\"#s792\"<:(Adjoint{T, var\"#s791\"} where var\"#s791\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LowerTriangular{var\"#s790\", var\"#s789\"} where {var\"#s790\", var\"#s789\"<:(Adjoint{T, var\"#s788\"} where var\"#s788\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:(Transpose{T, var\"#s794\"} where var\"#s794\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s793\", var\"#s792\"} where {var\"#s793\", var\"#s792\"<:(Adjoint{T, var\"#s791\"} where var\"#s791\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitLowerTriangular{var\"#s790\", var\"#s789\"} where {var\"#s790\", var\"#s789\"<:(Adjoint{T, var\"#s788\"} where var\"#s788\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:(Transpose{T, var\"#s794\"} where var\"#s794\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s793\", var\"#s792\"} where {var\"#s793\", var\"#s792\"<:(Adjoint{T, var\"#s791\"} where var\"#s791\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UpperTriangular{var\"#s790\", var\"#s789\"} where {var\"#s790\", var\"#s789\"<:(Adjoint{T, var\"#s788\"} where var\"#s788\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:(Transpose{T, var\"#s794\"} where var\"#s794\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s793\", var\"#s792\"} where {var\"#s793\", var\"#s792\"<:(Adjoint{T, var\"#s791\"} where var\"#s791\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "UnitUpperTriangular{var\"#s790\", var\"#s789\"} where {var\"#s790\", var\"#s789\"<:(Adjoint{T, var\"#s788\"} where var\"#s788\"<:(StridedMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitUpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "LowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitLowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Adjoint}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UpperTriangular{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitUpperTriangular{var\"#s808\", var\"#s807\"} where {var\"#s808\", var\"#s807\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "LowerTriangular{var\"#s802\", var\"#s801\"} where {var\"#s802\", var\"#s801\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "UnitLowerTriangular{var\"#s796\", var\"#s795\"} where {var\"#s796\", var\"#s795\"<:Transpose}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T} where T", "UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedVecOrMat{T}", "LinearAlgebra.QRCompactWYQ{T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedVecOrMat{T}", "LinearAlgebra.QRPackedQ{T, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "Q"],
      "arg_types": ["StridedMatrix{T} where T", "LinearAlgebra.QRPackedQ"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.QRCompactWYQ{T, M} where M<:AbstractMatrix{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.QRCompactWYQ{T, M} where M<:AbstractMatrix{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.QRPackedQ{T, S} where S<:AbstractMatrix{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedVecOrMat{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.QRPackedQ{T, S} where S<:AbstractMatrix{T})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjQ"],
      "arg_types": [
        "StridedMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.QRPackedQ}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "StridedMatrix{T}",
        "LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T)"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.LQPackedQ{T, S} where S<:(AbstractMatrix{T} where T))}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "D"],
      "arg_types": ["Union{LowerTriangular, UpperTriangular}", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "Q"],
      "arg_types": [
        "StridedMatrix{T}",
        "LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", false} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "adjQ"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", false} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "Q"],
      "arg_types": [
        "StridedMatrix{T}",
        "LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", true} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "adjQ"],
      "arg_types": [
        "StridedMatrix{T}",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:(LinearAlgebra.HessenbergQ{T, var\"#s814\", var\"#s813\", true} where {var\"#s814\"<:StridedMatrix{T}, var\"#s813\"<:StridedVector{T}})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["F", "x"],
      "arg_types": [
        "Hessenberg{var\"#s814\", var\"#s813\", S, W, V} where {var\"#s814\", var\"#s813\"<:(UpperHessenberg{T, S} where S<:AbstractMatrix{T}), S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), V<:Number}",
        "T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["F", "x"],
      "arg_types": [
        "Hessenberg{var\"#s814\", var\"#s813\", S, W, V} where {var\"#s814\", var\"#s813\"<:(SymTridiagonal{T, V} where V<:AbstractVector{T}), S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), V<:Number}",
        "T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "LinearAlgebra.AbstractTriangular",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Union{LinearAlgebra.QRCompactWYQ, LinearAlgebra.QRPackedQ}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjB"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "transB"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Transpose{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:Diagonal}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "J"],
      "arg_types": ["AbstractMatrix{T} where T", "UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "G"],
      "arg_types": ["AbstractMatrix{T} where T", "LinearAlgebra.Givens"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "adjR"],
      "arg_types": [
        "AbstractMatrix{T} where T",
        "Adjoint{var\"#s814\", var\"#s813\"} where {var\"#s814\", var\"#s813\"<:LinearAlgebra.Rotation}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "D"],
      "arg_types": ["UnitUpperTriangular", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["H", "x"],
      "arg_types": ["UpperHessenberg", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "LowerTriangular",
        "Union{LowerTriangular, UnitLowerTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "Q"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:StridedMatrix{T}",
        "LinearAlgebra.HessenbergQ{T, S, W, sym} where {S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), sym}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "adjQ"],
      "arg_types": [
        "Adjoint{T, var\"#s814\"} where var\"#s814\"<:StridedMatrix{T}",
        "Adjoint{var\"#s813\", var\"#s812\"} where {var\"#s813\", var\"#s812\"<:(LinearAlgebra.HessenbergQ{T, S, W, sym} where {S<:(AbstractMatrix{T} where T), W<:(AbstractVector{T} where T), sym})}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "B"],
      "arg_types": [
        "UpperTriangular",
        "Union{UnitUpperTriangular, UpperTriangular}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["X", "s"],
      "arg_types": ["AbstractArray", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n22 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "D"],
      "arg_types": ["UnitLowerTriangular", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rmul!",
      "arg_names": ["A", "D"],
      "arg_types": ["AbstractMatrix{T} where T", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = LinearAlgebra.UpperTriangular([1 2; 0 3]);\n\njulia> LinearAlgebra.rmul!(A, B);\n\njulia> A\n22 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n22 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
    },
    {
      "name": "rotate!",
      "arg_names": ["x", "y", "c", "s"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractVector{T} where T",
        "",
        ""
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nrotate!(x, y, c, s)\n```\n\nOverwrite `x` with `c*x + s*y` and `y` with `-conj(s)*x + c*y`. Returns `x` and `y`.\n\n!!! compat \"Julia 1.5\"\n    `rotate!` requires at least Julia 1.5.\n\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": [
        "Union{UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}, UpperTriangular{T, S} where S<:AbstractMatrix{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": [
        "Union{LowerTriangular{T, S} where S<:AbstractMatrix{T}, UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": ["Bidiagonal{T, V} where V<:AbstractVector{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{TA}", "StridedMatrix{TB}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur(A::StridedMatrix) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F../F.`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.`, and `F.`.\n"
    },
    {
      "name": "schur!",
      "arg_names": ["A"],
      "arg_types": [
        "StridedMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur!(A::StridedMatrix) -> F::Schur\n```\n\nSame as [`schur`](@ref) but uses the input argument `A` as workspace.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n22 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur!(A)\nSchur{Float64, Matrix{Float64}}\nT factor:\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n22 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> A\n22 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\n```\n"
    },
    {
      "name": "schur!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nschur!(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nSame as [`schur`](@ref) but uses the input matrices `A` and `B` as workspace.\n"
    },
    {
      "name": "svd",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": ["StridedVecOrMat{T}"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{TA}", "StridedMatrix{TB}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A", "full"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "Bool"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": ["kwargs..."],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["x"],
      "arg_types": ["Integer"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["x", "y"],
      "arg_types": ["Number", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": ["kw..."],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": ["Transpose"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": ["Adjoint"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd",
      "arg_names": ["A"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For a $M \\times N$ matrix `A`, in the full factorization `U` is `M \\times M` and `V` is `N \\times N`, while in the thin factorization `U` is `M \\times K` and `V` is `N \\times K`, where `K = \\min(M,N)` is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A  F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A  U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B]  [U*C; V*S]*H\ntrue\n\njulia> [A; B]  [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
    },
    {
      "name": "svd!",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": ["kwargs..."],
      "module": "LinearAlgebra",
      "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n\n```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
    },
    {
      "name": "svd!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": ["full", "alg"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n"
    },
    {
      "name": "svd!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
    },
    {
      "name": "svd!",
      "arg_names": ["M"],
      "arg_types": [
        "Bidiagonal{var\"#s811\", V} where {var\"#s811\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s811\"}}"
      ],
      "kwarg_names": ["full"],
      "module": "LinearAlgebra",
      "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n\n```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["A"],
      "arg_types": ["LinearAlgebra.AbstractTriangular"],
      "kwarg_names": ["kwargs..."],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["S"],
      "arg_types": [
        "SVD{var\"#s814\", T, M} where {var\"#s814\", M<:(AbstractArray{var\"#s814\", N} where N)}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{TA}", "StridedMatrix{TB}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["x", "y"],
      "arg_types": ["Number", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["A"],
      "arg_types": [
        "AbstractMatrix{var\"#s814\"} where var\"#s814\"<:Union{Float32, Float64, ComplexF32, ComplexF64}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n45 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n22 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n22 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
    },
    {
      "name": "svdvals!",
      "arg_names": ["A"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref). ```\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
    },
    {
      "name": "svdvals!",
      "arg_names": ["A"],
      "arg_types": ["StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref). ```\n"
    },
    {
      "name": "svdvals!",
      "arg_names": ["A", "B"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
    },
    {
      "name": "svdvals!",
      "arg_names": ["A"],
      "arg_types": [
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref). ```\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
    },
    {
      "name": "svdvals!",
      "arg_names": ["M"],
      "arg_types": [
        "Bidiagonal{var\"#s814\", V} where {var\"#s814\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref). ```\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
    },
    {
      "name": "sylvester",
      "arg_names": ["A", "B", "C"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n22 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n22 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B + C\n22 Matrix{Float64}:\n  2.66454e-15  1.77636e-15\n -3.77476e-15  4.44089e-16\n```\n"
    },
    {
      "name": "sylvester",
      "arg_names": ["A", "B", "C"],
      "arg_types": ["StridedMatrix{T}", "StridedMatrix{T}", "StridedMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n22 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n22 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B + C\n22 Matrix{Float64}:\n  2.66454e-15  1.77636e-15\n -3.77476e-15  4.44089e-16\n```\n"
    },
    {
      "name": "sylvester",
      "arg_names": ["a", "b", "c"],
      "arg_types": [
        "Union{Real, Complex}",
        "Union{Real, Complex}",
        "Union{Real, Complex}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n22 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n22 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n22 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n22 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B + C\n22 Matrix{Float64}:\n  2.66454e-15  1.77636e-15\n -3.77476e-15  4.44089e-16\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["x"],
      "arg_types": ["Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["A"],
      "arg_types": ["Matrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "tr",
      "arg_names": ["A"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n22 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["D"],
      "arg_types": [
        "Diagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": [
        "Hermitian{var\"#s814\", S} where {var\"#s814\"<:Real, S<:(AbstractMatrix{var\"#s814\"} where var\"#s814\"<:var\"#s814\")}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["B"],
      "arg_types": [
        "Bidiagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["B"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["S"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["J"],
      "arg_types": ["UniformScaling"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["F"],
      "arg_types": ["LU"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["Transpose"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["B"],
      "arg_types": ["Union{BitMatrix, BitVector}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["R"],
      "arg_types": ["LinearAlgebra.AbstractRotation"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["Adjoint{var\"#s814\", S} where {var\"#s814\"<:Real, S}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["S"],
      "arg_types": [
        "Tridiagonal{var\"#s814\", V} where {var\"#s814\"<:Number, V<:AbstractVector{var\"#s814\"}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["S"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["A"],
      "arg_types": ["AbstractVecOrMat{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose",
      "arg_names": ["a"],
      "arg_types": ["AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> transpose(A)\n22 transpose(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3+2im  8+7im\n 9+2im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["C", "B"],
      "arg_types": ["BitMatrix", "BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractVector{T} where T", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "transpose!",
      "arg_names": ["B", "A"],
      "arg_types": ["AbstractMatrix{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n22 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n22 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n22 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["M", "k"],
      "arg_types": ["Matrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M, k::Integer)\n```\n\nReturns the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n44 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["A", "k"],
      "arg_types": ["Hermitian", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M, k::Integer)\n```\n\nReturns the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n44 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["A", "k"],
      "arg_types": ["Symmetric", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M, k::Integer)\n```\n\nReturns the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["B"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n44 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["B", "k"],
      "arg_types": ["BitMatrix", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M, k::Integer)\n```\n\nReturns the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["M"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n44 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "tril",
      "arg_names": ["M", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril(M, k::Integer)\n```\n\nReturns the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["D", "k"],
      "arg_types": ["Diagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M", "k"],
      "arg_types": ["Bidiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A", "k"],
      "arg_types": ["LowerTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A", "k"],
      "arg_types": [
        "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}",
        "Integer"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A", "k"],
      "arg_types": ["UpperTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["A", "k"],
      "arg_types": ["UnitLowerTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M", "k"],
      "arg_types": ["Tridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M", "k"],
      "arg_types": ["SymTridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
    },
    {
      "name": "tril!",
      "arg_names": ["M", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n55 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["M", "k"],
      "arg_types": ["Matrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M, k::Integer)\n```\n\nReturns the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["A"],
      "arg_types": ["Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["A", "k"],
      "arg_types": ["Hermitian", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M, k::Integer)\n```\n\nReturns the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["A"],
      "arg_types": ["Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["A", "k"],
      "arg_types": ["Symmetric", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M, k::Integer)\n```\n\nReturns the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["B"],
      "arg_types": ["BitMatrix"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["B", "k"],
      "arg_types": ["BitMatrix", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M, k::Integer)\n```\n\nReturns the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["M"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
    },
    {
      "name": "triu",
      "arg_names": ["M", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu(M, k::Integer)\n```\n\nReturns the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n44 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n44 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["D"],
      "arg_types": ["Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["D", "k"],
      "arg_types": ["Diagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M"],
      "arg_types": ["Bidiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M", "k"],
      "arg_types": ["Bidiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A"],
      "arg_types": ["LowerTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A", "k"],
      "arg_types": ["LowerTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A"],
      "arg_types": ["UnitUpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A", "k"],
      "arg_types": ["UnitUpperTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A"],
      "arg_types": ["UpperTriangular"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A", "k"],
      "arg_types": ["UpperTriangular", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A"],
      "arg_types": ["UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["A", "k"],
      "arg_types": [
        "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}",
        "Integer"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M"],
      "arg_types": ["Tridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M", "k"],
      "arg_types": ["Tridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M"],
      "arg_types": ["SymTridiagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M", "k"],
      "arg_types": ["SymTridiagonal", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M"],
      "arg_types": ["AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
    },
    {
      "name": "triu!",
      "arg_names": ["M", "k"],
      "arg_types": ["AbstractMatrix{T} where T", "Integer"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n55 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n55 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
    },
    {
      "name": "cross",
      "arg_names": ["a", "b"],
      "arg_types": ["AbstractVector{T} where T", "AbstractVector{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ncross(x, y)\n(x,y)\n```\n\nCompute the cross product of two 3-vectors.\n\n# Examples\n\n```jldoctest\njulia> a = [0;1;0]\n3-element Vector{Int64}:\n 0\n 1\n 0\n\njulia> b = [0;0;1]\n3-element Vector{Int64}:\n 0\n 0\n 1\n\njulia> cross(a,b)\n3-element Vector{Int64}:\n 1\n 0\n 0\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": [
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}",
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": [
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}",
        "Union{DenseVector{T}, Base.ReinterpretArray{T, 1, S, A, IsReshaped} where {A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, 1, A, MI} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, SubArray{T, 1, A, I, L} where {A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A, MI} where {T, N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real, N} where N}, Tuple{AbstractUnitRange, Vararg{Any, N} where N}}}, DenseArray}, MI<:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}, N} where N}}, DenseArray}, I<:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex, Base.ReshapedArray{T, N, A, Tuple{}} where {T, N, A<:AbstractUnitRange}}, N} where N}, L}, DenseArray{T, N} where N}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "a", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Union{Real, Complex}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "a", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Number",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "transA", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Transpose{var\"#s814\", S} where {var\"#s814\"<:Real, S}",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "B", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Bidiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "LowerTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UnitUpperTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UpperTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UnitLowerTriangular",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Tridiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "S", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "SymTridiagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "J", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UniformScaling",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["Number", "Number"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Symmetric", "Symmetric"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "H", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "UpperHessenberg",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "adjA", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Adjoint",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "rx", "y", "ry"],
      "arg_types": [
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}",
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "rx", "y", "ry"],
      "arg_types": [
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}",
        "Vector{T}",
        "Union{AbstractRange{TI}, UnitRange{TI}}"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Diagonal", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["AbstractMatrix{T} where T", "Diagonal"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "D", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "Diagonal",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["D", "B"],
      "arg_types": ["Diagonal", "AbstractMatrix{T} where T"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["A", "B"],
      "arg_types": ["Hermitian", "Hermitian"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["BitVector", "BitVector"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": [
        "AbstractVector{T} where T",
        "AbstractMatrix{T} where T",
        "AbstractVector{T} where T"
      ],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["AbstractArray", "AbstractArray"],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "y"],
      "arg_types": ["", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, y)\nx  y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x  y` (where `` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    },
    {
      "name": "dot",
      "arg_names": ["x", "A", "y"],
      "arg_types": ["", "", ""],
      "kwarg_names": [],
      "module": "LinearAlgebra",
      "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> (1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
    }
  ]
}
