{
    "julia": "1.10.7",
    "methods": [
        {
            "name": "/",
            "arg_names": [
                "T",
                "D"
            ],
            "arg_types": [
                "Tridiagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Tridiagonal",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "Array{Complex{T}, 2}",
                "LinearAlgebra.AdjointFactorization{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "Hermitian",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "L"
            ],
            "arg_types": [
                "Bidiagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "L"
            ],
            "arg_types": [
                "Bidiagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "U"
            ],
            "arg_types": [
                "Bidiagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "U"
            ],
            "arg_types": [
                "Bidiagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Bidiagonal",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "L",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "Symmetric",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Adjoint{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "D"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UnitLowerTriangular{<:Any, <:Transpose}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UnitLowerTriangular{<:Any, <:Adjoint}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Transpose{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UnitUpperTriangular{<:Any, <:Transpose}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UnitUpperTriangular{<:Any, <:Adjoint}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UpperTriangular{<:Any, <:Transpose}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "UpperTriangular{<:Any, <:Adjoint}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "LowerTriangular{<:Any, <:Adjoint}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "LowerTriangular{<:Any, <:Transpose}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Union{LowerTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Union{UnitLowerTriangular, UnitUpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "LowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "D"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Transpose{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Adjoint{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Union{LowerTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Union{UnitLowerTriangular, UnitUpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "U",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "U",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "L",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "B"
            ],
            "arg_types": [
                "UpperHessenberg",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "U"
            ],
            "arg_types": [
                "UpperHessenberg",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "U"
            ],
            "arg_types": [
                "UpperHessenberg",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "S",
                "D"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "x"
            ],
            "arg_types": [
                "UpperHessenberg",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Adjoint{T, <:AbstractMatrix} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Adjoint{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "u",
                "A"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Transpose{T, <:AbstractMatrix} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Transpose{<:Any, <:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{UnitLowerTriangular, UnitUpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{LowerTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "Transpose{Complex{T}, Array{Complex{T}, 1}}",
                "LinearAlgebra.TransposeFactorization{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "UpperTriangular",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "LowerTriangular",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "Adjoint{Complex{T}, Array{Complex{T}, 1}}",
                "LinearAlgebra.AdjointFactorization{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "D",
                "x"
            ],
            "arg_types": [
                "Diagonal",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "J",
                "x"
            ],
            "arg_types": [
                "UniformScaling",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "J",
                "A"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "J1",
                "J2"
            ],
            "arg_types": [
                "UniformScaling",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "x"
            ],
            "arg_types": [
                "UpperHessenberg",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "H",
                "x"
            ],
            "arg_types": [
                "UpperHessenberg",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "Union{Array{Complex{T}, 2}, Adjoint{Complex{T}, Array{Complex{T}, 1}}, Transpose{Complex{T}, Array{Complex{T}, 1}}}",
                "LinearAlgebra.TransposeFactorization{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "Union{Array{Complex{T}, 2}, Adjoint{Complex{T}, Array{Complex{T}, 1}}, Transpose{Complex{T}, Array{Complex{T}, 1}}}",
                "Factorization{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "v",
                "J"
            ],
            "arg_types": [
                "AbstractVector",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{UnitLowerTriangular, UnitUpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{LowerTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n\n```\nA / B\n```\n\nMatrix right-division: `A / B` is equivalent to `(B' \\ A')'` where [`\\`](@ref) is the left-division operator. For square matrices, the result `X` is such that `A == X*B`.\n\nSee also: [`rdiv!`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = Float64[1 4 5; 3 9 2]; B = Float64[1 4 2; 3 4 2; 8 7 1];\n\njulia> X = A / B\n2×3 Matrix{Float64}:\n -0.65   3.75  -1.2\n  3.25  -2.75   1.0\n\njulia> isapprox(A, X*B)\ntrue\n\njulia> isapprox(X, A*pinv(B))\ntrue\n```\n"
        },
        {
            "name": "/",
            "arg_names": [
                "A",
                "Q"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n/(x, y)\n```\n\nRight division operator: multiplication of `x` by the inverse of `y` on the right. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 1/2\n0.5\n\njulia> 4/2\n2.0\n\njulia> 4.5/2\n2.25\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "J"
            ],
            "arg_types": [
                "Factorization",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{T, <:LU{T, <:StridedMatrix{T} where T}}",
                "Adjoint{T, <:StridedVecOrMat{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{T, <:LU}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{T, <:LU{T, <:StridedMatrix{T} where T}}",
                "Transpose{T, <:StridedVecOrMat{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "BIn"
            ],
            "arg_types": [
                "Union{QR{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}, LinearAlgebra.QRCompactWY{T, M, C} where {M<:AbstractMatrix{T}, C<:AbstractMatrix{T}}, QRPivoted{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Union{Cholesky{T, S}, BunchKaufman{T, S}, LQ{T, S, C} where C<:AbstractVector{T}, LU{T, S}, QR{T, S, C} where C<:AbstractVector{T}, LinearAlgebra.QRCompactWY{T, S, C} where C<:AbstractMatrix{T}, QRPivoted{T, S, C} where C<:AbstractVector{T}, SVD{T, var\"#s997\", S, C} where {var\"#s997\"<:Real, C<:AbstractVector{var\"#s997\"}}} where S",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Factorization{T}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{T, <:Union{Cholesky{T, S}, BunchKaufman{T, S}, LQ{T, S, C} where C<:AbstractVector{T}, LU{T, S}, QR{T, S, C} where C<:AbstractVector{T}, LinearAlgebra.QRCompactWY{T, S, C} where C<:AbstractMatrix{T}, QRPivoted{T, S, C} where C<:AbstractVector{T}, SVD{T, var\"#s997\", S, C} where {var\"#s997\"<:Real, C<:AbstractVector{var\"#s997\"}}} where {T, S}}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{T}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:Union{Cholesky{T, S}, BunchKaufman{T, S}, LQ{T, S, C} where C<:AbstractVector{T}, LU{T, S}, QR{T, S, C} where C<:AbstractVector{T}, LinearAlgebra.QRCompactWY{T, S, C} where C<:AbstractMatrix{T}, QRPivoted{T, S, C} where C<:AbstractVector{T}, SVD{T, var\"#s997\", S, C} where {var\"#s997\"<:Real, C<:AbstractVector{var\"#s997\"}}} where {T, S}}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{<:Any, <:LU}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{T}",
                "Union{Array{Complex{T}, 1}, Array{Complex{T}, 2}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Union{Cholesky{T, S}, BunchKaufman{T, S}, LQ{T, S, C} where C<:AbstractVector{T}, LU{T, S}, QR{T, S, C} where C<:AbstractVector{T}, LinearAlgebra.QRCompactWY{T, S, C} where C<:AbstractMatrix{T}, QRPivoted{T, S, C} where C<:AbstractVector{T}, SVD{T, var\"#s997\", S, C} where {var\"#s997\"<:Real, C<:AbstractVector{var\"#s997\"}}} where {T, S}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Factorization",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "L",
                "B"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "xA",
                "B"
            ],
            "arg_types": [
                "Union{Adjoint{var\"#s997\", var\"#s128\"}, Transpose{var\"#s997\", var\"#s128\"}} where {var\"#s997\", var\"#s128\"<:Bidiagonal}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Adjoint{<:Any, <:Tridiagonal}",
                "Adjoint{<:Any, <:AbstractVecOrMat}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "u",
                "v"
            ],
            "arg_types": [
                "Union{Adjoint{T, var\"#s997\"}, Transpose{T, var\"#s997\"}} where {T, var\"#s997\"<:(AbstractVector)}",
                "Union{Adjoint{T, var\"#s997\"}, Transpose{T, var\"#s997\"}} where {T, var\"#s997\"<:(AbstractVector)}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "Number",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "Number",
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "D"
            ],
            "arg_types": [
                "Number",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "J"
            ],
            "arg_types": [
                "Number",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "Number",
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "A"
            ],
            "arg_types": [
                "Number",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "H"
            ],
            "arg_types": [
                "Number",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "A"
            ],
            "arg_types": [
                "Number",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "A"
            ],
            "arg_types": [
                "Number",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "A"
            ],
            "arg_types": [
                "Number",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "U",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "U",
                "H"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "U",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "U",
                "H"
            ],
            "arg_types": [
                "UpperTriangular",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "Q",
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "L",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "T"
            ],
            "arg_types": [
                "Diagonal",
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "S"
            ],
            "arg_types": [
                "Diagonal",
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "H"
            ],
            "arg_types": [
                "Diagonal",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "x",
                "H"
            ],
            "arg_types": [
                "UniformScaling",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "J",
                "A"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "J1",
                "J2"
            ],
            "arg_types": [
                "UniformScaling",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "T",
                "B"
            ],
            "arg_types": [
                "SymTridiagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "H"
            ],
            "arg_types": [
                "Bidiagonal",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "D"
            ],
            "arg_types": [
                "Bidiagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "L"
            ],
            "arg_types": [
                "Bidiagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "L"
            ],
            "arg_types": [
                "Bidiagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "U"
            ],
            "arg_types": [
                "Bidiagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "B",
                "U"
            ],
            "arg_types": [
                "Bidiagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Bidiagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{LowerTriangular, UpperTriangular}",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{UnitLowerTriangular, UnitUpperTriangular}",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{LowerTriangular, UpperTriangular}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{UnitLowerTriangular, UnitUpperTriangular}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n\n```\n\\(A, B)\n```\n\nMatrix division using a polyalgorithm. For input matrices `A` and `B`, the result `X` is such that `A*X == B` when `A` is square. The solver that is used depends upon the structure of `A`.  If `A` is upper or lower triangular (or diagonal), no factorization of `A` is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used.\n\nFor rectangular `A` the result is the minimum-norm least squares solution computed by a pivoted QR factorization of `A` and a rank estimate of `A` based on the R factor.\n\nWhen `A` is sparse, a similar polyalgorithm is used. For indefinite matrices, the `LDLt` factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.\n\nSee also: [`factorize`](@ref), [`pinv`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 1 -2]; B = [32; -4];\n\njulia> X = A \\ B\n2-element Vector{Float64}:\n 32.0\n 18.0\n\njulia> A * X == B\ntrue\n```\n"
        },
        {
            "name": "\\",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\n\\(x, y)\n```\n\nLeft division operator: multiplication of `y` by the inverse of `x` on the left. Gives floating-point results for integer arguments.\n\n# Examples\n\n```jldoctest\njulia> 3 \\ 6\n2.0\n\njulia> inv(3) * 6\n2.0\n\njulia> A = [4 3; 2 1]; x = [5, 6];\n\njulia> A \\ x\n2-element Vector{Float64}:\n  6.5\n -7.0\n\njulia> inv(A) * x\n2-element Vector{Float64}:\n  6.5\n -7.0\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal{var\"#s997\", V} where {var\"#s997\"<:Real, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LU{var\"#s997\", S} where {var\"#s997\"<:Real, S<:AbstractMatrix{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LDLt{<:Real, <:SymTridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LDLt"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "SVD"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BunchKaufman"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Union{Cholesky, CholeskyPivoted}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "G"
            ],
            "arg_types": [
                "LinearAlgebra.Givens"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric{var\"#s997\", S} where {var\"#s997\"<:Real, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "adjR"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointRotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "R"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractRotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal{var\"#s997\", V} where {var\"#s997\"<:Real, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "adjQ"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Union{BitMatrix, BitVector}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Bidiagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{<:Number, <:Base.ReshapedArray{<:Number, 1, <:Adjoint}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n"
        },
        {
            "name": "adjoint",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nA'\nadjoint(A)\n```\n\nLazy adjoint (conjugate transposition). Note that `adjoint` is applied recursively to elements.\n\nFor number types, `adjoint` returns the complex conjugate, and therefore it is equivalent to the identity function for real numbers.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims).\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 0  0]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B = A' # equivalently adjoint(A)\n2×2 adjoint(::Matrix{Complex{Int64}}) with eltype Complex{Int64}:\n 3-2im  0+0im\n 9-2im  0+0im\n\njulia> B isa Adjoint\ntrue\n\njulia> adjoint(B) === A # the adjoint of an adjoint unwraps the parent\ntrue\n\njulia> Adjoint(B) # however, the constructor always wraps its argument\n2×2 adjoint(adjoint(::Matrix{Complex{Int64}})) with eltype Complex{Int64}:\n 3+2im  9+2im\n 0+0im  0+0im\n\njulia> B[1,2] = 4 + 5im; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 4-5im  0+0im\n```\n\nFor real matrices, the `adjoint` operation is equivalent to a `transpose`.\n\n```jldoctest\njulia> A = reshape([x for x in 1:4], 2, 2)\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> A'\n2×2 adjoint(::Matrix{Int64}) with eltype Int64:\n 1  2\n 3  4\n\njulia> adjoint(A) == transpose(A)\ntrue\n```\n\nThe adjoint of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> x = [3, 4im]\n2-element Vector{Complex{Int64}}:\n 3 + 0im\n 0 + 4im\n\njulia> x'\n1×2 adjoint(::Vector{Complex{Int64}}) with eltype Complex{Int64}:\n 3+0im  0-4im\n\njulia> x'x # compute the dot product, equivalently x' * x\n25 + 0im\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> A = reshape([x + im*x for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> C = reshape([A, 2A, 3A, 4A], 2, 2)\n2×2 Matrix{Matrix{Complex{Int64}}}:\n [1+1im 3+3im; 2+2im 4+4im]  [3+3im 9+9im; 6+6im 12+12im]\n [2+2im 6+6im; 4+4im 8+8im]  [4+4im 12+12im; 8+8im 16+16im]\n\njulia> C'\n2×2 adjoint(::Matrix{Matrix{Complex{Int64}}}) with eltype Adjoint{Complex{Int64}, Matrix{Complex{Int64}}}:\n [1-1im 2-2im; 3-3im 4-4im]    [2-2im 4-4im; 6-6im 8-8im]\n [3-3im 6-6im; 9-9im 12-12im]  [4-4im 8-8im; 12-12im 16-16im]\n```\n\n```\nadjoint(F::Factorization)\n```\n\nLazy adjoint of the factorization `F`. By default, returns an [`AdjointFactorization`](@ref) wrapper.\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n\n```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(dest,src)\n```\n\nConjugate transpose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> adjoint!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3-2im  8-7im\n 9-2im  4-6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n\n```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "adjoint!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nadjoint!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores the adjoint of the elements in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "axpby!",
            "arg_names": [
                "α",
                "x",
                "β",
                "y"
            ],
            "arg_types": [
                "Number",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Number",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpby!(α, x::AbstractArray, β, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y * β` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .= x .* a .+ y .* β`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpby!(2, x, 2, y)\n3-element Vector{Int64}:\n 10\n 14\n 18\n```\n"
        },
        {
            "name": "axpby!",
            "arg_names": [
                "α",
                "x",
                "β",
                "y"
            ],
            "arg_types": [
                "",
                "AbstractArray",
                "",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpby!(α, x::AbstractArray, β, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y * β` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .= x .* a .+ y .* β`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpby!(2, x, 2, y)\n3-element Vector{Int64}:\n 10\n 14\n 18\n```\n"
        },
        {
            "name": "axpy!",
            "arg_names": [
                "α",
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "Number",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "AbstractRange{<:Integer}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "AbstractRange{<:Integer}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpy!(α, x::AbstractArray, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .+= x .* a`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
        },
        {
            "name": "axpy!",
            "arg_names": [
                "α",
                "x",
                "y"
            ],
            "arg_types": [
                "Number",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpy!(α, x::AbstractArray, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .+= x .* a`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
        },
        {
            "name": "axpy!",
            "arg_names": [
                "α",
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "",
                "AbstractArray",
                "AbstractArray{<:Integer}",
                "AbstractArray",
                "AbstractArray{<:Integer}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpy!(α, x::AbstractArray, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .+= x .* a`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
        },
        {
            "name": "axpy!",
            "arg_names": [
                "α",
                "x",
                "y"
            ],
            "arg_types": [
                "",
                "AbstractArray",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\naxpy!(α, x::AbstractArray, y::AbstractArray)\n```\n\nOverwrite `y` with `x * α + y` and return `y`. If `x` and `y` have the same axes, it's equivalent with `y .+= x .* a`.\n\n# Examples\n\n```jldoctest\njulia> x = [1; 2; 3];\n\njulia> y = [4; 5; 6];\n\njulia> axpy!(2, x, y)\n3-element Vector{Int64}:\n  6\n  9\n 12\n```\n"
        },
        {
            "name": "bunchkaufman",
            "arg_names": [
                "A",
                "rook"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "Bool"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman(A, rook::Bool=false; check = true) -> S::BunchKaufman\n```\n\nCompute the Bunch-Kaufman [^Bunch1977] factorization of a symmetric or Hermitian matrix `A` as `P'*U*D*U'*P` or `P'*L*D*L'*P`, depending on which triangle is stored in `A`, and return a [`BunchKaufman`](@ref) object. Note that if `A` is complex symmetric then `U'` and `L'` denote the unconjugated transposes, i.e. `transpose(U)` and `transpose(L)`.\n\nIterating the decomposition produces the components `S.D`, `S.U` or `S.L` as appropriate given `S.uplo`, and `S.p`.\n\nIf `rook` is `true`, rook pivoting is used. If `rook` is false, rook pivoting is not used.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe following functions are available for `BunchKaufman` objects: [`size`](@ref), `\\`, [`inv`](@ref), [`issymmetric`](@ref), [`ishermitian`](@ref), [`getindex`](@ref).\n\n[^Bunch1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179. [url](http://www.ams.org/journals/mcom/1977-31-137/S0025-5718-1977-0428694-0/).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 3]\n2×2 Matrix{Int64}:\n 1  2\n 2  3\n\njulia> S = bunchkaufman(A) # A gets wrapped internally by Symmetric(A)\nBunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}\nD factor:\n2×2 Tridiagonal{Float64, Vector{Float64}}:\n -0.333333  0.0\n  0.0       3.0\nU factor:\n2×2 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.666667\n  ⋅   1.0\npermutation:\n2-element Vector{Int64}:\n 1\n 2\n\njulia> d, u, p = S; # destructuring via iteration\n\njulia> d == S.D && u == S.U && p == S.p\ntrue\n\njulia> S.U*S.D*S.U' - S.P*A*S.P'\n2×2 Matrix{Float64}:\n 0.0  0.0\n 0.0  0.0\n\njulia> S = bunchkaufman(Symmetric(A, :L))\nBunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}\nD factor:\n2×2 Tridiagonal{Float64, Vector{Float64}}:\n 3.0   0.0\n 0.0  -0.333333\nL factor:\n2×2 UnitLowerTriangular{Float64, Matrix{Float64}}:\n 1.0        ⋅\n 0.666667  1.0\npermutation:\n2-element Vector{Int64}:\n 2\n 1\n\njulia> S.L*S.D*S.L' - A[S.p, S.p]\n2×2 Matrix{Float64}:\n 0.0  0.0\n 0.0  0.0\n```\n"
        },
        {
            "name": "bunchkaufman",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman(A, rook::Bool=false; check = true) -> S::BunchKaufman\n```\n\nCompute the Bunch-Kaufman [^Bunch1977] factorization of a symmetric or Hermitian matrix `A` as `P'*U*D*U'*P` or `P'*L*D*L'*P`, depending on which triangle is stored in `A`, and return a [`BunchKaufman`](@ref) object. Note that if `A` is complex symmetric then `U'` and `L'` denote the unconjugated transposes, i.e. `transpose(U)` and `transpose(L)`.\n\nIterating the decomposition produces the components `S.D`, `S.U` or `S.L` as appropriate given `S.uplo`, and `S.p`.\n\nIf `rook` is `true`, rook pivoting is used. If `rook` is false, rook pivoting is not used.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe following functions are available for `BunchKaufman` objects: [`size`](@ref), `\\`, [`inv`](@ref), [`issymmetric`](@ref), [`ishermitian`](@ref), [`getindex`](@ref).\n\n[^Bunch1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179. [url](http://www.ams.org/journals/mcom/1977-31-137/S0025-5718-1977-0428694-0/).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 3]\n2×2 Matrix{Int64}:\n 1  2\n 2  3\n\njulia> S = bunchkaufman(A) # A gets wrapped internally by Symmetric(A)\nBunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}\nD factor:\n2×2 Tridiagonal{Float64, Vector{Float64}}:\n -0.333333  0.0\n  0.0       3.0\nU factor:\n2×2 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.666667\n  ⋅   1.0\npermutation:\n2-element Vector{Int64}:\n 1\n 2\n\njulia> d, u, p = S; # destructuring via iteration\n\njulia> d == S.D && u == S.U && p == S.p\ntrue\n\njulia> S.U*S.D*S.U' - S.P*A*S.P'\n2×2 Matrix{Float64}:\n 0.0  0.0\n 0.0  0.0\n\njulia> S = bunchkaufman(Symmetric(A, :L))\nBunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}\nD factor:\n2×2 Tridiagonal{Float64, Vector{Float64}}:\n 3.0   0.0\n 0.0  -0.333333\nL factor:\n2×2 UnitLowerTriangular{Float64, Matrix{Float64}}:\n 1.0        ⋅\n 0.666667  1.0\npermutation:\n2-element Vector{Int64}:\n 2\n 1\n\njulia> S.L*S.D*S.L' - A[S.p, S.p]\n2×2 Matrix{Float64}:\n 0.0  0.0\n 0.0  0.0\n```\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A",
                "rook"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s124\"} where var\"#s124\"<:Union{Float32, Float64, ComplexF64, ComplexF32}",
                "Bool"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s125\"} where var\"#s125\"<:Union{Float32, Float64, ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A",
                "rook"
            ],
            "arg_types": [
                "Hermitian{<:Union{ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}",
                "Bool"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A",
                "rook"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s121\", var\"#s120\"}, Symmetric{var\"#s121\", var\"#s120\"}, Symmetric{Complex{var\"#s121\"}, var\"#s120\"}} where {var\"#s121\"<:Union{Float32, Float64}, var\"#s120\"<:(StridedMatrix{T} where T)}",
                "Bool"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian{<:Union{ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "bunchkaufman!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s123\", var\"#s122\"}, Symmetric{var\"#s123\", var\"#s122\"}, Symmetric{Complex{var\"#s123\"}, var\"#s122\"}} where {var\"#s123\"<:Union{Float32, Float64}, var\"#s122\"<:(StridedMatrix{T} where T)}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nbunchkaufman!(A, rook::Bool=false; check = true) -> BunchKaufman\n```\n\n`bunchkaufman!` is the same as [`bunchkaufman`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Diagonal",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "x",
                "uplo"
            ],
            "arg_types": [
                "Number",
                "Symbol"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix{Float16}",
                "RowMaximum"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "RowMaximum"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s123\", var\"#s122\"}, Hermitian{Complex{var\"#s123\"}, var\"#s122\"}, Symmetric{var\"#s123\", var\"#s122\"}} where {var\"#s123\"<:Real, var\"#s122\"<:SymTridiagonal}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{Float16}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "S",
                ""
            ],
            "arg_types": [
                "SymTridiagonal",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "S",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s121\", var\"#s120\"}, Hermitian{Complex{var\"#s121\"}, var\"#s120\"}, Symmetric{var\"#s121\", var\"#s120\"}} where {var\"#s121\"<:Real, var\"#s120\"<:SymTridiagonal}",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix{Float16}",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{<:Real, <:StridedMatrix{T} where T}, Hermitian{Complex{var\"#s124\"}, <:StridedMatrix{T} where T} where var\"#s124\"<:Real, Symmetric{<:Real, <:StridedMatrix{T} where T}, StridedMatrix}",
                "Val{true}"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{ComplexF16, <:StridedMatrix{T} where T}, Hermitian{Float16, <:StridedMatrix{T} where T}, Symmetric{Float16, <:StridedMatrix{T} where T}, StridedMatrix{Float16}}",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{<:Real, <:StridedMatrix{T} where T}, Hermitian{Complex{var\"#s124\"}, <:StridedMatrix{T} where T} where var\"#s124\"<:Real, Symmetric{<:Real, <:StridedMatrix{T} where T}, StridedMatrix}",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky(A, NoPivot(); check = true) -> Cholesky\n```\n\nCompute the Cholesky factorization of a dense symmetric positive definite matrix `A` and return a [`Cholesky`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, where `A ≈ F.U' * F.U ≈ F.L * F.L'`.\n\nThe following functions are available for `Cholesky` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), [`logdet`](@ref) and [`isposdef`](@ref).\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 12. -16.; 12. 37. -43.; -16. -43. 98.]\n3×3 Matrix{Float64}:\n   4.0   12.0  -16.0\n  12.0   37.0  -43.0\n -16.0  -43.0   98.0\n\njulia> C = cholesky(A)\nCholesky{Float64, Matrix{Float64}}\nU factor:\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.U\n3×3 UpperTriangular{Float64, Matrix{Float64}}:\n 2.0  6.0  -8.0\n  ⋅   1.0   5.0\n  ⋅    ⋅    3.0\n\njulia> C.L\n3×3 LowerTriangular{Float64, Matrix{Float64}}:\n  2.0   ⋅    ⋅\n  6.0  1.0   ⋅\n -8.0  5.0  3.0\n\njulia> C.L * C.U == A\ntrue\n```\n\n```\ncholesky(A, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nCompute the pivoted Cholesky factorization of a dense symmetric positive semi-definite matrix `A` and return a [`CholeskyPivoted`](@ref) factorization. The matrix `A` can either be a [`Symmetric`](@ref) or [`Hermitian`](@ref) [`AbstractMatrix`](@ref) or a *perfectly* symmetric or Hermitian `AbstractMatrix`.\n\nThe triangular Cholesky factor can be obtained from the factorization `F` via `F.L` and `F.U`, and the permutation via `F.p`, where `A[F.p, F.p] ≈ Ur' * Ur ≈ Lr * Lr'` with `Ur = F.U[1:F.rank, :]` and `Lr = F.L[:, 1:F.rank]`, or alternatively `A ≈ Up' * Up ≈ Lp * Lp'` with `Up = F.U[1:F.rank, invperm(F.p)]` and `Lp = F.L[invperm(F.p), 1:F.rank]`.\n\nThe following functions are available for `CholeskyPivoted` objects: [`size`](@ref), [`\\`](@ref), [`inv`](@ref), [`det`](@ref), and [`rank`](@ref).\n\nThe argument `tol` determines the tolerance for determining the rank. For negative values, the tolerance is the machine precision.\n\nIf you have a matrix `A` that is slightly non-Hermitian due to roundoff errors in its construction, wrap it in `Hermitian(A)` before passing it to `cholesky` in order to treat it as perfectly Hermitian.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\n# Examples\n\n```jldoctest\njulia> X = [1.0, 2.0, 3.0, 4.0];\n\njulia> A = X * X';\n\njulia> C = cholesky(A, RowMaximum(), check = false)\nCholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}\nU factor with rank 1:\n4×4 UpperTriangular{Float64, Matrix{Float64}}:\n 4.0  2.0  3.0  1.0\n  ⋅   0.0  6.0  2.0\n  ⋅    ⋅   9.0  3.0\n  ⋅    ⋅    ⋅   1.0\npermutation:\n4-element Vector{Int64}:\n 4\n 2\n 3\n 1\n\njulia> C.U[1:C.rank, :]' * C.U[1:C.rank, :] ≈ A[C.p, C.p]\ntrue\n\njulia> l, u = C; # destructuring via iteration\n\njulia> l == C.L && u == C.U\ntrue\n```\n\n```\ncholesky(A::SparseMatrixCSC; shift = 0.0, check = true, perm = nothing) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky factorization of a sparse positive definite matrix `A`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/[`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian. If `perm` is not given, a fill-reducing permutation is used. `F = cholesky(A)` is most frequently used to solve systems of equations with `F\\b`, but also the methods [`diag`](@ref), [`det`](@ref), and [`logdet`](@ref) are defined for `F`. You can also extract individual factors from `F`, using `F.L`. However, since pivoting is on by default, the factorization is internally represented as `A == P'*L*L'*P` with a permutation matrix `P`; using just `L` without accounting for `P` will give incorrect answers. To include the effects of permutation, it's typically preferable to extract \"combined\" factors like `PtL = F.PtL` (the equivalent of `P'*L`) and `LtP = F.UP` (the equivalent of `L'*P`).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nSetting the optional `shift` keyword argument computes the factorization of `A+shift*I` instead of `A`. If the `perm` argument is provided, it should be a permutation of `1:size(A,1)` giving the ordering to use (instead of CHOLMOD's default AMD ordering).\n\n# Examples\n\nIn the following example, the fill-reducing permutation used is `[3, 2, 1]`. If `perm` is set to `1:3` to enforce no permutation, the number of nonzero elements in the factor is 6.\n\n```jldoctest\njulia> A = [2 1 1; 1 2 0; 1 0 2]\n3×3 Matrix{Int64}:\n 2  1  1\n 1  2  0\n 1  0  2\n\njulia> C = cholesky(sparse(A))\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  5\nnnz:     5\nsuccess: true\n\njulia> C.p\n3-element Vector{Int64}:\n 3\n 2\n 1\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421   0.0       0.0\n 0.0       1.41421   0.0\n 0.707107  0.707107  1.0\n\njulia> L * L' ≈ A[C.p, C.p]\ntrue\n\njulia> P = sparse(1:3, C.p, ones(3))\n3×3 SparseMatrixCSC{Float64, Int64} with 3 stored entries:\n  ⋅    ⋅   1.0\n  ⋅   1.0   ⋅\n 1.0   ⋅    ⋅\n\njulia> P' * L * L' * P ≈ A\ntrue\n\njulia> C = cholesky(sparse(A), perm=1:3)\nSparseArrays.CHOLMOD.Factor{Float64, Int64}\ntype:    LLt\nmethod:  simplicial\nmaxnnz:  6\nnnz:     6\nsuccess: true\n\njulia> L = sparse(C.L);\n\njulia> Matrix(L)\n3×3 Matrix{Float64}:\n 1.41421    0.0       0.0\n 0.707107   1.22474   0.0\n 0.707107  -0.408248  1.1547\n\njulia> L * L' ≈ A\ntrue\n```\n\n!!! note\n    This method uses the CHOLMOD[^ACM887][^DavisHager2009] library from [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). CHOLMOD only supports double or complex double element types. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n    Many other functions from CHOLMOD are wrapped but not exported from the `Base.SparseArrays.CHOLMOD` module.\n\n\n[^ACM887]: Chen, Y., Davis, T. A., Hager, W. W., & Rajamanickam, S. (2008). Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw., 35(3). [doi:10.1145/1391989.1391995](https://doi.org/10.1145/1391989.1391995)\n\n[^DavisHager2009]: Davis, Timothy A., & Hager, W. W. (2009). Dynamic Supernodes in Sparse Cholesky Update/Downdate and Triangular Solves. ACM Trans. Math. Softw., 35(4). [doi:10.1145/1462173.1462176](https://doi.org/10.1145/1462173.1462176)\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Diagonal",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Diagonal",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s123\", var\"#s122\"}, Hermitian{Complex{var\"#s123\"}, var\"#s122\"}, Symmetric{var\"#s123\", var\"#s122\"}} where {var\"#s123\"<:Union{Float32, Float64}, var\"#s122\"<:(StridedMatrix{T} where T)}",
                "RowMaximum"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s125\", S}, Hermitian{Complex{var\"#s125\"}, S}, Symmetric{var\"#s125\", S}} where {var\"#s125\"<:Real, S}",
                "RowMaximum"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "RowMaximum"
            ],
            "kwarg_names": [
                "tol",
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "NoPivot"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "StridedMatrix{T} where T",
                "Val{true}"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s124\", var\"#s123\"}, Hermitian{Complex{var\"#s124\"}, var\"#s123\"}, Symmetric{var\"#s124\", var\"#s123\"}} where {var\"#s124\"<:Union{Float32, Float64}, var\"#s123\"<:(StridedMatrix{T} where T)}",
                "Val{true}"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s125\", S}, Hermitian{Complex{var\"#s125\"}, S}, Symmetric{var\"#s125\", S}} where {var\"#s125\"<:Real, S}",
                "Val{true}"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "StridedMatrix{T} where T",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cholesky!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncholesky!(A::AbstractMatrix, NoPivot(); check = true) -> Cholesky\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> cholesky!(A)\nERROR: InexactError: Int64(6.782329983125268)\nStacktrace:\n[...]\n```\n\n```\ncholesky!(A::AbstractMatrix, RowMaximum(); tol = 0.0, check = true) -> CholeskyPivoted\n```\n\nThe same as [`cholesky`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n```\ncholesky!(F::CHOLMOD.Factor, A::SparseMatrixCSC; shift = 0.0, check = true) -> CHOLMOD.Factor\n```\n\nCompute the Cholesky ($LL'$) factorization of `A`, reusing the symbolic factorization `F`. `A` must be a [`SparseMatrixCSC`](@ref) or a [`Symmetric`](@ref)/ [`Hermitian`](@ref) view of a `SparseMatrixCSC`. Note that even if `A` doesn't have the type tag, it must still be symmetric or Hermitian.\n\nSee also [`cholesky`](@ref).\n\n!!! note\n    This method uses the CHOLMOD library from SuiteSparse, which only supports doubles or complex doubles. Input matrices not of those element types will be converted to `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "x",
                "p"
            ],
            "arg_types": [
                "Number",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "UnitUpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "UpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "UnitLowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "LowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "cond",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncond(M, p::Real=2)\n```\n\nCondition number of the matrix `M`, computed using the operator `p`-norm. Valid values for `p` are `1`, `2` (default), or `Inf`.\n"
        },
        {
            "name": "condskeel",
            "arg_names": [
                "A",
                "x",
                "p"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVector",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
        },
        {
            "name": "condskeel",
            "arg_names": [
                "A",
                "x"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
        },
        {
            "name": "condskeel",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
        },
        {
            "name": "condskeel",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncondskeel(M, [x, p::Real=Inf])\n```\n\n$$\n\\kappa_S(M, p) = \\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\right\\Vert_p \\\\\n\\kappa_S(M, x, p) = \\frac{\\left\\Vert \\left\\vert M \\right\\vert \\left\\vert M^{-1} \\right\\vert \\left\\vert x \\right\\vert \\right\\Vert_p}{\\left \\Vert x \\right \\Vert_p}\n$$\n\nSkeel condition number $\\kappa_S$ of the matrix `M`, optionally with respect to the vector `x`, as computed using the operator `p`-norm. $\\left\\vert M \\right\\vert$ denotes the matrix of (entry wise) absolute values of $M$; $\\left\\vert M \\right\\vert_{ij} = \\left\\vert M_{ij} \\right\\vert$. Valid values for `p` are `1`, `2` and `Inf` (default).\n\nThis quantity is also known in the literature as the Bauer condition number, relative condition number, or componentwise relative condition number.\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "f"
            ],
            "arg_types": [
                "Type{T}",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "A"
            ],
            "arg_types": [
                "Type{Transpose{T, S}}",
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "r"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractRotation{T}}",
                "LinearAlgebra.AdjointRotation{T, S} where S<:LinearAlgebra.AbstractRotation{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "r"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractRotation{T}}",
                "LinearAlgebra.AdjointRotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "r"
            ],
            "arg_types": [
                "Type{T}",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "A"
            ],
            "arg_types": [
                "Type{Adjoint{T, S}}",
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "J"
            ],
            "arg_types": [
                "Type{UniformScaling{T}}",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.LQPackedQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.QRCompactWYQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.QRPackedQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "adjQ"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.AdjointQ{T, S} where S<:LinearAlgebra.AbstractQ{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "adjQ"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.AdjointQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{LinearAlgebra.AbstractQ{T}}",
                "LinearAlgebra.AbstractQ{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{AbstractMatrix{T}}",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{Hermitian, Symmetric}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LinearAlgebra.AbstractTriangular, Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LinearAlgebra.AbstractTriangular, Bidiagonal, Diagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LowerTriangular, UnitLowerTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LinearAlgebra.AbstractTriangular, Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{Hermitian, Symmetric}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "A"
            ],
            "arg_types": [
                "Type{T}",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "f"
            ],
            "arg_types": [
                "Type{T}",
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "f"
            ],
            "arg_types": [
                "Type{T}",
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "r"
            ],
            "arg_types": [
                "Type{T}",
                "LinearAlgebra.AbstractRotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LinearAlgebra.AbstractTriangular, Bidiagonal, Diagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{UnitUpperTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "m"
            ],
            "arg_types": [
                "Type{T}",
                "Union{LinearAlgebra.AbstractTriangular, Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "convert",
            "arg_names": [
                "",
                "Q"
            ],
            "arg_types": [
                "Type{T}",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nconvert(T, x)\n```\n\nConvert `x` to a value of type `T`.\n\nIf `T` is an [`Integer`](@ref) type, an [`InexactError`](@ref) will be raised if `x` is not representable by `T`, for example if `x` is not integer-valued, or is outside the range supported by `T`.\n\n# Examples\n\n```jldoctest\njulia> convert(Int, 3.0)\n3\n\njulia> convert(Int, 3.5)\nERROR: InexactError: Int64(3.5)\nStacktrace:\n[...]\n```\n\nIf `T` is a [`AbstractFloat`](@ref) type, then it will return the closest value to `x` representable by `T`.\n\n```jldoctest\njulia> x = 1/3\n0.3333333333333333\n\njulia> convert(Float32, x)\n0.33333334f0\n\njulia> convert(BigFloat, x)\n0.333333333333333314829616256247390992939472198486328125\n```\n\nIf `T` is a collection type and `x` a collection, the result of `convert(T, x)` may alias all or part of `x`.\n\n```jldoctest\njulia> x = Int[1, 2, 3];\n\njulia> y = convert(Vector{Int}, x);\n\njulia> y === x\ntrue\n```\n\nSee also: [`round`](@ref), [`trunc`](@ref), [`oftype`](@ref), [`reinterpret`](@ref).\n"
        },
        {
            "name": "copy_transpose!",
            "arg_names": [
                "B",
                "ir_dest",
                "jr_dest",
                "tM",
                "M",
                "ir_src",
                "jr_src"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractUnitRange{Int64}",
                "AbstractUnitRange{Int64}",
                "AbstractChar",
                "AbstractVecOrMat",
                "AbstractUnitRange{Int64}",
                "AbstractUnitRange{Int64}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "No documentation found.\n\n`LinearAlgebra.copy_transpose!` is a `Function`.\n\n```\n# 2 methods for generic function \"copy_transpose!\" from LinearAlgebra:\n [1] copy_transpose!(B::AbstractMatrix, ir_dest::AbstractUnitRange{Int64}, jr_dest::AbstractUnitRange{Int64}, tM::AbstractChar, M::AbstractVecOrMat, ir_src::AbstractUnitRange{Int64}, jr_src::AbstractUnitRange{Int64})\n     @ /opt/hostedtoolcache/julia/1.10.7/x64/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:668\n [2] copy_transpose!(B::AbstractVecOrMat, ir_dest::AbstractRange{Int64}, jr_dest::AbstractRange{Int64}, A::AbstractVecOrMat, ir_src::AbstractRange{Int64}, jr_src::AbstractRange{Int64})\n     @ /opt/hostedtoolcache/julia/1.10.7/x64/share/julia/stdlib/v1.10/LinearAlgebra/src/transpose.jl:186\n```\n"
        },
        {
            "name": "copy_transpose!",
            "arg_names": [
                "B",
                "ir_dest",
                "jr_dest",
                "A",
                "ir_src",
                "jr_src"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "AbstractRange{Int64}",
                "AbstractRange{Int64}",
                "AbstractVecOrMat",
                "AbstractRange{Int64}",
                "AbstractRange{Int64}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "No documentation found.\n\n`LinearAlgebra.copy_transpose!` is a `Function`.\n\n```\n# 2 methods for generic function \"copy_transpose!\" from LinearAlgebra:\n [1] copy_transpose!(B::AbstractMatrix, ir_dest::AbstractUnitRange{Int64}, jr_dest::AbstractUnitRange{Int64}, tM::AbstractChar, M::AbstractVecOrMat, ir_src::AbstractUnitRange{Int64}, jr_src::AbstractUnitRange{Int64})\n     @ /opt/hostedtoolcache/julia/1.10.7/x64/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:668\n [2] copy_transpose!(B::AbstractVecOrMat, ir_dest::AbstractRange{Int64}, jr_dest::AbstractRange{Int64}, A::AbstractVecOrMat, ir_src::AbstractRange{Int64}, jr_src::AbstractRange{Int64})\n     @ /opt/hostedtoolcache/julia/1.10.7/x64/share/julia/stdlib/v1.10/LinearAlgebra/src/transpose.jl:186\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "Hermitian",
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "rdest",
                "src",
                "rsrc"
            ],
            "arg_types": [
                "Array{T}",
                "AbstractRange{Ti}",
                "Array{T}",
                "AbstractRange{Ti}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "Tridiagonal",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "Tridiagonal",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "Tridiagonal",
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "Diagonal",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "Diagonal",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "D1",
                "D2"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "LowerTriangular",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "PermutedDimsArray{T, 2, perm}",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "SymTridiagonal",
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "Symmetric",
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "UpperTriangular",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "bc"
            ],
            "arg_types": [
                "Bidiagonal",
                "Base.Broadcast.Broadcasted{<:LinearAlgebra.StructuredMatrixStyle}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "Union{Bidiagonal, SymTridiagonal}",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "dest",
                "src"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "T",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "T",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n"
        },
        {
            "name": "copyto!",
            "arg_names": [
                "B",
                "ir_dest",
                "jr_dest",
                "tM",
                "M",
                "ir_src",
                "jr_src"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "AbstractUnitRange{Int64}",
                "AbstractUnitRange{Int64}",
                "AbstractChar",
                "AbstractVecOrMat",
                "AbstractUnitRange{Int64}",
                "AbstractUnitRange{Int64}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncopyto!(dest, do, src, so, N)\n```\n\nCopy `N` elements from collection `src` starting at the linear index `so`, to array `dest` starting at the index `do`. Return `dest`.\n\n```\ncopyto!(dest::AbstractArray, src) -> dest\n```\n\nCopy all elements from collection `src` to array `dest`, whose length must be greater than or equal to the length `n` of `src`. The first `n` elements of `dest` are overwritten, the other elements are left untouched.\n\nSee also [`copy!`](@ref Base.copy!), [`copy`](@ref).\n\n# Examples\n\n```jldoctest\njulia> x = [1., 0., 3., 0., 5.];\n\njulia> y = zeros(7);\n\njulia> copyto!(y, x);\n\njulia> y\n7-element Vector{Float64}:\n 1.0\n 0.0\n 3.0\n 0.0\n 5.0\n 0.0\n 0.0\n```\n\n```\ncopyto!(dest, Rdest::CartesianIndices, src, Rsrc::CartesianIndices) -> dest\n```\n\nCopy the block of `src` in the range of `Rsrc` to the block of `dest` in the range of `Rdest`. The sizes of the two regions must match.\n\n# Examples\n\n```jldoctest\njulia> A = zeros(5, 5);\n\njulia> B = [1 2; 3 4];\n\njulia> Ainds = CartesianIndices((2:3, 2:3));\n\njulia> Binds = CartesianIndices(B);\n\njulia> copyto!(A, Ainds, B, Binds)\n5×5 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  2.0  0.0  0.0\n 0.0  3.0  4.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n```\n\n```\ncopyto!(dest::AbstractMatrix, src::UniformScaling)\n```\n\nCopies a [`UniformScaling`](@ref) onto a matrix.\n\n!!! compat \"Julia 1.1\"\n    In Julia 1.0 this method only supported a square destination matrix. Julia 1.1. added support for a rectangular matrix.\n\n"
        },
        {
            "name": "cross",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncross(x, y)\n×(x,y)\n```\n\nCompute the cross product of two 3-vectors.\n\n# Examples\n\n```jldoctest\njulia> a = [0;1;0]\n3-element Vector{Int64}:\n 0\n 1\n 0\n\njulia> b = [0;0;1]\n3-element Vector{Int64}:\n 0\n 0\n 1\n\njulia> cross(a,b)\n3-element Vector{Int64}:\n 1\n 0\n 0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Cholesky"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LU{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Eigen"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Hessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "CholeskyPivoted"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.QRCompactWYQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric{var\"#s997\", S} where {var\"#s997\"<:Real, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "UpperHessenberg"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.QRPackedQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.LQPackedQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{BigInt}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "det",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.HessenbergQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndet(M)\n```\n\nMatrix determinant.\n\nSee also: [`logdet`](@ref) and [`logabsdet`](@ref).\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> det(M)\n2.0\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M",
                "n"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "D",
                "k"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M",
                "n"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M",
                "n"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "M",
                "n"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diag",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiag(M, k::Integer=0)\n```\n\nThe `k`th diagonal of a matrix, as a vector.\n\nSee also [`diagm`](@ref), [`diagind`](@ref), [`Diagonal`](@ref), [`isdiag`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diag(A,1)\n2-element Vector{Int64}:\n 2\n 6\n```\n"
        },
        {
            "name": "diagind",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\nSee also: [`diag`](@ref), [`diagm`](@ref), [`Diagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
        },
        {
            "name": "diagind",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\nSee also: [`diag`](@ref), [`diagm`](@ref), [`Diagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
        },
        {
            "name": "diagind",
            "arg_names": [
                "m",
                "n",
                "k"
            ],
            "arg_types": [
                "Integer",
                "Integer",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\nSee also: [`diag`](@ref), [`diagm`](@ref), [`Diagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
        },
        {
            "name": "diagind",
            "arg_names": [
                "m",
                "n"
            ],
            "arg_types": [
                "Integer",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagind(M, k::Integer=0)\n```\n\nAn `AbstractRange` giving the indices of the `k`th diagonal of the matrix `M`.\n\nSee also: [`diag`](@ref), [`diagm`](@ref), [`Diagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2 3; 4 5 6; 7 8 9]\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\njulia> diagind(A,-1)\n2:4:6\n```\n"
        },
        {
            "name": "diagm",
            "arg_names": [
                "kv"
            ],
            "arg_types": [
                "Pair{<:Integer, <:AbstractVector}..."
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m`×`n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments. For repeated diagonal indices `kv.first` the values in the corresponding vectors `kv.second` will be added.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], 1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  2  0  0\n 0  0  4  0\n 0  0  0  6\n 0  0  0  0\n```\n"
        },
        {
            "name": "diagm",
            "arg_names": [
                "m",
                "n",
                "v"
            ],
            "arg_types": [
                "Integer",
                "Integer",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m`×`n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments. For repeated diagonal indices `kv.first` the values in the corresponding vectors `kv.second` will be added.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], 1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  2  0  0\n 0  0  4  0\n 0  0  0  6\n 0  0  0  0\n```\n\n```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m`×`n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n3×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
        },
        {
            "name": "diagm",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m`×`n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n3×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
        },
        {
            "name": "diagm",
            "arg_names": [
                "m",
                "n",
                "kv"
            ],
            "arg_types": [
                "Integer",
                "Integer",
                "Pair{<:Integer, <:AbstractVector}..."
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndiagm(kv::Pair{<:Integer,<:AbstractVector}...)\ndiagm(m::Integer, n::Integer, kv::Pair{<:Integer,<:AbstractVector}...)\n```\n\nConstruct a matrix from `Pair`s of diagonals and vectors. Vector `kv.second` will be placed on the `kv.first` diagonal. By default the matrix is square and its size is inferred from `kv`, but a non-square size `m`×`n` (padded with zeros as needed) can be specified by passing `m,n` as the first arguments. For repeated diagonal indices `kv.first` the values in the corresponding vectors `kv.second` will be added.\n\n`diagm` constructs a full matrix; if you want storage-efficient versions with fast arithmetic, see [`Diagonal`](@ref), [`Bidiagonal`](@ref) [`Tridiagonal`](@ref) and [`SymTridiagonal`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diagm(1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 0  0  2  0\n 0  0  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], -1 => [4,5])\n4×4 Matrix{Int64}:\n 0  1  0  0\n 4  0  2  0\n 0  5  0  3\n 0  0  0  0\n\njulia> diagm(1 => [1,2,3], 1 => [1,2,3])\n4×4 Matrix{Int64}:\n 0  2  0  0\n 0  0  4  0\n 0  0  0  6\n 0  0  0  0\n```\n\n```\ndiagm(v::AbstractVector)\ndiagm(m::Integer, n::Integer, v::AbstractVector)\n```\n\nConstruct a matrix with elements of the vector as diagonal elements. By default, the matrix is square and its size is given by `length(v)`, but a non-square size `m`×`n` can be specified by passing `m,n` as the first arguments.\n\n# Examples\n\n```jldoctest\njulia> diagm([1,2,3])\n3×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n 0  0  3\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "BitVector",
                "BitVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Union{Real, Complex}, S<:(AbstractMatrix{<:var\"#s997\"})}",
                "Hermitian{var\"#s128\", S} where {var\"#s128\"<:Union{Real, Complex}, S<:(AbstractMatrix{<:var\"#s128\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Adjoint{<:Union{Real, Complex}}",
                "Adjoint{<:Union{Real, Complex}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "Vector{T}",
                "AbstractRange{TI}",
                "Vector{T}",
                "AbstractRange{TI}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "Vector{T}",
                "AbstractRange{TI}",
                "Vector{T}",
                "AbstractRange{TI}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "J",
                "A"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Symmetric",
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Transpose",
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "B",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Bidiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Tridiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "adjA",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Adjoint",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "D",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Diagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "J",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UniformScaling",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "S",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "SymTridiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UnitLowerTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "transA",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Transpose{<:Real}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "H",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UpperHessenberg",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "a",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Real, Complex}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "a",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Number",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UnitUpperTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UpperTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "LowerTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Hermitian{var\"#s997\", var\"#s128\"}, Symmetric{var\"#s997\", var\"#s128\"}, Symmetric{Complex{var\"#s997\"}, var\"#s128\"}} where {var\"#s997\"<:Real, var\"#s128\"<:Diagonal}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractMatrix",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "AbstractArray",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "",
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "Da",
                "Db"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Cholesky"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{TA, S}, Symmetric{TA, S}} where S",
                "Union{Hermitian{TB, S}, Symmetric{TB, S}} where S"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{TA}",
                "AbstractMatrix{TB}"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "vl",
                "vu"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A; permute::Bool=true, scale::Bool=true, sortby) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving an eigenvalue problem of the form `Ax =  λx`, where `A` is a matrix, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nFor general nonsymmetric matrices it is possible to specify how the matrix is balanced before the eigenvector calculation. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. The default is `true` for both options.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.   Some special matrix types (e.g. [`Diagonal`](@ref) or [`SymTridiagonal`](@ref)) may implement their own sorting convention and not accept a `sortby` keyword.\n\n# Examples\n\n```jldoctest\njulia> F = eigen([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\nEigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}\nvalues:\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\nvectors:\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> F.values\n3-element Vector{Float64}:\n  1.0\n  3.0\n 18.0\n\njulia> F.vectors\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A, B; sortby) -> GeneralizedEigen\n```\n\nCompute the generalized eigenvalue decomposition of `A` and `B`, returning a [`GeneralizedEigen`](@ref) factorization object `F` which contains the generalized eigenvalues in `F.values` and the generalized eigenvectors in the columns of the matrix `F.vectors`. This corresponds to solving a generalized eigenvalue problem of the form `Ax =  λBx`, where `A, B` are matrices, `x` is an eigenvector, and `λ` is an eigenvalue. (The `k`th generalized eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nBy default, the eigenvalues and vectors are sorted lexicographically by `(real(λ),imag(λ))`. A different comparison function `by(λ)` can be passed to `sortby`, or you can pass `sortby=nothing` to leave the eigenvalues in an arbitrary order.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> F = eigen(A, B);\n\njulia> F.values\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> F.vectors\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n\njulia> vals, vecs = F; # destructuring via iteration\n\njulia> vals == F.values && vecs == F.vectors\ntrue\n```\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n\n```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "vl",
                "vh"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\n`vl` is the lower bound of the window of eigenvalues to search for, and `vu` is the upper bound.\n\n!!! note\n    If [`vl`, `vu`] does not contain all eigenvalues of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> Eigen\n```\n\nCompute the eigenvalue decomposition of `A`, returning an [`Eigen`](@ref) factorization object `F` which contains the eigenvalues in `F.values` and the eigenvectors in the columns of the matrix `F.vectors`. (The `k`th eigenvector can be obtained from the slice `F.vectors[:, k]`.)\n\nIterating the decomposition produces the components `F.values` and `F.vectors`.\n\nThe following functions are available for `Eigen` objects: [`inv`](@ref), [`det`](@ref), and [`isposdef`](@ref).\n\nThe [`UnitRange`](@ref) `irange` specifies indices of the sorted eigenvalues to search for.\n\n!!! note\n    If `irange` is not `1:n`, where `n` is the dimension of `A`, then the returned factorization will be a *truncated* factorization.\n\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Hermitian{T, S}",
                "Hermitian{T, S}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}}",
                "Union{Hermitian{T, S}, Symmetric{T, S}}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Cholesky"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "vl",
                "vu"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "vl",
                "vh"
            ],
            "arg_types": [
                "Union{Hermitian{T, var\"#s997\"}, Hermitian{Complex{T}, var\"#s997\"}, Symmetric{T, var\"#s997\"}} where var\"#s997\"<:(StridedMatrix{T} where T)",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s997\", var\"#s128\"}, Hermitian{Complex{var\"#s997\"}, var\"#s128\"}, Symmetric{var\"#s997\", var\"#s128\"}} where {var\"#s997\"<:Union{Float32, Float64}, var\"#s128\"<:(StridedMatrix{T} where T)}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s125\", var\"#s124\"}, Hermitian{Complex{var\"#s125\"}, var\"#s124\"}, Symmetric{var\"#s125\", var\"#s124\"}} where {var\"#s125\"<:Union{Float32, Float64}, var\"#s124\"<:(StridedMatrix{T} where T)}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigen!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigen!(A; permute, scale, sortby)\neigen!(A, B; sortby)\n```\n\nSame as [`eigen`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating a copy.\n"
        },
        {
            "name": "eigmax",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s997\", S}, Hermitian{Complex{var\"#s997\"}, S}, Symmetric{var\"#s997\", S}} where {var\"#s997\"<:Real, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigmax",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigmax",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Number, AbstractMatrix}"
            ],
            "kwarg_names": [
                "permute",
                "scale"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmax(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the largest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmax(A)\n1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmax(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigmin",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s997\", S}, Hermitian{Complex{var\"#s997\"}, S}, Symmetric{var\"#s997\", S}} where {var\"#s997\"<:Real, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigmin",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigmin",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Number, AbstractMatrix}"
            ],
            "kwarg_names": [
                "permute",
                "scale"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigmin(A; permute::Bool=true, scale::Bool=true)\n```\n\nReturn the smallest eigenvalue of `A`. The option `permute=true` permutes the matrix to become closer to upper triangular, and `scale=true` scales the matrix by its diagonal elements to make rows and columns more equal in norm. Note that if the eigenvalues of `A` are complex, this method will fail, since complex numbers cannot be sorted.\n\n# Examples\n\n```jldoctest\njulia> A = [0 im; -im 0]\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+1im\n 0-1im  0+0im\n\njulia> eigmin(A)\n-1.0\n\njulia> A = [0 im; -1 0]\n2×2 Matrix{Complex{Int64}}:\n  0+0im  0+1im\n -1+0im  0+0im\n\njulia> eigmin(A)\nERROR: DomainError with Complex{Int64}[0+0im 0+1im; -1+0im 0+0im]:\n`A` cannot have complex eigenvalues.\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s126\", V} where {var\"#s126\"<:Number, V<:AbstractVector{var\"#s126\"}}"
            ],
            "kwarg_names": [
                "permute",
                "scale"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
                "permute",
                "scale"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Cholesky"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{TA, S}, Symmetric{TA, S}} where S",
                "Union{Hermitian{TB, S}, Symmetric{TB, S}} where S"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{TA}",
                "AbstractMatrix{TB}"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "vl",
                "vu"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\nFor a scalar input, `eigvals` will return a scalar.\n\n# Example\n\n```jldoctest\njulia> eigvals(-2)\n-2\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n\n```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "vl",
                "vh"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a pair `vl` and `vu` for the lower and upper boundaries of the eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, -1, 2)\n1-element Vector{Float64}:\n 1.0000000000000009\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nReturn the eigenvalues of `A`. It is possible to calculate only a subset of the eigenvalues by specifying a [`UnitRange`](@ref) `irange` covering indices of the sorted eigenvalues, e.g. the 2nd to 8th eigenvalues.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A, 2:2)\n1-element Vector{Float64}:\n 0.9999999999999996\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n```\n"
        },
        {
            "name": "eigvals",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Union{Eigen, GeneralizedEigen}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nReturn the eigenvalues of `A`.\n\nFor general non-symmetric matrices it is possible to specify how the matrix is balanced before the eigenvalue calculation. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> diag_matrix = [1 0; 0 4]\n2×2 Matrix{Int64}:\n 1  0\n 0  4\n\njulia> eigvals(diag_matrix)\n2-element Vector{Float64}:\n 1.0\n 4.0\n```\n\n```\neigvals(A, B) -> values\n```\n\nCompute the generalized eigenvalues of `A` and `B`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvals(A,B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n```\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Hermitian{T, S}",
                "Hermitian{T, S}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}}",
                "Union{Hermitian{T, S}, Symmetric{T, S}}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "Cholesky{T}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "vl",
                "vu"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64}, <:StridedVector{T} where T}",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "vl",
                "vh"
            ],
            "arg_types": [
                "Union{Hermitian{T, var\"#s997\"}, Hermitian{Complex{T}, var\"#s997\"}, Symmetric{T, var\"#s997\"}} where var\"#s997\"<:(StridedMatrix{T} where T)",
                "Real",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "irange"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s997\", var\"#s128\"}, Hermitian{Complex{var\"#s997\"}, var\"#s128\"}, Symmetric{var\"#s997\", var\"#s128\"}} where {var\"#s997\"<:Union{Float32, Float64}, var\"#s128\"<:(StridedMatrix{T} where T)}",
                "UnitRange"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s125\", var\"#s124\"}, Hermitian{Complex{var\"#s125\"}, var\"#s124\"}, Symmetric{var\"#s125\", var\"#s124\"}} where {var\"#s125\"<:Union{Float32, Float64}, var\"#s124\"<:(StridedMatrix{T} where T)}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s125\"} where var\"#s125\"<:Union{Float32, Float64}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s126\"} where var\"#s126\"<:Union{ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
                "permute",
                "scale",
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n"
        },
        {
            "name": "eigvals!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "sortby"
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvals!(A; permute::Bool=true, scale::Bool=true, sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n!!! note\n    The input matrix `A` will not contain its eigenvalues after `eigvals!` is called on it - `A` is used as a workspace.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> eigvals!(A)\n2-element Vector{Float64}:\n -0.3722813232690143\n  5.372281323269014\n\njulia> A\n2×2 Matrix{Float64}:\n -0.372281  -1.0\n  0.0        5.37228\n```\n\n```\neigvals!(A, B; sortby) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A` (and `B`), instead of creating copies.\n\n!!! note\n    The input matrices `A` and `B` will not contain their eigenvalues after `eigvals!` is called. They are used as workspaces.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> eigvals!(A, B)\n2-element Vector{ComplexF64}:\n 0.0 - 1.0im\n 0.0 + 1.0im\n\njulia> A\n2×2 Matrix{Float64}:\n -0.0  -1.0\n  1.0  -0.0\n\njulia> B\n2×2 Matrix{Float64}:\n 1.0  0.0\n 0.0  1.0\n```\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, irange::UnitRange) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `irange` is a range of eigenvalue *indices* to search for - for instance, the 2nd to 8th eigenvalues.\n\n```\neigvals!(A::Union{SymTridiagonal, Hermitian, Symmetric}, vl::Real, vu::Real) -> values\n```\n\nSame as [`eigvals`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. `vl` is the lower bound of the interval to search for eigenvalues, and `vu` is the upper bound.\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A",
                "eigvals"
            ],
            "arg_types": [
                "SymTridiagonal{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedVector{T} where T}",
                "Vector{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A::SymTridiagonal[, eigvals]) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\nIf the optional vector of eigenvalues `eigvals` is specified, `eigvecs` returns the specific corresponding eigenvectors.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n\njulia> eigvecs(A)\n3×3 Matrix{Float64}:\n  0.418304  -0.83205      0.364299\n -0.656749  -7.39009e-16  0.754109\n  0.627457   0.5547       0.546448\n\njulia> eigvecs(A, [1.])\n3×1 Matrix{Float64}:\n  0.8320502943378438\n  4.263514128092366e-17\n -0.5547001962252291\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A, B) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the generalized eigenvectors of `A` and `B`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvecs(A, B)\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Union{Eigen, GeneralizedEigen}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A::SymTridiagonal[, eigvals]) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\nIf the optional vector of eigenvalues `eigvals` is specified, `eigvecs` returns the specific corresponding eigenvectors.\n\n# Examples\n\n```jldoctest\njulia> A = SymTridiagonal([1.; 2.; 1.], [2.; 3.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 1.0  2.0   ⋅\n 2.0  2.0  3.0\n  ⋅   3.0  1.0\n\njulia> eigvals(A)\n3-element Vector{Float64}:\n -2.1400549446402604\n  1.0000000000000002\n  5.140054944640259\n\njulia> eigvecs(A)\n3×3 Matrix{Float64}:\n  0.418304  -0.83205      0.364299\n -0.656749  -7.39009e-16  0.754109\n  0.627457   0.5547       0.546448\n\njulia> eigvecs(A, [1.])\n3×1 Matrix{Float64}:\n  0.8320502943378438\n  4.263514128092366e-17\n -0.5547001962252291\n```\n\n```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n\n```\neigvecs(A, B) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the generalized eigenvectors of `A` and `B`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.)\n\n# Examples\n\n```jldoctest\njulia> A = [1 0; 0 -1]\n2×2 Matrix{Int64}:\n 1   0\n 0  -1\n\njulia> B = [0 1; 1 0]\n2×2 Matrix{Int64}:\n 0  1\n 1  0\n\njulia> eigvecs(A, B)\n2×2 Matrix{ComplexF64}:\n  0.0+1.0im   0.0-1.0im\n -1.0+0.0im  -1.0-0.0im\n```\n"
        },
        {
            "name": "eigvecs",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Number, AbstractMatrix}"
            ],
            "kwarg_names": [
                "kws..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\neigvecs(A; permute::Bool=true, scale::Bool=true, `sortby`) -> Matrix\n```\n\nReturn a matrix `M` whose columns are the eigenvectors of `A`. (The `k`th eigenvector can be obtained from the slice `M[:, k]`.) The `permute`, `scale`, and `sortby` keywords are the same as for [`eigen`](@ref).\n\n# Examples\n\n```jldoctest\njulia> eigvecs([1.0 0.0 0.0; 0.0 3.0 0.0; 0.0 0.0 18.0])\n3×3 Matrix{Float64}:\n 1.0  0.0  0.0\n 0.0  1.0  0.0\n 0.0  0.0  1.0\n```\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "factorize",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nfactorize(A)\n```\n\nCompute a convenient factorization of `A`, based upon the type of the input matrix. `factorize` checks `A` to see if it is symmetric/triangular/etc. if `A` is passed as a generic matrix. `factorize` checks every element of `A` to verify/rule out each property. It will short-circuit as soon as it can rule out symmetry/triangular structure. The return value can be reused for efficient solving of multiple systems. For example: `A=factorize(A); x=A\\b; y=A\\C`.\n\n| Properties of `A`          | type of factorization                      |\n|:-------------------------- |:------------------------------------------ |\n| Positive-definite          | Cholesky (see [`cholesky`](@ref))          |\n| Dense Symmetric/Hermitian  | Bunch-Kaufman (see [`bunchkaufman`](@ref)) |\n| Sparse Symmetric/Hermitian | LDLt (see [`ldlt`](@ref))                  |\n| Triangular                 | Triangular                                 |\n| Diagonal                   | Diagonal                                   |\n| Bidiagonal                 | Bidiagonal                                 |\n| Tridiagonal                | LU (see [`lu`](@ref))                      |\n| Symmetric real tridiagonal | LDLt (see [`ldlt`](@ref))                  |\n| General square             | LU (see [`lu`](@ref))                      |\n| General non-square         | QR (see [`qr`](@ref))                      |\n\nIf `factorize` is called on a Hermitian positive-definite matrix, for instance, then `factorize` will return a Cholesky factorization.\n\n# Examples\n\n```jldoctest\njulia> A = Array(Bidiagonal(fill(1.0, (5, 5)), :U))\n5×5 Matrix{Float64}:\n 1.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0\n 0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0\n\njulia> factorize(A) # factorize will check to see that A is already factorized\n5×5 Bidiagonal{Float64, Vector{Float64}}:\n 1.0  1.0   ⋅    ⋅    ⋅\n  ⋅   1.0  1.0   ⋅    ⋅\n  ⋅    ⋅   1.0  1.0   ⋅\n  ⋅    ⋅    ⋅   1.0  1.0\n  ⋅    ⋅    ⋅    ⋅   1.0\n```\n\nThis returns a `5×5 Bidiagonal{Float64}`, which can now be passed to other linear algebra functions (e.g. eigensolvers) which will use specialized methods for `Bidiagonal` types.\n"
        },
        {
            "name": "givens",
            "arg_names": [
                "x",
                "i1",
                "i2"
            ],
            "arg_types": [
                "AbstractVector",
                "Integer",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ngivens(x::AbstractVector, i1::Integer, i2::Integer) -> (G::Givens, r)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that the result of the multiplication\n\n```\nB = G*x\n```\n\nhas the property that\n\n```\nB[i1] = r\nB[i2] = 0\n```\n\nSee also [`LinearAlgebra.Givens`](@ref).\n"
        },
        {
            "name": "givens",
            "arg_names": [
                "A",
                "i1",
                "i2",
                "j"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer",
                "Integer",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ngivens(A::AbstractArray, i1::Integer, i2::Integer, j::Integer) -> (G::Givens, r)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that the result of the multiplication\n\n```\nB = G*A\n```\n\nhas the property that\n\n```\nB[i1,j] = r\nB[i2,j] = 0\n```\n\nSee also [`LinearAlgebra.Givens`](@ref).\n"
        },
        {
            "name": "givens",
            "arg_names": [
                "f",
                "g",
                "i1",
                "i2"
            ],
            "arg_types": [
                "T",
                "T",
                "Integer",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ngivens(f::T, g::T, i1::Integer, i2::Integer) where {T} -> (G::Givens, r::T)\n```\n\nComputes the Givens rotation `G` and scalar `r` such that for any vector `x` where\n\n```\nx[i1] = f\nx[i2] = g\n```\n\nthe result of the multiplication\n\n```\ny = G*x\n```\n\nhas the property that\n\n```\ny[i1] = r\ny[i2] = 0\n```\n\nSee also [`LinearAlgebra.Givens`](@ref).\n"
        },
        {
            "name": "hermitianpart",
            "arg_names": [
                "A",
                "uplo"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Symbol"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhermitianpart(A::AbstractMatrix, uplo::Symbol=:U) -> Hermitian\n```\n\nReturn the Hermitian part of the square matrix `A`, defined as `(A + A') / 2`, as a [`Hermitian`](@ref) matrix. For real matrices `A`, this is also known as the symmetric part of `A`; it is also sometimes called the \"operator real part\". The optional argument `uplo` controls the corresponding argument of the [`Hermitian`](@ref) view. For real matrices, the latter is equivalent to a [`Symmetric`](@ref) view.\n\nSee also [`hermitianpart!`](@ref) for the corresponding in-place operation.\n\n!!! compat \"Julia 1.10\"\n    This function requires Julia 1.10 or later.\n\n"
        },
        {
            "name": "hermitianpart",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhermitianpart(A::AbstractMatrix, uplo::Symbol=:U) -> Hermitian\n```\n\nReturn the Hermitian part of the square matrix `A`, defined as `(A + A') / 2`, as a [`Hermitian`](@ref) matrix. For real matrices `A`, this is also known as the symmetric part of `A`; it is also sometimes called the \"operator real part\". The optional argument `uplo` controls the corresponding argument of the [`Hermitian`](@ref) view. For real matrices, the latter is equivalent to a [`Symmetric`](@ref) view.\n\nSee also [`hermitianpart!`](@ref) for the corresponding in-place operation.\n\n!!! compat \"Julia 1.10\"\n    This function requires Julia 1.10 or later.\n\n"
        },
        {
            "name": "hermitianpart!",
            "arg_names": [
                "A",
                "uplo"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Symbol"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhermitianpart!(A::AbstractMatrix, uplo::Symbol=:U) -> Hermitian\n```\n\nOverwrite the square matrix `A` in-place with its Hermitian part `(A + A') / 2`, and return [`Hermitian(A, uplo)`](@ref). For real matrices `A`, this is also known as the symmetric part of `A`.\n\nSee also [`hermitianpart`](@ref) for the corresponding out-of-place operation.\n\n!!! compat \"Julia 1.10\"\n    This function requires Julia 1.10 or later.\n\n"
        },
        {
            "name": "hermitianpart!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhermitianpart!(A::AbstractMatrix, uplo::Symbol=:U) -> Hermitian\n```\n\nOverwrite the square matrix `A` in-place with its Hermitian part `(A + A') / 2`, and return [`Hermitian(A, uplo)`](@ref). For real matrices `A`, this is also known as the symmetric part of `A`.\n\nSee also [`hermitianpart`](@ref) for the corresponding out-of-place operation.\n\n!!! compat \"Julia 1.10\"\n    This function requires Julia 1.10 or later.\n\n"
        },
        {
            "name": "hessenberg",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhessenberg(A) -> Hessenberg\n```\n\nCompute the Hessenberg decomposition of `A` and return a `Hessenberg` object. If `F` is the factorization object, the unitary matrix can be accessed with `F.Q` (of type `LinearAlgebra.HessenbergQ`) and the Hessenberg matrix with `F.H` (of type [`UpperHessenberg`](@ref)), either of which may be converted to a regular matrix with `Matrix(F.H)` or `Matrix(F.Q)`.\n\nIf `A` is [`Hermitian`](@ref) or real-[`Symmetric`](@ref), then the Hessenberg decomposition produces a real-symmetric tridiagonal matrix and `F.H` is of type [`SymTridiagonal`](@ref).\n\nNote that the shifted factorization `A+μI = Q (H+μI) Q'` can be constructed efficiently by `F + μ*I` using the [`UniformScaling`](@ref) object [`I`](@ref), which creates a new `Hessenberg` object with shared storage and a modified shift.   The shift of a given `F` is obtained by `F.μ`. This is useful because multiple shifted solves `(F + μ*I) \\ b` (for different `μ` and/or `b`) can be performed efficiently once `F` is created.\n\nIterating the decomposition produces the factors `F.Q, F.H, F.μ`.\n\n# Examples\n\n```julia-repl\njulia> A = [4. 9. 7.; 4. 4. 1.; 4. 3. 2.]\n3×3 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> F = hessenberg(A)\nHessenberg{Float64, UpperHessenberg{Float64, Matrix{Float64}}, Matrix{Float64}, Vector{Float64}, Bool}\nQ factor: 3×3 LinearAlgebra.HessenbergQ{Float64, Matrix{Float64}, Vector{Float64}, false}\nH factor:\n3×3 UpperHessenberg{Float64, Matrix{Float64}}:\n  4.0      -11.3137       -1.41421\n -5.65685    5.0           2.0\n   ⋅        -8.88178e-16   1.0\n\njulia> F.Q * F.H * F.Q'\n3×3 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> q, h = F; # destructuring via iteration\n\njulia> q == F.Q && h == F.H\ntrue\n```\n"
        },
        {
            "name": "hessenberg",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhessenberg(A) -> Hessenberg\n```\n\nCompute the Hessenberg decomposition of `A` and return a `Hessenberg` object. If `F` is the factorization object, the unitary matrix can be accessed with `F.Q` (of type `LinearAlgebra.HessenbergQ`) and the Hessenberg matrix with `F.H` (of type [`UpperHessenberg`](@ref)), either of which may be converted to a regular matrix with `Matrix(F.H)` or `Matrix(F.Q)`.\n\nIf `A` is [`Hermitian`](@ref) or real-[`Symmetric`](@ref), then the Hessenberg decomposition produces a real-symmetric tridiagonal matrix and `F.H` is of type [`SymTridiagonal`](@ref).\n\nNote that the shifted factorization `A+μI = Q (H+μI) Q'` can be constructed efficiently by `F + μ*I` using the [`UniformScaling`](@ref) object [`I`](@ref), which creates a new `Hessenberg` object with shared storage and a modified shift.   The shift of a given `F` is obtained by `F.μ`. This is useful because multiple shifted solves `(F + μ*I) \\ b` (for different `μ` and/or `b`) can be performed efficiently once `F` is created.\n\nIterating the decomposition produces the factors `F.Q, F.H, F.μ`.\n\n# Examples\n\n```julia-repl\njulia> A = [4. 9. 7.; 4. 4. 1.; 4. 3. 2.]\n3×3 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> F = hessenberg(A)\nHessenberg{Float64, UpperHessenberg{Float64, Matrix{Float64}}, Matrix{Float64}, Vector{Float64}, Bool}\nQ factor: 3×3 LinearAlgebra.HessenbergQ{Float64, Matrix{Float64}, Vector{Float64}, false}\nH factor:\n3×3 UpperHessenberg{Float64, Matrix{Float64}}:\n  4.0      -11.3137       -1.41421\n -5.65685    5.0           2.0\n   ⋅        -8.88178e-16   1.0\n\njulia> F.Q * F.H * F.Q'\n3×3 Matrix{Float64}:\n 4.0  9.0  7.0\n 4.0  4.0  1.0\n 4.0  3.0  2.0\n\njulia> q, h = F; # destructuring via iteration\n\njulia> q == F.Q && h == F.H\ntrue\n```\n"
        },
        {
            "name": "hessenberg!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{<:Union{Float32, Float64, ComplexF64, ComplexF32}, <:StridedMatrix{T} where T}, Symmetric{<:Union{Float32, Float64}, <:StridedMatrix{T} where T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhessenberg!(A) -> Hessenberg\n```\n\n`hessenberg!` is the same as [`hessenberg`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "hessenberg!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s997\"} where var\"#s997\"<:Union{Float32, Float64, ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nhessenberg!(A) -> Hessenberg\n```\n\n`hessenberg!` is the same as [`hessenberg`](@ref), but saves space by overwriting the input `A`, instead of creating a copy.\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{var\"#s997\", var\"#s128\"}, Symmetric{var\"#s997\", var\"#s128\"}} where {var\"#s997\", var\"#s128\"<:Diagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where {T, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "isdiag",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisdiag(A) -> Bool\n```\n\nTest whether a matrix is diagonal in the sense that `iszero(A[i,j])` is true unless `i == j`. Note that it is not necessary for `A` to be square; if you would also like to check that, you need to check that `size(A, 1) == size(A, 2)`.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> isdiag(a)\nfalse\n\njulia> b = [im 0; 0 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  0+0im\n 0+0im  0-1im\n\njulia> isdiag(b)\ntrue\n\njulia> c = [1 0 0; 0 2 0]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  0\n\njulia> isdiag(c)\ntrue\n\njulia> d = [1 0 0; 0 2 3]\n2×3 Matrix{Int64}:\n 1  0  0\n 0  2  3\n\njulia> isdiag(d)\nfalse\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Real, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BunchKaufman{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric{var\"#s997\", S} where {var\"#s997\"<:Real, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric{var\"#s997\", S} where {var\"#s997\"<:Complex, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "ishermitian",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nishermitian(A) -> Bool\n```\n\nTest whether a matrix is Hermitian.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> ishermitian(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> ishermitian(b)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Union{Cholesky, CholeskyPivoted}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Eigen, GeneralizedEigen}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`.\n\nSee also [`isposdef!`](@ref), [`cholesky`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 2 50]\n2×2 Matrix{Int64}:\n 1   2\n 2  50\n\njulia> isposdef(A)\ntrue\n```\n"
        },
        {
            "name": "isposdef!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nisposdef!(A) -> Bool\n```\n\nTest whether a matrix is positive definite (and Hermitian) by trying to perform a Cholesky factorization of `A`, overwriting `A` in the process. See also [`isposdef`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 2.; 2. 50.];\n\njulia> isposdef!(A)\ntrue\n\njulia> A\n2×2 Matrix{Float64}:\n 1.0  2.0\n 2.0  6.78233\n```\n"
        },
        {
            "name": "issuccess",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BunchKaufman"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> issuccess(F)\nfalse\n```\n"
        },
        {
            "name": "issuccess",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LU"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> issuccess(F)\nfalse\n```\n"
        },
        {
            "name": "issuccess",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Union{Cholesky, CholeskyPivoted}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissuccess(F::Factorization)\n```\n\nTest that a factorization of a matrix succeeded.\n\n!!! compat \"Julia 1.6\"\n    `issuccess(::CholeskyPivoted)` requires Julia 1.6 or later.\n\n\n```jldoctest\njulia> F = cholesky([1 0; 0 1]);\n\njulia> issuccess(F)\ntrue\n\njulia> F = lu([1 0; 0 0]; check = false);\n\njulia> issuccess(F)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Real, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Complex, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BunchKaufman"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "issymmetric",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nissymmetric(A) -> Bool\n```\n\nTest whether a matrix is symmetric.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> issymmetric(a)\ntrue\n\njulia> b = [1 im; -im 1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im  0+1im\n 0-1im  1+0im\n\njulia> issymmetric(b)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Tridiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Adjoint",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "D",
                "k"
            ],
            "arg_types": [
                "Diagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Transpose",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{LowerTriangular, UnitLowerTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Bidiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Union{LowerTriangular, UnitLowerTriangular}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistril(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is lower triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istril(a)\nfalse\n\njulia> istril(a, 1)\ntrue\n\njulia> b = [1 0; -im -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+0im\n 0-1im  -1+0im\n\njulia> istril(b)\ntrue\n\njulia> istril(b, -1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Tridiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Adjoint",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "D",
                "k"
            ],
            "arg_types": [
                "Diagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "SymTridiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UpperHessenberg",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Transpose",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{UnitUpperTriangular, UpperTriangular}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Bidiagonal",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Union{UnitUpperTriangular, UpperTriangular}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "istriu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nistriu(A::AbstractMatrix, k::Integer = 0) -> Bool\n```\n\nTest whether `A` is upper triangular starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = [1 2; 2 -1]\n2×2 Matrix{Int64}:\n 1   2\n 2  -1\n\njulia> istriu(a)\nfalse\n\njulia> istriu(a, -1)\ntrue\n\njulia> b = [1 im; 0 -1]\n2×2 Matrix{Complex{Int64}}:\n 1+0im   0+1im\n 0+0im  -1+0im\n\njulia> istriu(b)\ntrue\n\njulia> istriu(b, 1)\nfalse\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "BitMatrix",
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "BitVector",
                "BitVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "Number",
                "Union{Number, AbstractVecOrMat}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Adjoint{T, <:AbstractVector} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVector{T}",
                "AbstractVector{S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "Union{Adjoint{T, var\"#s997\"}, Transpose{T, var\"#s997\"}} where {T, var\"#s997\"<:(AbstractVector)}",
                "Union{Adjoint{T, var\"#s997\"}, Transpose{T, var\"#s997\"}} where {T, var\"#s997\"<:(AbstractVector)}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat{T}",
                "AbstractVecOrMat{S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron(A, B)\n```\n\nComputes the Kronecker product of two vectors, matrices or numbers.\n\nFor real vectors `v` and `w`, the Kronecker product is related to the outer product by `kron(v,w) == vec(w * transpose(v))` or `w * transpose(v) == reshape(kron(v,w), (length(w), length(v)))`. Note how the ordering of `v` and `w` differs on the left and right of these expressions (due to column-major storage). For complex vectors, the outer product `w * v'` also differs by conjugation of `v`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> B = [im 1; 1 -im]\n2×2 Matrix{Complex{Int64}}:\n 0+1im  1+0im\n 1+0im  0-1im\n\njulia> kron(A, B)\n4×4 Matrix{Complex{Int64}}:\n 0+1im  1+0im  0+2im  2+0im\n 1+0im  0-1im  2+0im  0-2im\n 0+3im  3+0im  0+4im  4+0im\n 3+0im  0-3im  4+0im  0-4im\n\njulia> v = [1, 2]; w = [3, 4, 5];\n\njulia> w*transpose(v)\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n\njulia> reshape(kron(v,w), (length(w), length(v)))\n3×2 Matrix{Int64}:\n 3   6\n 4   8\n 5  10\n```\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "R",
                "a",
                "b"
            ],
            "arg_types": [
                "BitMatrix",
                "BitMatrix",
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "R",
                "a",
                "b"
            ],
            "arg_types": [
                "BitVector",
                "BitVector",
                "BitVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "c",
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVector",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "c",
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "AbstractVecOrMat",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "c",
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Number",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "kron!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "AbstractVecOrMat",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nkron!(C, A, B)\n```\n\nComputes the Kronecker product of `A` and `B` and stores the result in `C`, overwriting the existing content of `C`. This is the in-place version of [`kron`](@ref).\n\n!!! compat \"Julia 1.6\"\n    This function requires Julia 1.6 or later.\n\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "Cholesky{T, <:StridedMatrix{T} where T}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "Cholesky",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "T",
                "D",
                "S"
            ],
            "arg_types": [
                "Tridiagonal",
                "Diagonal",
                "Union{SymTridiagonal, Tridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.QRCompactWY{T, M, C} where {M<:AbstractMatrix{T}, C<:AbstractMatrix{T}}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "b"
            ],
            "arg_types": [
                "LinearAlgebra.QRCompactWY{T, M, C} where {M<:AbstractMatrix{T}, C<:AbstractMatrix{T}}",
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "S",
                "B"
            ],
            "arg_types": [
                "LDLt{<:Any, <:SymTridiagonal}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LU{T, Tridiagonal{T, V}}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LU{T, <:StridedMatrix{T} where T}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LU",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "s",
                "X"
            ],
            "arg_types": [
                "Number",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n\n```\nldiv!(a::Number, B::AbstractArray)\n```\n\nDivide each entry in an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rdiv!`](@ref) to divide scalar from right.\n\n# Examples\n\n```jldoctest\njulia> B = [1.0 2.0; 3.0 4.0]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> ldiv!(2.0, B)\n2×2 Matrix{Float64}:\n 0.5  1.0\n 1.5  2.0\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{T, <:LU{T, <:StridedMatrix{T} where T}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:LU}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:LU{T, Tridiagonal{T, V}}}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Fadj",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:Union{QR, LinearAlgebra.QRCompactWY, QRPivoted}}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Fadj",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:LQ}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Any, <:Hessenberg}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "D",
                "A"
            ],
            "arg_types": [
                "UpperTriangular",
                "Diagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "D",
                "A"
            ],
            "arg_types": [
                "UpperTriangular",
                "Diagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Q",
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "D",
                "A"
            ],
            "arg_types": [
                "LowerTriangular",
                "Diagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "D",
                "A"
            ],
            "arg_types": [
                "LowerTriangular",
                "Diagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "QRPivoted{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "QRPivoted",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "QRPivoted{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}",
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "b"
            ],
            "arg_types": [
                "QRPivoted",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B",
                "rcond"
            ],
            "arg_types": [
                "QRPivoted{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}",
                "AbstractMatrix{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Dc",
                "Da",
                "Db"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Hessenberg{<:Complex, <:Any, <:AbstractMatrix{<:Real}}",
                "AbstractVecOrMat{<:Complex}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "Hessenberg",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "SVD{T, Tr, M} where {Tr, M<:(AbstractArray{T})}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "B",
                "R"
            ],
            "arg_types": [
                "BunchKaufman{T, <:StridedMatrix{T} where T}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "B",
                "R"
            ],
            "arg_types": [
                "BunchKaufman{T, <:StridedMatrix{T} where T}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "SymTridiagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "J",
                "B"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "QR",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "QR{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "F",
                "B"
            ],
            "arg_types": [
                "UpperHessenberg",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "CholeskyPivoted{T, <:StridedMatrix{T} where T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "CholeskyPivoted",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "CholeskyPivoted{T, <:StridedMatrix{T} where T}",
                "StridedVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "CholeskyPivoted",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "Q",
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "B",
                "D",
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Diagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Y",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVector",
                "Factorization",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Y",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Factorization",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Y",
                "s",
                "X"
            ],
            "arg_types": [
                "AbstractArray",
                "Number",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractTriangular",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "c",
                "A",
                "b"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Bidiagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "c",
                "A",
                "b"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Union{Adjoint{var\"#s997\", var\"#s128\"}, Transpose{var\"#s997\", var\"#s128\"}} where {var\"#s997\", var\"#s128\"<:Bidiagonal}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Y",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractArray",
                "AbstractMatrix",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "Y",
                "J",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "UniformScaling",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(Y, A, B) -> Y\n```\n\nCompute `A \\ B` in-place and store the result in `Y`, returning the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = zero(X);\n\njulia> ldiv!(Y, qr(A), X);\n\njulia> Y ≈ A\\X\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "transA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{T, <:LU{T, <:StridedMatrix{T} where T}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "transA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{<:Any, <:LU{T, Tridiagonal{T, V}}}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "transA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization{<:Any, <:LU}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "b"
            ],
            "arg_types": [
                "Bidiagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldiv!",
            "arg_names": [
                "A",
                "b"
            ],
            "arg_types": [
                "Union{Adjoint{var\"#s997\", var\"#s128\"}, Transpose{var\"#s997\", var\"#s128\"}} where {var\"#s997\", var\"#s128\"<:Bidiagonal}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldiv!(A, B)\n```\n\nCompute `A \\ B` in-place and overwriting `B` to store the result.\n\nThe argument `A` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `ldiv!` usually also require fine-grained control over the factorization of `A`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2.2 4; 3.1 0.2 3; 4 1 2];\n\njulia> X = [1; 2.5; 3];\n\njulia> Y = copy(X);\n\njulia> ldiv!(qr(A), X);\n\njulia> X ≈ A\\Y\ntrue\n```\n"
        },
        {
            "name": "ldlt",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldlt(S::SymTridiagonal) -> LDLt\n```\n\nCompute an `LDLt` (i.e., $LDL^T$) factorization of the real symmetric tridiagonal matrix `S` such that `S = L*Diagonal(d)*L'` where `L` is a unit lower triangular matrix and `d` is a vector. The main use of an `LDLt` factorization `F = ldlt(S)` is to solve the linear system of equations `Sx = b` with `F\\b`.\n\nSee also [`bunchkaufman`](@ref) for a similar, but pivoted, factorization of arbitrary symmetric or Hermitian matrices.\n\n# Examples\n\n```jldoctest\njulia> S = SymTridiagonal([3., 4., 5.], [1., 2.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0  1.0   ⋅\n 1.0  4.0  2.0\n  ⋅   2.0  5.0\n\njulia> ldltS = ldlt(S);\n\njulia> b = [6., 7., 8.];\n\njulia> ldltS \\ b\n3-element Vector{Float64}:\n 1.7906976744186047\n 0.627906976744186\n 1.3488372093023255\n\njulia> S \\ b\n3-element Vector{Float64}:\n 1.7906976744186047\n 0.627906976744186\n 1.3488372093023255\n```\n"
        },
        {
            "name": "ldlt!",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal{T, V}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nldlt!(S::SymTridiagonal) -> LDLt\n```\n\nSame as [`ldlt`](@ref), but saves space by overwriting the input `S`, instead of creating a copy.\n\n# Examples\n\n```jldoctest\njulia> S = SymTridiagonal([3., 4., 5.], [1., 2.])\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0  1.0   ⋅\n 1.0  4.0  2.0\n  ⋅   2.0  5.0\n\njulia> ldltS = ldlt!(S);\n\njulia> ldltS === S\nfalse\n\njulia> S\n3×3 SymTridiagonal{Float64, Vector{Float64}}:\n 3.0       0.333333   ⋅\n 0.333333  3.66667   0.545455\n  ⋅        0.545455  3.90909\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Tridiagonal",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.QRCompactWYQ{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractMatrix{T}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "G",
                "R"
            ],
            "arg_types": [
                "LinearAlgebra.Givens",
                "LinearAlgebra.Rotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "G",
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.Givens",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "x",
                "F"
            ],
            "arg_types": [
                "T",
                "Hessenberg{<:Any, <:SymTridiagonal{T, V} where V<:AbstractVector{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "x",
                "F"
            ],
            "arg_types": [
                "T",
                "Hessenberg{<:Any, <:UpperHessenberg{T, S} where S<:AbstractMatrix{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "x",
                "H"
            ],
            "arg_types": [
                "Number",
                "UpperHessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "c",
                "A"
            ],
            "arg_types": [
                "Number",
                "Union{LowerTriangular{T, S}, UnitLowerTriangular{T, S}, UnitUpperTriangular{T, S}, UpperTriangular{T, S}} where {T, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "s",
                "X"
            ],
            "arg_types": [
                "Number",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(a::Number, B::AbstractArray)\n```\n\nScale an array `B` by a scalar `a` overwriting `B` in-place.  Use [`rmul!`](@ref) to multiply scalar from right.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between `a` and an element of `B`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `B` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> B = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> lmul!(2, B)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> lmul!(0.0, [Inf])\n1-element Vector{Float64}:\n NaN\n```\n\n```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjR",
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointRotation{<:Any, <:LinearAlgebra.Rotation}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "T"
            ],
            "arg_types": [
                "Diagonal",
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "A"
            ],
            "arg_types": [
                "Diagonal",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "J",
                "B"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "R",
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.Rotation",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.QRPackedQ{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.QRPackedQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LQ",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "Q",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.HessenbergQ{T}",
                "Adjoint{T, <:StridedVecOrMat{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "Q",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, true}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "Q",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, false}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T}}",
                "Adjoint{T, <:StridedVecOrMat{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ}",
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjA",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRCompactWYQ{T, var\"#s127\", C} where {var\"#s127\"<:(StridedMatrix{T} where T), C<:AbstractMatrix{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRCompactWYQ{T, var\"#s127\", C} where {var\"#s127\"<:(StridedMatrix{T} where T), C<:AbstractMatrix{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ{T, var\"#s127\", C} where {var\"#s127\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "B"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ{T, var\"#s127\", C} where {var\"#s127\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, false}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "lmul!",
            "arg_names": [
                "adjQ",
                "X"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, true}}",
                "StridedVecOrMat{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `B`, and return the result. Here, `A` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> B = [0 1; 1 0];\n\njulia> A = UpperTriangular([1 2; 0 3]);\n\njulia> lmul!(A, B);\n\njulia> B\n2×2 Matrix{Int64}:\n 2  1\n 3  0\n\njulia> B = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> lmul!(F.Q, B)\n2×2 Matrix{Float64}:\n 3.0  4.0\n 1.0  2.0\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "BunchKaufman"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LDLt{<:Any, <:SymTridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LU{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "UpperHessenberg"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{LowerTriangular{T, S} where S<:AbstractMatrix{T}, UpperTriangular{T, S} where S<:AbstractMatrix{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Hessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logabsdet",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Union{Cholesky, CholeskyPivoted}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogabsdet(M)\n```\n\nLog of absolute value of matrix determinant. Equivalent to `(log(abs(det(M))), sign(det(M)))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> A = [-1. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n -1.0  0.0\n  0.0  1.0\n\njulia> det(A)\n-1.0\n\njulia> logabsdet(A)\n(0.0, -1.0)\n\njulia> B = [2. 0.; 0. 1.]\n2×2 Matrix{Float64}:\n 2.0  0.0\n 0.0  1.0\n\njulia> det(B)\n2.0\n\njulia> logabsdet(B)\n(0.6931471805599453, 1.0)\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "Cholesky"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "CholeskyPivoted"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Hessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Complex, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "logdet",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlogdet(M)\n```\n\nLog of matrix determinant. Equivalent to `log(det(M))`, but may provide increased accuracy and/or speed.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0; 2 2]\n2×2 Matrix{Int64}:\n 1  0\n 2  2\n\njulia> logdet(M)\n0.6931471805599453\n\njulia> logdet(Matrix(I, 3, 3))\n0.0\n```\n"
        },
        {
            "name": "lowrankdowndate",
            "arg_names": [
                "C",
                "v"
            ],
            "arg_types": [
                "Cholesky",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlowrankdowndate(C::Cholesky, v::AbstractVector) -> CC::Cholesky\n```\n\nDowndate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U - v*v')` but the computation of `CC` only uses `O(n^2)` operations.\n"
        },
        {
            "name": "lowrankdowndate!",
            "arg_names": [
                "C",
                "v"
            ],
            "arg_types": [
                "Cholesky",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlowrankdowndate!(C::Cholesky, v::AbstractVector) -> CC::Cholesky\n```\n\nDowndate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U - v*v')` but the computation of `CC` only uses `O(n^2)` operations. The input factorization `C` is updated in place such that on exit `C == CC`. The vector `v` is destroyed during the computation.\n"
        },
        {
            "name": "lowrankupdate",
            "arg_names": [
                "C",
                "v"
            ],
            "arg_types": [
                "Cholesky",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlowrankupdate(C::Cholesky, v::AbstractVector) -> CC::Cholesky\n```\n\nUpdate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U + v*v')` but the computation of `CC` only uses `O(n^2)` operations.\n"
        },
        {
            "name": "lowrankupdate!",
            "arg_names": [
                "C",
                "v"
            ],
            "arg_types": [
                "Cholesky",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlowrankupdate!(C::Cholesky, v::AbstractVector) -> CC::Cholesky\n```\n\nUpdate a Cholesky factorization `C` with the vector `v`. If `A = C.U'C.U` then `CC = cholesky(C.U'C.U + v*v')` but the computation of `CC` only uses `O(n^2)` operations. The input factorization `C` is updated in place such that on exit `C == CC`. The vector `v` is destroyed during the computation.\n"
        },
        {
            "name": "lq",
            "arg_names": [
                "Q",
                "arg..."
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ{T}",
                ""
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlq(A) -> S::LQ\n```\n\nCompute the LQ decomposition of `A`. The decomposition's lower triangular component can be obtained from the [`LQ`](@ref) object `S` via `S.L`, and the orthogonal/unitary component via `S.Q`, such that `A ≈ S.L*S.Q`.\n\nIterating the decomposition produces the components `S.L` and `S.Q`.\n\nThe LQ decomposition is the QR decomposition of `transpose(A)`, and it is useful in order to compute the minimum-norm solution `lq(A) \\ b` to an underdetermined system of equations (`A` has more columns than rows, but has full row rank).\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> S = lq(A)\nLQ{Float64, Matrix{Float64}, Vector{Float64}}\nL factor:\n2×2 Matrix{Float64}:\n -8.60233   0.0\n  4.41741  -0.697486\nQ factor: 2×2 LinearAlgebra.LQPackedQ{Float64, Matrix{Float64}, Vector{Float64}}\n\njulia> S.L * S.Q\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> l, q = S; # destructuring via iteration\n\njulia> l == S.L &&  q == S.Q\ntrue\n```\n"
        },
        {
            "name": "lq",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlq(A) -> S::LQ\n```\n\nCompute the LQ decomposition of `A`. The decomposition's lower triangular component can be obtained from the [`LQ`](@ref) object `S` via `S.L`, and the orthogonal/unitary component via `S.Q`, such that `A ≈ S.L*S.Q`.\n\nIterating the decomposition produces the components `S.L` and `S.Q`.\n\nThe LQ decomposition is the QR decomposition of `transpose(A)`, and it is useful in order to compute the minimum-norm solution `lq(A) \\ b` to an underdetermined system of equations (`A` has more columns than rows, but has full row rank).\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> S = lq(A)\nLQ{Float64, Matrix{Float64}, Vector{Float64}}\nL factor:\n2×2 Matrix{Float64}:\n -8.60233   0.0\n  4.41741  -0.697486\nQ factor: 2×2 LinearAlgebra.LQPackedQ{Float64, Matrix{Float64}, Vector{Float64}}\n\njulia> S.L * S.Q\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> l, q = S; # destructuring via iteration\n\njulia> l == S.L &&  q == S.Q\ntrue\n```\n"
        },
        {
            "name": "lq",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlq(A) -> S::LQ\n```\n\nCompute the LQ decomposition of `A`. The decomposition's lower triangular component can be obtained from the [`LQ`](@ref) object `S` via `S.L`, and the orthogonal/unitary component via `S.Q`, such that `A ≈ S.L*S.Q`.\n\nIterating the decomposition produces the components `S.L` and `S.Q`.\n\nThe LQ decomposition is the QR decomposition of `transpose(A)`, and it is useful in order to compute the minimum-norm solution `lq(A) \\ b` to an underdetermined system of equations (`A` has more columns than rows, but has full row rank).\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> S = lq(A)\nLQ{Float64, Matrix{Float64}, Vector{Float64}}\nL factor:\n2×2 Matrix{Float64}:\n -8.60233   0.0\n  4.41741  -0.697486\nQ factor: 2×2 LinearAlgebra.LQPackedQ{Float64, Matrix{Float64}, Vector{Float64}}\n\njulia> S.L * S.Q\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> l, q = S; # destructuring via iteration\n\njulia> l == S.L &&  q == S.Q\ntrue\n```\n"
        },
        {
            "name": "lq!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s997\"} where var\"#s997\"<:Union{Float32, Float64, ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlq!(A) -> LQ\n```\n\nCompute the [`LQ`](@ref) factorization of `A`, using the input matrix as a workspace. See also [`lq`](@ref).\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "A",
                "pivot"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "Union{NoPivot, RowMaximum, RowNonZero}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{true}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n\n```\nlu(A::AbstractSparseMatrixCSC; check = true, q = nothing, control = get_umfpack_control()) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`.\n\nFor sparse `A` with real or complex element type, the return type of `F` is `UmfpackLU{Tv, Ti}`, with `Tv` = [`Float64`](@ref) or `ComplexF64` respectively and `Ti` is an integer type ([`Int32`](@ref) or [`Int64`](@ref)).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero-based, a zero-based copy is made.\n\nThe `control` vector defaults to the package's default configuration for UMFPACK, but can be changed by passing a vector of length `UMFPACK_CONTROL`. See the UMFPACK manual for possible configurations. The corresponding variables are named `JL_UMFPACK_` since Julia uses one-based indexing.\n\nThe individual components of the factorization `F` can be accessed by indexing:\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `L`       | `L` (lower triangular) part of `LU` |\n| `U`       | `U` (upper triangular) part of `LU` |\n| `p`       | right permutation `Vector`          |\n| `q`       | left permutation `Vector`           |\n| `Rs`      | `Vector` of scaling factors         |\n| `:`       | `(L,U,p,q,Rs)` components           |\n\nThe relation between `F` and `A` is\n\n`F.L*F.U == (F.Rs .* A)[F.p, F.q]`\n\n`F` further supports the following functions:\n\n  * [`\\`](@ref)\n  * [`det`](@ref)\n\nSee also [`lu!`](@ref)\n\n!!! note\n    `lu(A::AbstractSparseMatrixCSC)` uses the UMFPACK[^ACM832] library that is part of [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n[^ACM832]: Davis, Timothy A. (2004b). Algorithm 832: UMFPACK V4.3–-an Unsymmetric-Pattern Multifrontal Method. ACM Trans. Math. Softw., 30(2), 196–199. [doi:10.1145/992200.992206](https://doi.org/10.1145/992200.992206)\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n\n```\nlu(A::AbstractSparseMatrixCSC; check = true, q = nothing, control = get_umfpack_control()) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`.\n\nFor sparse `A` with real or complex element type, the return type of `F` is `UmfpackLU{Tv, Ti}`, with `Tv` = [`Float64`](@ref) or `ComplexF64` respectively and `Ti` is an integer type ([`Int32`](@ref) or [`Int64`](@ref)).\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero-based, a zero-based copy is made.\n\nThe `control` vector defaults to the package's default configuration for UMFPACK, but can be changed by passing a vector of length `UMFPACK_CONTROL`. See the UMFPACK manual for possible configurations. The corresponding variables are named `JL_UMFPACK_` since Julia uses one-based indexing.\n\nThe individual components of the factorization `F` can be accessed by indexing:\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `L`       | `L` (lower triangular) part of `LU` |\n| `U`       | `U` (upper triangular) part of `LU` |\n| `p`       | right permutation `Vector`          |\n| `q`       | left permutation `Vector`           |\n| `Rs`      | `Vector` of scaling factors         |\n| `:`       | `(L,U,p,q,Rs)` components           |\n\nThe relation between `F` and `A` is\n\n`F.L*F.U == (F.Rs .* A)[F.p, F.q]`\n\n`F` further supports the following functions:\n\n  * [`\\`](@ref)\n  * [`det`](@ref)\n\nSee also [`lu!`](@ref)\n\n!!! note\n    `lu(A::AbstractSparseMatrixCSC)` uses the UMFPACK[^ACM832] library that is part of [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n[^ACM832]: Davis, Timothy A. (2004b). Algorithm 832: UMFPACK V4.3–-an Unsymmetric-Pattern Multifrontal Method. ACM Trans. Math. Softw., 30(2), 196–199. [doi:10.1145/992200.992206](https://doi.org/10.1145/992200.992206)\n"
        },
        {
            "name": "lu",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "LU"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu(A, pivot = RowMaximum(); check = true) -> F::LU\n```\n\nCompute the LU factorization of `A`.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nIn most cases, if `A` is a subtype `S` of `AbstractMatrix{T}` with an element type `T` supporting `+`, `-`, `*` and `/`, the return type is `LU{T,S{T}}`.\n\nIn general, LU factorization involves a permutation of the rows of the matrix (corresponding to the `F.p` output described below), known as \"pivoting\" (because it corresponds to choosing which row contains the \"pivot\", the diagonal entry of `F.U`). One of the following pivoting strategies can be selected via the optional `pivot` argument:\n\n  * `RowMaximum()` (default): the standard pivoting strategy; the pivot corresponds to the element of maximum absolute value among the remaining, to be factorized rows. This pivoting strategy requires the element type to also support [`abs`](@ref) and [`<`](@ref). (This is generally the only numerically stable option for floating-point matrices.)\n  * `RowNonZero()`: the pivot corresponds to the first non-zero element among the remaining, to be factorized rows.  (This corresponds to the typical choice in hand calculations, and is also useful for more general algebraic number types that support [`iszero`](@ref) but not `abs` or `<`.)\n  * `NoPivot()`: pivoting turned off (may fail if a zero entry is encountered).\n\nThe individual components of the factorization `F` can be accessed via [`getproperty`](@ref):\n\n| Component | Description                         |\n|:--------- |:----------------------------------- |\n| `F.L`     | `L` (lower triangular) part of `LU` |\n| `F.U`     | `U` (upper triangular) part of `LU` |\n| `F.p`     | (right) permutation `Vector`        |\n| `F.P`     | (right) permutation `Matrix`        |\n\nIterating the factorization produces the components `F.L`, `F.U`, and `F.p`.\n\nThe relationship between `F` and `A` is\n\n`F.L*F.U == A[F.p, :]`\n\n`F` further supports the following functions:\n\n| Supported function  | `LU` | `LU{T,Tridiagonal{T}}` |\n|:------------------- |:---- |:---------------------- |\n| [`/`](@ref)         | ✓    |                        |\n| [`\\`](@ref)         | ✓    | ✓                      |\n| [`inv`](@ref)       | ✓    | ✓                      |\n| [`det`](@ref)       | ✓    | ✓                      |\n| [`logdet`](@ref)    | ✓    | ✓                      |\n| [`logabsdet`](@ref) | ✓    | ✓                      |\n| [`size`](@ref)      | ✓    | ✓                      |\n\n# Examples\n\n```jldoctest\njulia> A = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> F = lu(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> F.L * F.U == A[F.p, :]\ntrue\n\njulia> l, u, p = lu(A); # destructuring via iteration\n\njulia> l == F.L && u == F.U && p == F.p\ntrue\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                "pivot"
            ],
            "arg_types": [
                "Tridiagonal{T, V}",
                "Union{NoPivot, RowMaximum}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Tridiagonal{T, V}"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "RowMaximum"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                "pivot"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where S",
                "Union{NoPivot, RowMaximum, RowNonZero}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                "pivot"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{NoPivot, RowMaximum, RowNonZero}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s126\"} where var\"#s126\"<:Union{Float32, Float64, ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Symmetric{T, S}} where S"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
                "..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{T, S} where {T, S}, Symmetric{T, S} where {T, S}, Tridiagonal, StridedMatrix}",
                "Val{true}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lu!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "Union{Hermitian{T, S} where {T, S}, Symmetric{T, S} where {T, S}, Tridiagonal, StridedMatrix}",
                "Val{false}"
            ],
            "kwarg_names": [
                "check"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlu!(A, pivot = RowMaximum(); check = true) -> LU\n```\n\n`lu!` is the same as [`lu`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n# Examples\n\n```jldoctest\njulia> A = [4. 3.; 6. 3.]\n2×2 Matrix{Float64}:\n 4.0  3.0\n 6.0  3.0\n\njulia> F = lu!(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n2×2 Matrix{Float64}:\n 1.0       0.0\n 0.666667  1.0\nU factor:\n2×2 Matrix{Float64}:\n 6.0  3.0\n 0.0  1.0\n\njulia> iA = [4 3; 6 3]\n2×2 Matrix{Int64}:\n 4  3\n 6  3\n\njulia> lu!(iA)\nERROR: InexactError: Int64(0.6666666666666666)\nStacktrace:\n[...]\n```\n\n```\nlu!(F::UmfpackLU, A::AbstractSparseMatrixCSC; check=true, reuse_symbolic=true, q=nothing) -> F::UmfpackLU\n```\n\nCompute the LU factorization of a sparse matrix `A`, reusing the symbolic factorization of an already existing LU factorization stored in `F`. Unless `reuse_symbolic` is set to false, the sparse matrix `A` must have an identical nonzero pattern as the matrix used to create the LU factorization `F`, otherwise an error is thrown. If the size of `A` and `F` differ, all vectors will be resized accordingly.\n\nWhen `check = true`, an error is thrown if the decomposition fails. When `check = false`, responsibility for checking the decomposition's validity (via [`issuccess`](@ref)) lies with the user.\n\nThe permutation `q` can either be a permutation vector or `nothing`. If no permutation vector is provided or `q` is `nothing`, UMFPACK's default is used. If the permutation is not zero based, a zero based copy is made.\n\nSee also [`lu`](@ref)\n\n!!! note\n    `lu!(F::UmfpackLU, A::AbstractSparseMatrixCSC)` uses the UMFPACK library that is part of SuiteSparse. As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, `lu!` will automatically convert the types to those set by the LU factorization or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n!!! compat \"Julia 1.5\"\n    `lu!` for `UmfpackLU` requires at least Julia 1.5.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse(Float64[1.0 2.0; 0.0 3.0]);\n\njulia> F = lu(A);\n\njulia> B = sparse(Float64[1.0 1.0; 0.0 1.0]);\n\njulia> lu!(F, B);\n\njulia> F \\ ones(2)\n2-element Vector{Float64}:\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "lyap",
            "arg_names": [
                "a",
                "c"
            ],
            "arg_types": [
                "Union{Real, Complex}",
                "Union{Real, Complex}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n2×2 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' ≈ -B\ntrue\n```\n"
        },
        {
            "name": "lyap",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n2×2 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' ≈ -B\ntrue\n```\n"
        },
        {
            "name": "lyap",
            "arg_names": [
                "A",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nlyap(A, C)\n```\n\nComputes the solution `X` to the continuous Lyapunov equation `AX + XA' + C = 0`, where no eigenvalue of `A` has a zero real part and no two eigenvalues are negative complex conjugates of each other.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> X = lyap(A, B)\n2×2 Matrix{Float64}:\n  0.5  -0.5\n -0.5   0.25\n\njulia> A*X + X*A' ≈ -B\ntrue\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "A",
                "B",
                "C",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular",
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "A",
                "B",
                "C",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "out",
                "a",
                "B",
                "α",
                "β"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "Number",
                "UniformScaling",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "s",
                "X",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractArray",
                "Number",
                "AbstractArray",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractVector",
                "LinearAlgebra.AbstractTriangular",
                "AbstractVector",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "AbstractVector",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "AbstractMatrix",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "AbstractMatrix",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "X",
                "s",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractArray",
                "AbstractArray",
                "Number",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "J",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "UniformScaling",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "AbstractVector",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "y",
                "A",
                "x",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVecOrMat",
                "AbstractVector",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Bidiagonal, Diagonal, SymTridiagonal, Tridiagonal}",
                "AbstractVector",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B",
                "α",
                "β"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVecOrMat",
                "AbstractVecOrMat",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "Q"
            ],
            "arg_types": [
                "AbstractVecOrMat{T}",
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractQ{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "out",
                "A",
                "b",
                "α",
                "β"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling",
                "Number",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "J",
                "B",
                "alpha",
                "beta"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "UniformScaling",
                "AbstractVecOrMat",
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n\n```\nmul!(C, A, B, α, β) -> C\n```\n\nCombined inplace matrix-matrix or matrix-vector multiply-add $A B α + C β$. The result is stored in `C` by overwriting it.  Note that `C` must not be aliased with either `A` or `B`.\n\n!!! compat \"Julia 1.3\"\n    Five-argument `mul!` requires at least Julia 1.3.\n\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; C=[1.0 2.0; 3.0 4.0];\n\njulia> mul!(C, A, B, 100.0, 10.0) === C\ntrue\n\njulia> C\n2×2 Matrix{Float64}:\n 310.0  320.0\n 730.0  740.0\n```\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "Q",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat{T}",
                "LinearAlgebra.AbstractQ{T}",
                "Union{LinearAlgebra.AbstractQ, AbstractVecOrMat}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "mul!",
            "arg_names": [
                "C",
                "A",
                "B"
            ],
            "arg_types": [
                "",
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nmul!(Y, A, B) -> Y\n```\n\nCalculates the matrix-matrix or matrix-vector product $AB$ and stores the result in `Y`, overwriting the existing value of `Y`. Note that `Y` must not be aliased with either `A` or `B`.\n\n# Examples\n\n```jldoctest\njulia> A=[1.0 2.0; 3.0 4.0]; B=[1.0 1.0; 1.0 1.0]; Y = similar(B); mul!(Y, A, B);\n\njulia> Y\n2×2 Matrix{Float64}:\n 3.0  3.0\n 7.0  7.0\n```\n\n# Implementation\n\nFor custom matrix and vector types, it is recommended to implement 5-argument `mul!` rather than implementing 3-argument `mul!` directly if possible.\n"
        },
        {
            "name": "norm",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "Missing"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "",
                "p"
            ],
            "arg_types": [
                "Missing",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "x",
                "p"
            ],
            "arg_types": [
                "Number",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "x",
                "rx"
            ],
            "arg_types": [
                "StridedVector{T}",
                "Union{AbstractRange{TI}, UnitRange{TI}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n\n```\nnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$.\n\n# Examples\n\n```jldoctest\njulia> norm(2, 1)\n2.0\n\njulia> norm(-2, 1)\n2.0\n\njulia> norm(2, 2)\n2.0\n\njulia> norm(-2, 2)\n2.0\n\njulia> norm(2, Inf)\n2.0\n\njulia> norm(-2, Inf)\n2.0\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "v",
                "p"
            ],
            "arg_types": [
                "Union{Adjoint{T, S}, Transpose{T, S}} where {T, S}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "itr"
            ],
            "arg_types": [
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
        },
        {
            "name": "norm",
            "arg_names": [
                "itr",
                "p"
            ],
            "arg_types": [
                "",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnorm(A, p::Real=2)\n```\n\nFor any iterable container `A` (including arrays of any dimension) of numbers (or any element type for which `norm` is defined), compute the `p`-norm (defaulting to `p=2`) as if `A` were a vector of the corresponding length.\n\nThe `p`-norm is defined as\n\n$$\n\\|A\\|_p = \\left( \\sum_{i=1}^n | a_i | ^p \\right)^{1/p}\n$$\n\nwith $a_i$ the entries of $A$, $| a_i |$ the [`norm`](@ref) of $a_i$, and $n$ the length of $A$. Since the `p`-norm is computed using the [`norm`](@ref)s of the entries of `A`, the `p`-norm of a vector of vectors is not compatible with the interpretation of it as a block vector in general if `p != 2`.\n\n`p` can assume any numeric value (even though not all values produce a mathematically valid vector norm). In particular, `norm(A, Inf)` returns the largest value in `abs.(A)`, whereas `norm(A, -Inf)` returns the smallest. If `A` is a matrix and `p=2`, then this is equivalent to the Frobenius norm.\n\nThe second argument `p` is not necessarily a part of the interface for `norm`, i.e. a custom type may only implement `norm(A)` without second argument.\n\nUse [`opnorm`](@ref) to compute the operator norm of a matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [3, -2, 6]\n3-element Vector{Int64}:\n  3\n -2\n  6\n\njulia> norm(v)\n7.0\n\njulia> norm(v, 1)\n11.0\n\njulia> norm(v, Inf)\n6.0\n\njulia> norm([1 2 3; 4 5 6; 7 8 9])\n16.881943016134134\n\njulia> norm([1 2 3 4 5 6 7 8 9])\n16.881943016134134\n\njulia> norm(1:9)\n16.881943016134134\n\njulia> norm(hcat(v,v), 1) == norm(vcat(v,v), 1) != norm([v,v], 1)\ntrue\n\njulia> norm(hcat(v,v), 2) == norm(vcat(v,v), 2) == norm([v,v], 2)\ntrue\n\njulia> norm(hcat(v,v), Inf) == norm(vcat(v,v), Inf) != norm([v,v], Inf)\ntrue\n```\n"
        },
        {
            "name": "normalize",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize(a, p::Real=2)\n```\n\nNormalize `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. For scalars, this is similar to sign(a), except normalize(0) = NaN. See also [`normalize!`](@ref), [`norm`](@ref), and [`sign`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n2×3 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n2×3 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\njulia> normalize(3, 1)\n1.0\n\njulia> normalize(-8, 1)\n-1.0\n\njulia> normalize(0, 1)\nNaN\n```\n"
        },
        {
            "name": "normalize",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize(a, p::Real=2)\n```\n\nNormalize `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. For scalars, this is similar to sign(a), except normalize(0) = NaN. See also [`normalize!`](@ref), [`norm`](@ref), and [`sign`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n2×3 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n2×3 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\njulia> normalize(3, 1)\n1.0\n\njulia> normalize(-8, 1)\n-1.0\n\njulia> normalize(0, 1)\nNaN\n```\n"
        },
        {
            "name": "normalize",
            "arg_names": [
                "a",
                "p"
            ],
            "arg_types": [
                "AbstractArray",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize(a, p::Real=2)\n```\n\nNormalize `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. For scalars, this is similar to sign(a), except normalize(0) = NaN. See also [`normalize!`](@ref), [`norm`](@ref), and [`sign`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n2×3 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n2×3 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\njulia> normalize(3, 1)\n1.0\n\njulia> normalize(-8, 1)\n-1.0\n\njulia> normalize(0, 1)\nNaN\n```\n"
        },
        {
            "name": "normalize",
            "arg_names": [
                "x",
                "p"
            ],
            "arg_types": [
                "",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize(a, p::Real=2)\n```\n\nNormalize `a` so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. For scalars, this is similar to sign(a), except normalize(0) = NaN. See also [`normalize!`](@ref), [`norm`](@ref), and [`sign`](@ref).\n\n# Examples\n\n```jldoctest\njulia> a = [1,2,4];\n\njulia> b = normalize(a)\n3-element Vector{Float64}:\n 0.2182178902359924\n 0.4364357804719848\n 0.8728715609439696\n\njulia> norm(b)\n1.0\n\njulia> c = normalize(a, 1)\n3-element Vector{Float64}:\n 0.14285714285714285\n 0.2857142857142857\n 0.5714285714285714\n\njulia> norm(c, 1)\n1.0\n\njulia> a = [1 2 4 ; 1 2 4]\n2×3 Matrix{Int64}:\n 1  2  4\n 1  2  4\n\njulia> norm(a)\n6.48074069840786\n\njulia> normalize(a)\n2×3 Matrix{Float64}:\n 0.154303  0.308607  0.617213\n 0.154303  0.308607  0.617213\n\njulia> normalize(3, 1)\n1.0\n\njulia> normalize(-8, 1)\n-1.0\n\njulia> normalize(0, 1)\nNaN\n```\n"
        },
        {
            "name": "normalize!",
            "arg_names": [
                "a",
                "p"
            ],
            "arg_types": [
                "AbstractArray",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize!(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` in-place so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize`](@ref) and [`norm`](@ref).\n"
        },
        {
            "name": "normalize!",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnormalize!(a::AbstractArray, p::Real=2)\n```\n\nNormalize the array `a` in-place so that its `p`-norm equals unity, i.e. `norm(a, p) == 1`. See also [`normalize`](@ref) and [`norm`](@ref).\n"
        },
        {
            "name": "nullspace",
            "arg_names": [
                "A",
                "tol"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes smaller than `max(atol, rtol*σ₁)`, where `σ₁` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n3×3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n3×3 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "nullspace",
            "arg_names": [
                "A",
                "tol"
            ],
            "arg_types": [
                "AbstractVector",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes smaller than `max(atol, rtol*σ₁)`, where `σ₁` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n3×3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n3×3 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "nullspace",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
                "atol",
                "rtol"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nnullspace(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nnullspace(M, rtol::Real) = nullspace(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes a basis for the nullspace of `M` by including the singular vectors of `M` whose singular values have magnitudes smaller than `max(atol, rtol*σ₁)`, where `σ₁` is `M`'s largest singular value.\n\nBy default, the relative tolerance `rtol` is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\n# Examples\n\n```jldoctest\njulia> M = [1 0 0; 0 1 0; 0 0 0]\n3×3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n 0  0  0\n\njulia> nullspace(M)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n\njulia> nullspace(M, rtol=3)\n3×3 Matrix{Float64}:\n 0.0  1.0  0.0\n 1.0  0.0  0.0\n 0.0  0.0  1.0\n\njulia> nullspace(M, atol=0.95)\n3×1 Matrix{Float64}:\n 0.0\n 0.0\n 1.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "v",
                "q"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1 × n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "v",
                "q"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "x",
                "p"
            ],
            "arg_types": [
                "Number",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "A",
                "p"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "J",
                "p"
            ],
            "arg_types": [
                "UniformScaling",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1 × n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
        },
        {
            "name": "opnorm",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nopnorm(A::AbstractMatrix, p::Real=2)\n```\n\nCompute the operator norm (or matrix norm) induced by the vector `p`-norm, where valid values of `p` are `1`, `2`, or `Inf`. (Note that for sparse matrices, `p=2` is currently not implemented.) Use [`norm`](@ref) to compute the Frobenius norm.\n\nWhen `p=1`, the operator norm is the maximum absolute column sum of `A`:\n\n$$\n\\|A\\|_1 = \\max_{1 ≤ j ≤ n} \\sum_{i=1}^m | a_{ij} |\n$$\n\nwith $a_{ij}$ the entries of $A$, and $m$ and $n$ its dimensions.\n\nWhen `p=2`, the operator norm is the spectral norm, equal to the largest singular value of `A`.\n\nWhen `p=Inf`, the operator norm is the maximum absolute row sum of `A`:\n\n$$\n\\|A\\|_\\infty = \\max_{1 ≤ i ≤ m} \\sum _{j=1}^n | a_{ij} |\n$$\n\n# Examples\n\n```jldoctest\njulia> A = [1 -2 -3; 2 3 -1]\n2×3 Matrix{Int64}:\n 1  -2  -3\n 2   3  -1\n\njulia> opnorm(A, Inf)\n6.0\n\njulia> opnorm(A, 1)\n5.0\n```\n\n```\nopnorm(x::Number, p::Real=2)\n```\n\nFor numbers, return $\\left( |x|^p \\right)^{1/p}$. This is equivalent to [`norm`](@ref).\n\n```\nopnorm(A::Adjoint{<:Any,<:AbstracVector}, q::Real=2)\nopnorm(A::Transpose{<:Any,<:AbstracVector}, q::Real=2)\n```\n\nFor Adjoint/Transpose-wrapped vectors, return the operator $q$-norm of `A`, which is equivalent to the `p`-norm with value `p = q/(q-1)`. They coincide at `p = q = 2`. Use [`norm`](@ref) to compute the `p` norm of `A` as a vector.\n\nThe difference in norm between a vector space and its dual arises to preserve the relationship between duality and the dot product, and the result is consistent with the operator `p`-norm of a `1 × n` matrix.\n\n# Examples\n\n```jldoctest\njulia> v = [1; im];\n\njulia> vc = v';\n\njulia> opnorm(vc, 1)\n1.0\n\njulia> norm(vc, 1)\n2.0\n\njulia> norm(v, 1)\n2.0\n\njulia> opnorm(vc, 2)\n1.4142135623730951\n\njulia> norm(vc, 2)\n1.4142135623730951\n\njulia> norm(v, 2)\n1.4142135623730951\n\njulia> opnorm(vc, Inf)\n2.0\n\njulia> norm(vc, Inf)\n1.0\n\njulia> norm(v, Inf)\n1.0\n```\n"
        },
        {
            "name": "ordschur",
            "arg_names": [
                "gschur",
                "select"
            ],
            "arg_types": [
                "GeneralizedSchur",
                "Union{BitVector, Vector{Bool}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nordschur(F::GeneralizedSchur, select::Union{Vector{Bool},BitVector}) -> F::GeneralizedSchur\n```\n\nReorders the Generalized Schur factorization `F` of a matrix pair `(A, B) = (Q*S*Z', Q*T*Z')` according to the logical array `select` and returns a GeneralizedSchur object `F`. The selected eigenvalues appear in the leading diagonal of both `F.S` and `F.T`, and the left and right orthogonal/unitary Schur vectors are also reordered such that `(A, B) = F.Q*(F.S, F.T)*F.Z'` still holds and the generalized eigenvalues of `A` and `B` can still be obtained with `F.α./F.β`.\n"
        },
        {
            "name": "ordschur",
            "arg_names": [
                "schur",
                "select"
            ],
            "arg_types": [
                "Schur",
                "Union{BitVector, Vector{Bool}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nordschur(F::Schur, select::Union{Vector{Bool},BitVector}) -> F::Schur\n```\n\nReorders the Schur factorization `F` of a matrix `A = Z*T*Z'` according to the logical array `select` returning the reordered factorization `F` object. The selected eigenvalues appear in the leading diagonal of `F.Schur` and the corresponding leading columns of `F.vectors` form an orthogonal/unitary basis of the corresponding right invariant subspace. In the real case, a complex conjugate pair of eigenvalues must be either both included or both excluded via `select`.\n"
        },
        {
            "name": "ordschur!",
            "arg_names": [
                "gschur",
                "select"
            ],
            "arg_types": [
                "GeneralizedSchur",
                "Union{BitVector, Vector{Bool}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nordschur!(F::GeneralizedSchur, select::Union{Vector{Bool},BitVector}) -> F::GeneralizedSchur\n```\n\nSame as `ordschur` but overwrites the factorization `F`.\n"
        },
        {
            "name": "ordschur!",
            "arg_names": [
                "schur",
                "select"
            ],
            "arg_types": [
                "Schur",
                "Union{BitVector, Vector{Bool}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nordschur!(F::Schur, select::Union{Vector{Bool},BitVector}) -> F::Schur\n```\n\nSame as [`ordschur`](@ref) but overwrites the factorization `F`.\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v",
                "tol"
            ],
            "arg_types": [
                "Transpose{T, <:AbstractVector} where T",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v",
                "tol"
            ],
            "arg_types": [
                "Adjoint{T, <:AbstractVector} where T",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "D",
                "tol"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "A",
                "tol"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v",
                "tol"
            ],
            "arg_types": [
                "AbstractVector{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v",
                "tol"
            ],
            "arg_types": [
                "AbstractVector{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v",
                "tol"
            ],
            "arg_types": [
                "AbstractVector{T}",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
                "atol",
                "rtol"
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "pinv",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\npinv(M; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\npinv(M, rtol::Real) = pinv(M; rtol=rtol) # to be deprecated in Julia 2.0\n```\n\nComputes the Moore-Penrose pseudoinverse.\n\nFor matrices `M` with floating point elements, it is convenient to compute the pseudoinverse by inverting only singular values greater than `max(atol, rtol*σ₁)` where `σ₁` is the largest singular value of `M`.\n\nThe optimal choice of absolute (`atol`) and relative tolerance (`rtol`) varies both with the value of `M` and the intended application of the pseudoinverse. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `M`, and `ϵ` is the [`eps`](@ref) of the element type of `M`.\n\nFor inverting dense ill-conditioned matrices in a least-squares sense, `rtol = sqrt(eps(real(float(oneunit(eltype(M))))))` is recommended.\n\nFor more information, see [^issue8859], [^B96], [^S84], [^KY88].\n\n# Examples\n\n```jldoctest\njulia> M = [1.5 1.3; 1.2 1.9]\n2×2 Matrix{Float64}:\n 1.5  1.3\n 1.2  1.9\n\njulia> N = pinv(M)\n2×2 Matrix{Float64}:\n  1.47287   -1.00775\n -0.930233   1.16279\n\njulia> M * N\n2×2 Matrix{Float64}:\n 1.0          -2.22045e-16\n 4.44089e-16   1.0\n```\n\n[^issue8859]: Issue 8859, \"Fix least squares\", [https://github.com/JuliaLang/julia/pull/8859](https://github.com/JuliaLang/julia/pull/8859)\n\n[^B96]: Åke Björck, \"Numerical Methods for Least Squares Problems\",  SIAM Press, Philadelphia, 1996, \"Other Titles in Applied Mathematics\", Vol. 51. [doi:10.1137/1.9781611971484](http://epubs.siam.org/doi/book/10.1137/1.9781611971484)\n\n[^S84]: G. W. Stewart, \"Rank Degeneracy\", SIAM Journal on Scientific and Statistical Computing, 5(2), 1984, 403-413. [doi:10.1137/0905030](http://epubs.siam.org/doi/abs/10.1137/0905030)\n\n[^KY88]: Konstantinos Konstantinides and Kung Yao, \"Statistical analysis of effective singular values in matrix rank determination\", IEEE Transactions on Acoustics, Speech and Signal Processing, 36(5), 1988, 757-763. [doi:10.1109/29.1585](https://doi.org/10.1109/29.1585)\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "Q",
                "arg..."
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ{T}",
                ""
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n\n```\nqr(A::SparseMatrixCSC; tol=_default_tol(A), ordering=ORDERING_DEFAULT) -> QRSparse\n```\n\nCompute the `QR` factorization of a sparse matrix `A`. Fill-reducing row and column permutations are used such that `F.R = F.Q'*A[F.prow,F.pcol]`. The main application of this type is to solve least squares or underdetermined problems with [`\\`](@ref). The function calls the C library SPQR[^ACM933].\n\n!!! note\n    `qr(A::SparseMatrixCSC)` uses the SPQR library that is part of [SuiteSparse](https://github.com/DrTimothyAldenDavis/SuiteSparse). As this library only supports sparse matrices with [`Float64`](@ref) or `ComplexF64` elements, as of Julia v1.4 `qr` converts `A` into a copy that is of type `SparseMatrixCSC{Float64}` or `SparseMatrixCSC{ComplexF64}` as appropriate.\n\n\n# Examples\n\n```jldoctest\njulia> A = sparse([1,2,3,4], [1,1,2,2], [1.0,1.0,1.0,1.0])\n4×2 SparseMatrixCSC{Float64, Int64} with 4 stored entries:\n 1.0   ⋅\n 1.0   ⋅\n  ⋅   1.0\n  ⋅   1.0\n\njulia> qr(A)\nSparseArrays.SPQR.QRSparse{Float64, Int64}\nQ factor:\n4×4 SparseArrays.SPQR.QRSparseQ{Float64, Int64}\nR factor:\n2×2 SparseMatrixCSC{Float64, Int64} with 2 stored entries:\n -1.41421    ⋅\n   ⋅       -1.41421\nRow permutation:\n4-element Vector{Int64}:\n 1\n 3\n 4\n 2\nColumn permutation:\n2-element Vector{Int64}:\n 1\n 2\n```\n\n[^ACM933]: Foster, L. V., & Davis, T. A. (2013). Algorithm 933: Reliable Calculation of Numerical Rank, Null Space Bases, Pseudoinverse Solutions, and Basic Solutions Using SuitesparseQR. ACM Trans. Math. Softw., 40(1). [doi:10.1145/2513109.2513116](https://doi.org/10.1145/2513109.2513116)\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{false}"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{true}"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "A",
                "arg..."
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                ""
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr",
            "arg_names": [
                "v"
            ],
            "arg_types": [
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr(A, pivot = NoPivot(); blocksize) -> F\n```\n\nCompute the QR factorization of the matrix `A`: an orthogonal (or unitary if `A` is complex-valued) matrix `Q`, and an upper triangular matrix `R` such that\n\n$$\nA = Q R\n$$\n\nThe returned object `F` stores the factorization in a packed format:\n\n  * if `pivot == ColumnNorm()` then `F` is a [`QRPivoted`](@ref) object,\n  * otherwise if the element type of `A` is a BLAS type ([`Float32`](@ref), [`Float64`](@ref), `ComplexF32` or `ComplexF64`), then `F` is a [`QRCompactWY`](@ref) object,\n  * otherwise `F` is a [`QR`](@ref) object.\n\nThe individual components of the decomposition `F` can be retrieved via property accessors:\n\n  * `F.Q`: the orthogonal/unitary matrix `Q`\n  * `F.R`: the upper triangular matrix `R`\n  * `F.p`: the permutation vector of the pivot ([`QRPivoted`](@ref) only)\n  * `F.P`: the permutation matrix of the pivot ([`QRPivoted`](@ref) only)\n\nIterating the decomposition produces the components `Q`, `R`, and if extant `p`.\n\nThe following functions are available for the `QR` objects: [`inv`](@ref), [`size`](@ref), and [`\\`](@ref). When `A` is rectangular, `\\` will return a least squares solution and if the solution is not unique, the one with smallest norm is returned. When `A` is not full rank, factorization with (column) pivoting is required to obtain a minimum norm solution.\n\nMultiplication with respect to either full/square or non-full/square `Q` is allowed, i.e. both `F.Q*F.R` and `F.Q*A` are supported. A `Q` matrix can be converted into a regular matrix with [`Matrix`](@ref). This operation returns the \"thin\" Q factor, i.e., if `A` is `m`×`n` with `m>=n`, then `Matrix(F.Q)` yields an `m`×`n` matrix with orthonormal columns.  To retrieve the \"full\" Q factor, an `m`×`m` orthogonal matrix, use `F.Q*I` or `collect(F.Q)`. If `m<=n`, then `Matrix(F.Q)` yields an `m`×`m` orthogonal matrix.\n\nThe block size for QR decomposition can be specified by keyword argument `blocksize :: Integer` when `pivot == NoPivot()` and `A isa StridedMatrix{<:BlasFloat}`. It is ignored when `blocksize > minimum(size(A))`. See [`QRCompactWY`](@ref).\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = [3.0 -6.0; 4.0 -8.0; 0.0 1.0]\n3×2 Matrix{Float64}:\n 3.0  -6.0\n 4.0  -8.0\n 0.0   1.0\n\njulia> F = qr(A)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -5.0  10.0\n  0.0  -1.0\n\njulia> F.Q * F.R == A\ntrue\n```\n\n!!! note\n    `qr` returns multiple types because LAPACK uses several representations that minimize the memory storage requirements of products of Householder elementary reflectors, so that the `Q` and `R` matrices can be stored compactly rather as two separate dense matrices.\n\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{false}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "Val{true}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "StridedMatrix{var\"#s997\"} where var\"#s997\"<:Union{Float32, Float64, ComplexF64, ComplexF32}",
                "ColumnNorm"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "ColumnNorm"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "StridedMatrix{var\"#s125\"} where var\"#s125\"<:Union{Float32, Float64, ComplexF64, ComplexF32}",
                "NoPivot"
            ],
            "kwarg_names": [
                "blocksize"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "qr!",
            "arg_names": [
                "A",
                ""
            ],
            "arg_types": [
                "AbstractMatrix",
                "NoPivot"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nqr!(A, pivot = NoPivot(); blocksize)\n```\n\n`qr!` is the same as [`qr`](@ref) when `A` is a subtype of [`AbstractMatrix`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. An [`InexactError`](@ref) exception is thrown if the factorization produces a number not representable by the element type of `A`, e.g. for integer types.\n\n!!! compat \"Julia 1.4\"\n    The `blocksize` keyword argument requires Julia 1.4 or later.\n\n\n# Examples\n\n```jldoctest\njulia> a = [1. 2.; 3. 4.]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> qr!(a)\nLinearAlgebra.QRCompactWY{Float64, Matrix{Float64}, Matrix{Float64}}\nQ factor: 2×2 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\nR factor:\n2×2 Matrix{Float64}:\n -3.16228  -4.42719\n  0.0      -0.632456\n\njulia> a = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> qr!(a)\nERROR: InexactError: Int64(3.1622776601683795)\nStacktrace:\n[...]\n```\n"
        },
        {
            "name": "rank",
            "arg_names": [
                "A",
                "tol"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Real"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the numerical rank of a matrix by counting how many outputs of `svdvals(A)` are greater than `max(atol, rtol*σ₁)` where `σ₁` is `A`'s largest calculated singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `A`, and `ϵ` is the [`eps`](@ref) of the element type of `A`.\n\n!!! note\n    Numerical rank can be a sensitive and imprecise characterization of ill-conditioned matrices with singular values that are close to the threshold tolerance `max(atol, rtol*σ₁)`. In such cases, slight perturbations to the singular-value computation or to the matrix can change the result of `rank` by pushing one or more singular values across the threshold. These variations can even occur due to changes in floating-point errors between different Julia versions, architectures, compilers, or operating systems.\n\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n\n```\nrank(::QRSparse{Tv,Ti}) -> Ti\n```\n\nReturn the rank of the QR factorization\n\n```\nrank(S::SparseMatrixCSC{Tv,Ti}; [tol::Real]) -> Ti\n```\n\nCalculate rank of `S` by calculating its QR factorization. Values smaller than `tol` are considered as zero. See SPQR's manual.\n"
        },
        {
            "name": "rank",
            "arg_names": [
                "C"
            ],
            "arg_types": [
                "CholeskyPivoted"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the numerical rank of a matrix by counting how many outputs of `svdvals(A)` are greater than `max(atol, rtol*σ₁)` where `σ₁` is `A`'s largest calculated singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `A`, and `ϵ` is the [`eps`](@ref) of the element type of `A`.\n\n!!! note\n    Numerical rank can be a sensitive and imprecise characterization of ill-conditioned matrices with singular values that are close to the threshold tolerance `max(atol, rtol*σ₁)`. In such cases, slight perturbations to the singular-value computation or to the matrix can change the result of `rank` by pushing one or more singular values across the threshold. These variations can even occur due to changes in floating-point errors between different Julia versions, architectures, compilers, or operating systems.\n\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n\n```\nrank(::QRSparse{Tv,Ti}) -> Ti\n```\n\nReturn the rank of the QR factorization\n\n```\nrank(S::SparseMatrixCSC{Tv,Ti}; [tol::Real]) -> Ti\n```\n\nCalculate rank of `S` by calculating its QR factorization. Values smaller than `tol` are considered as zero. See SPQR's manual.\n"
        },
        {
            "name": "rank",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Union{Number, AbstractVector}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the numerical rank of a matrix by counting how many outputs of `svdvals(A)` are greater than `max(atol, rtol*σ₁)` where `σ₁` is `A`'s largest calculated singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `A`, and `ϵ` is the [`eps`](@ref) of the element type of `A`.\n\n!!! note\n    Numerical rank can be a sensitive and imprecise characterization of ill-conditioned matrices with singular values that are close to the threshold tolerance `max(atol, rtol*σ₁)`. In such cases, slight perturbations to the singular-value computation or to the matrix can change the result of `rank` by pushing one or more singular values across the threshold. These variations can even occur due to changes in floating-point errors between different Julia versions, architectures, compilers, or operating systems.\n\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n\n```\nrank(::QRSparse{Tv,Ti}) -> Ti\n```\n\nReturn the rank of the QR factorization\n\n```\nrank(S::SparseMatrixCSC{Tv,Ti}; [tol::Real]) -> Ti\n```\n\nCalculate rank of `S` by calculating its QR factorization. Values smaller than `tol` are considered as zero. See SPQR's manual.\n"
        },
        {
            "name": "rank",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
                "atol",
                "rtol"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrank(A::AbstractMatrix; atol::Real=0, rtol::Real=atol>0 ? 0 : n*ϵ)\nrank(A::AbstractMatrix, rtol::Real)\n```\n\nCompute the numerical rank of a matrix by counting how many outputs of `svdvals(A)` are greater than `max(atol, rtol*σ₁)` where `σ₁` is `A`'s largest calculated singular value. `atol` and `rtol` are the absolute and relative tolerances, respectively. The default relative tolerance is `n*ϵ`, where `n` is the size of the smallest dimension of `A`, and `ϵ` is the [`eps`](@ref) of the element type of `A`.\n\n!!! note\n    Numerical rank can be a sensitive and imprecise characterization of ill-conditioned matrices with singular values that are close to the threshold tolerance `max(atol, rtol*σ₁)`. In such cases, slight perturbations to the singular-value computation or to the matrix can change the result of `rank` by pushing one or more singular values across the threshold. These variations can even occur due to changes in floating-point errors between different Julia versions, architectures, compilers, or operating systems.\n\n\n!!! compat \"Julia 1.1\"\n    The `atol` and `rtol` keyword arguments requires at least Julia 1.1. In Julia 1.0 `rtol` is available as a positional argument, but this will be deprecated in Julia 2.0.\n\n\n# Examples\n\n```jldoctest\njulia> rank(Matrix(I, 3, 3))\n3\n\njulia> rank(diagm(0 => [1, 0, 2]))\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.1)\n2\n\njulia> rank(diagm(0 => [1, 0.001, 2]), rtol=0.00001)\n3\n\njulia> rank(diagm(0 => [1, 0.001, 2]), atol=1.5)\n1\n```\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "AbstractVecOrMat{<:Complex}",
                "Hessenberg{<:Complex, <:Any, <:AbstractMatrix{<:Real}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Hessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Cholesky"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "LowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LU{T, Tridiagonal{T, V}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LU"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "SymTridiagonal"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "F"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UpperHessenberg"
            ],
            "kwarg_names": [
                "shift"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "S"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LDLt{<:Any, <:SymTridiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "X",
                "s"
            ],
            "arg_types": [
                "AbstractArray",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n\n```\nrdiv!(A::AbstractArray, b::Number)\n```\n\nDivide each entry in an array `A` by a scalar `b` overwriting `A` in-place.  Use [`ldiv!`](@ref) to divide scalar from left.\n\n# Examples\n\n```jldoctest\njulia> A = [1.0 2.0; 3.0 4.0]\n2×2 Matrix{Float64}:\n 1.0  2.0\n 3.0  4.0\n\njulia> rdiv!(A, 2.0)\n2×2 Matrix{Float64}:\n 0.5  1.0\n 1.5  2.0\n```\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AdjointFactorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.TransposeFactorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "B",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "CholeskyPivoted"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "Q"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "rdiv!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Union{Adjoint{var\"#s997\", var\"#s128\"}, Transpose{var\"#s997\", var\"#s128\"}} where {var\"#s997\", var\"#s128\"<:Bidiagonal}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrdiv!(A, B)\n```\n\nCompute `A / B` in-place and overwriting `A` to store the result.\n\nThe argument `B` should *not* be a matrix.  Rather, instead of matrices it should be a factorization object (e.g. produced by [`factorize`](@ref) or [`cholesky`](@ref)). The reason for this is that factorization itself is both expensive and typically allocates memory (although it can also be done in-place via, e.g., [`lu!`](@ref)), and performance-critical situations requiring `rdiv!` usually also require fine-grained control over the factorization of `B`.\n\n!!! note\n    Certain structured matrix types, such as `Diagonal` and `UpperTriangular`, are permitted, as these are already in a factorized form\n\n"
        },
        {
            "name": "reflect!",
            "arg_names": [
                "x",
                "y",
                "c",
                "s"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVector",
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nreflect!(x, y, c, s)\n```\n\nOverwrite `x` with `c*x + s*y` and `y` with `conj(s)*x - c*y`. Returns `x` and `y`.\n\n!!! compat \"Julia 1.5\"\n    `reflect!` requires at least Julia 1.5.\n\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "LowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "LowerTriangular",
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "T",
                "D"
            ],
            "arg_types": [
                "Tridiagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "F",
                "x"
            ],
            "arg_types": [
                "Hessenberg{<:Any, <:SymTridiagonal{T, V} where V<:AbstractVector{T}}",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "F",
                "x"
            ],
            "arg_types": [
                "Hessenberg{<:Any, <:UpperHessenberg{T, S} where S<:AbstractMatrix{T}}",
                "T"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "adjQ"
            ],
            "arg_types": [
                "Adjoint{T, <:StridedVecOrMat{T}}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjQ"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjB"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjB"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, true}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, false}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRCompactWYQ{T, M, C} where {M<:AbstractMatrix{T}, C<:AbstractMatrix{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjQ"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.AdjointQ{<:Any, <:LinearAlgebra.QRCompactWYQ{T, M, C} where {M<:AbstractMatrix{T}, C<:AbstractMatrix{T}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "Q"
            ],
            "arg_types": [
                "Adjoint{T, <:StridedVecOrMat{T}}",
                "LinearAlgebra.HessenbergQ{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "R",
                "G"
            ],
            "arg_types": [
                "LinearAlgebra.Rotation",
                "LinearAlgebra.Givens"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "H",
                "x"
            ],
            "arg_types": [
                "UpperHessenberg",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "UpperTriangular",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "UpperTriangular",
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "B",
                "D"
            ],
            "arg_types": [
                "Bidiagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "D"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "G"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.Givens"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "R"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.Rotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.QRPackedQ{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractVector{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "Q"
            ],
            "arg_types": [
                "AbstractVecOrMat",
                "LinearAlgebra.QRPackedQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "c"
            ],
            "arg_types": [
                "Union{LowerTriangular{T, S}, UnitLowerTriangular{T, S}, UnitUpperTriangular{T, S}, UpperTriangular{T, S}} where {T, S}",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "s"
            ],
            "arg_types": [
                "AbstractArray",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A::AbstractArray, b::Number)\n```\n\nScale an array `A` by a scalar `b` overwriting `A` in-place.  Use [`lmul!`](@ref) to multiply scalar from left.  The scaling operation respects the semantics of the multiplication [`*`](@ref) between an element of `A` and `b`.  In particular, this also applies to multiplication involving non-finite numbers such as `NaN` and `±Inf`.\n\n!!! compat \"Julia 1.1\"\n    Prior to Julia 1.1, `NaN` and `±Inf` entries in `A` were treated inconsistently.\n\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> rmul!(A, 2)\n2×2 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> rmul!([NaN], 0.0)\n1-element Vector{Float64}:\n NaN\n```\n\n```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "adjR"
            ],
            "arg_types": [
                "AbstractMatrix",
                "LinearAlgebra.AdjointRotation{<:Any, <:LinearAlgebra.Rotation}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.LQPackedQ{T, S, C} where {S<:AbstractMatrix{T}, C<:AbstractVector{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "Q"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, true}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "X",
                "Q"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.HessenbergQ{T, <:StridedMatrix{T}, <:StridedVector{T}, false}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rmul!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedVecOrMat{T}",
                "LinearAlgebra.QRCompactWYQ{T, var\"#s997\", C} where {var\"#s997\"<:(StridedMatrix{T} where T), C<:AbstractMatrix{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrmul!(A, B)\n```\n\nCalculate the matrix-matrix product $AB$, overwriting `A`, and return the result. Here, `B` must be of special matrix type, like, e.g., [`Diagonal`](@ref), [`UpperTriangular`](@ref) or [`LowerTriangular`](@ref), or of some orthogonal type, see [`QR`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [0 1; 1 0];\n\njulia> B = UpperTriangular([1 2; 0 3]);\n\njulia> rmul!(A, B);\n\njulia> A\n2×2 Matrix{Int64}:\n 0  3\n 1  2\n\njulia> A = [1.0 2.0; 3.0 4.0];\n\njulia> F = qr([0 1; -1 0]);\n\njulia> rmul!(A, F.Q)\n2×2 Matrix{Float64}:\n 2.0  1.0\n 4.0  3.0\n```\n"
        },
        {
            "name": "rotate!",
            "arg_names": [
                "x",
                "y",
                "c",
                "s"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVector",
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nrotate!(x, y, c, s)\n```\n\nOverwrite `x` with `c*x + s*y` and `y` with `-conj(s)*x + c*y`. Returns `x` and `y`.\n\n!!! compat \"Julia 1.5\"\n    `rotate!` requires at least Julia 1.5.\n\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{TA}",
                "AbstractMatrix{TB}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{LowerTriangular{T, S} where S<:AbstractMatrix{T}, UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}, UpperTriangular{T, S} where S<:AbstractMatrix{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperHessenberg{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur(A) -> F::Schur\n```\n\nComputes the Schur factorization of the matrix `A`. The (quasi) triangular Schur factor can be obtained from the `Schur` object `F` with either `F.Schur` or `F.T` and the orthogonal/unitary Schur vectors can be obtained with `F.vectors` or `F.Z` such that `A = F.vectors * F.Schur * F.vectors'`. The eigenvalues of `A` can be obtained with `F.values`.\n\nFor real `A`, the Schur factorization is \"quasitriangular\", which means that it is upper-triangular except with 2×2 diagonal blocks for any conjugate pair of complex eigenvalues; this allows the factorization to be purely real even when there are complex eigenvalues.  To obtain the (complex) purely upper-triangular Schur factorization from a real quasitriangular factorization, you can use `Schur{Complex}(schur(A))`.\n\nIterating the decomposition produces the components `F.T`, `F.Z`, and `F.values`.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> F.vectors * F.Schur * F.vectors'\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> t, z, vals = F; # destructuring via iteration\n\njulia> t == F.T && z == F.Z && vals == F.values\ntrue\n```\n\n```\nschur(A, B) -> F::GeneralizedSchur\n```\n\nComputes the Generalized Schur (or QZ) factorization of the matrices `A` and `B`. The (quasi) triangular Schur factors can be obtained from the `Schur` object `F` with `F.S` and `F.T`, the left unitary/orthogonal Schur vectors can be obtained with `F.left` or `F.Q` and the right unitary/orthogonal Schur vectors can be obtained with `F.right` or `F.Z` such that `A=F.left*F.S*F.right'` and `B=F.left*F.T*F.right'`. The generalized eigenvalues of `A` and `B` can be obtained with `F.α./F.β`.\n\nIterating the decomposition produces the components `F.S`, `F.T`, `F.Q`, `F.Z`, `F.α`, and `F.β`.\n"
        },
        {
            "name": "schur!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur!(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nSame as [`schur`](@ref) but uses the input matrices `A` and `B` as workspace.\n"
        },
        {
            "name": "schur!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperHessenberg{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur!(A) -> F::Schur\n```\n\nSame as [`schur`](@ref) but uses the input argument `A` as workspace.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur!(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> A\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\n```\n\n```\nschur!(A::StridedMatrix, B::StridedMatrix) -> F::GeneralizedSchur\n```\n\nSame as [`schur`](@ref) but uses the input matrices `A` and `B` as workspace.\n"
        },
        {
            "name": "schur!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{var\"#s997\"} where var\"#s997\"<:Union{Float32, Float64, ComplexF64, ComplexF32}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nschur!(A) -> F::Schur\n```\n\nSame as [`schur`](@ref) but uses the input argument `A` as workspace.\n\n# Examples\n\n```jldoctest\njulia> A = [5. 7.; -2. -4.]\n2×2 Matrix{Float64}:\n  5.0   7.0\n -2.0  -4.0\n\njulia> F = schur!(A)\nSchur{Float64, Matrix{Float64}, Vector{Float64}}\nT factor:\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\nZ factor:\n2×2 Matrix{Float64}:\n  0.961524  0.274721\n -0.274721  0.961524\neigenvalues:\n2-element Vector{Float64}:\n  3.0\n -2.0\n\njulia> A\n2×2 Matrix{Float64}:\n 3.0   9.0\n 0.0  -2.0\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Integer"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{TA}",
                "AbstractMatrix{TB}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
                "kw..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
                "full"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat{T}"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat{T}"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\nCompute the singular value decomposition (SVD) of `A` and return an `SVD` object.\n\n`U`, `S`, `V` and `Vt` can be obtained from the factorization `F` with `F.U`, `F.S`, `F.V` and `F.Vt`, such that `A = U * Diagonal(S) * Vt`. The algorithm produces `Vt` and hence `Vt` is more efficient to extract than `V`. The singular values in `S` are sorted in descending order.\n\nIterating the decomposition produces the components `U`, `S`, and `V`.\n\nIf `full = false` (default), a \"thin\" SVD is returned. For an $M \\times N$ matrix `A`, in the full factorization `U` is $M \\times M$ and `V` is $N \\times N$, while in the thin factorization `U` is $M \\times K$ and `V` is $N \\times K$, where $K = \\min(M,N)$ is the number of singular values.\n\nIf `alg = DivideAndConquer()` a divide-and-conquer algorithm is used to calculate the SVD. Another (typically slower but more accurate) option is `alg = QRIteration()`.\n\n!!! compat \"Julia 1.3\"\n    The `alg` keyword argument requires Julia 1.3 or later.\n\n\n# Examples\n\n```jldoctest\njulia> A = rand(4,3);\n\njulia> F = svd(A); # Store the Factorization Object\n\njulia> A ≈ F.U * Diagonal(F.S) * F.Vt\ntrue\n\njulia> U, S, V = F; # destructuring via iteration\n\njulia> A ≈ U * Diagonal(S) * V'\ntrue\n\njulia> Uonly, = svd(A); # Store U only\n\njulia> Uonly == U\ntrue\n```\n\n```\nsvd(A, B) -> GeneralizedSVD\n```\n\nCompute the generalized SVD of `A` and `B`, returning a `GeneralizedSVD` factorization object `F` such that `[A;B] = [F.U * F.D1; F.V * F.D2] * F.R0 * F.Q'`\n\n  * `U` is a M-by-M orthogonal matrix,\n  * `V` is a P-by-P orthogonal matrix,\n  * `Q` is a N-by-N orthogonal matrix,\n  * `D1` is a M-by-(K+L) diagonal matrix with 1s in the first K entries,\n  * `D2` is a P-by-(K+L) matrix whose top right L-by-L block is diagonal,\n  * `R0` is a (K+L)-by-N matrix whose rightmost (K+L)-by-(K+L) block is          nonsingular upper block triangular,\n\n`K+L` is the effective numerical rank of the matrix `[A; B]`.\n\nIterating the decomposition produces the components `U`, `V`, `Q`, `D1`, `D2`, and `R0`.\n\nThe generalized SVD is used in applications such as when one wants to compare how much belongs to `A` vs. how much belongs to `B`, as in human vs yeast genome, or signal vs noise, or between clusters vs within clusters. (See Edelman and Wang for discussion: https://arxiv.org/abs/1901.00485)\n\nIt decomposes `[A; B]` into `[UC; VS]H`, where `[UC; VS]` is a natural orthogonal basis for the column space of `[A; B]`, and `H = RQ'` is a natural non-orthogonal basis for the rowspace of `[A;B]`, where the top rows are most closely attributed to the `A` matrix, and the bottom to the `B` matrix. The multi-cosine/sine matrices `C` and `S` provide a multi-measure of how much `A` vs how much `B`, and `U` and `V` provide directions in which these are measured.\n\n# Examples\n\n```jldoctest\njulia> A = randn(3,2); B=randn(4,2);\n\njulia> F = svd(A, B);\n\njulia> U,V,Q,C,S,R = F;\n\njulia> H = R*Q';\n\njulia> [A; B] ≈ [U*C; V*S]*H\ntrue\n\njulia> [A; B] ≈ [F.U*F.D1; F.V*F.D2]*F.R0*F.Q'\ntrue\n\njulia> Uonly, = svd(A,B);\n\njulia> U == Uonly\ntrue\n```\n"
        },
        {
            "name": "svd!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{var\"#s125\", V} where {var\"#s125\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s125\"}}"
            ],
            "kwarg_names": [
                "full"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n\n```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
        },
        {
            "name": "svd!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
        },
        {
            "name": "svd!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedVector{T}"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n\n```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
        },
        {
            "name": "svd!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
                "full",
                "alg"
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n"
        },
        {
            "name": "svd!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvd!(A; full::Bool = false, alg::Algorithm = default_svd_alg(A)) -> SVD\n```\n\n`svd!` is the same as [`svd`](@ref), but saves space by overwriting the input `A`, instead of creating a copy. See documentation of [`svd`](@ref) for details.\n\n```\nsvd!(A, B) -> GeneralizedSVD\n```\n\n`svd!` is the same as [`svd`](@ref), but modifies the arguments `A` and `B` in-place, instead of making copies. See documentation of [`svd`](@ref) for details.\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SVD{var\"#s997\", T, M, C} where {var\"#s997\", M<:(AbstractArray{var\"#s997\"}), C<:AbstractVector{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractTriangular"
            ],
            "kwarg_names": [
                "kwargs..."
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A)\n```\n\nReturn the singular values of `A` in descending order.\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0. 0. 0. 2.; 0. 0. 3. 0. 0.; 0. 0. 0. 0. 0.; 0. 2. 0. 0. 0.]\n4×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  2.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  2.0  0.0  0.0  0.0\n\njulia> svdvals(A)\n4-element Vector{Float64}:\n 3.0\n 2.23606797749979\n 2.0\n 0.0\n```\n\n```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix{TA}",
                "AbstractMatrix{TB}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`. See also [`svd`](@ref).\n\n# Examples\n\n```jldoctest\njulia> A = [1. 0.; 0. -1.]\n2×2 Matrix{Float64}:\n 1.0   0.0\n 0.0  -1.0\n\njulia> B = [0. 1.; 1. 0.]\n2×2 Matrix{Float64}:\n 0.0  1.0\n 1.0  0.0\n\njulia> svdvals(A, B)\n2-element Vector{Float64}:\n 1.0\n 1.0\n```\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{var\"#s997\", V} where {var\"#s997\"<:Union{Float32, Float64}, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref).\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref).\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "StridedMatrix{T}",
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref).\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "StridedMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref).\n"
        },
        {
            "name": "svdvals!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsvdvals!(A)\n```\n\nReturn the singular values of `A`, saving space by overwriting the input. See also [`svdvals`](@ref) and [`svd`](@ref).\n\n```\nsvdvals!(A, B)\n```\n\nReturn the generalized singular values from the generalized singular value decomposition of `A` and `B`, saving space by overwriting `A` and `B`. See also [`svd`](@ref) and [`svdvals`](@ref).\n"
        },
        {
            "name": "sylvester",
            "arg_names": [
                "a",
                "b",
                "c"
            ],
            "arg_types": [
                "Union{Real, Complex}",
                "Union{Real, Complex}",
                "Union{Real, Complex}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n2×2 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n2×2 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B ≈ -C\ntrue\n```\n"
        },
        {
            "name": "sylvester",
            "arg_names": [
                "A",
                "B",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix{T}",
                "AbstractMatrix{T}",
                "AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n2×2 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n2×2 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B ≈ -C\ntrue\n```\n"
        },
        {
            "name": "sylvester",
            "arg_names": [
                "A",
                "B",
                "C"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\nsylvester(A, B, C)\n```\n\nComputes the solution `X` to the Sylvester equation `AX + XB + C = 0`, where `A`, `B` and `C` have compatible dimensions and `A` and `-B` have no eigenvalues with equal real part.\n\n# Examples\n\n```jldoctest\njulia> A = [3. 4.; 5. 6]\n2×2 Matrix{Float64}:\n 3.0  4.0\n 5.0  6.0\n\njulia> B = [1. 1.; 1. 2.]\n2×2 Matrix{Float64}:\n 1.0  1.0\n 1.0  2.0\n\njulia> C = [1. 2.; -2. 1]\n2×2 Matrix{Float64}:\n  1.0  2.0\n -2.0  1.0\n\njulia> X = sylvester(A, B, C)\n2×2 Matrix{Float64}:\n -4.46667   1.93333\n  3.73333  -1.8\n\njulia> A*X + X*B ≈ -C\ntrue\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Number, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Matrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric{var\"#s997\", S} where {var\"#s997\"<:Number, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "x"
            ],
            "arg_types": [
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "tr",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntr(M)\n```\n\nMatrix trace. Sums the diagonal elements of `M`.\n\n# Examples\n\n```jldoctest\njulia> A = [1 2; 3 4]\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> tr(A)\n5\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Real, S<:(AbstractMatrix{<:var\"#s997\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "Tridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LU{var\"#s997\", S} where {var\"#s997\"<:Real, S<:AbstractMatrix{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LinearAlgebra.AdjointFactorization{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Hessenberg{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "Hessenberg"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LQ{var\"#s997\", S, C} where {var\"#s997\"<:Real, S<:AbstractMatrix{var\"#s997\"}, C<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "LQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "LinearAlgebra.TransposeFactorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Union{QR{var\"#s997\", S, C} where {var\"#s997\"<:Real, S<:AbstractMatrix{var\"#s997\"}, C<:AbstractVector{var\"#s997\"}}, LinearAlgebra.QRCompactWY{var\"#s127\", M, C} where {var\"#s127\"<:Real, M<:AbstractMatrix{var\"#s127\"}, C<:AbstractMatrix{var\"#s127\"}}, QRPivoted{var\"#s128\", S, C} where {var\"#s128\"<:Real, S<:AbstractMatrix{var\"#s128\"}, C<:AbstractVector{var\"#s128\"}}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                ""
            ],
            "arg_types": [
                "Union{QR, LinearAlgebra.QRCompactWY, QRPivoted}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Factorization{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "F"
            ],
            "arg_types": [
                "Factorization"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n\n```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "Q"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractQ"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n\n```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Adjoint{<:Real}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "R"
            ],
            "arg_types": [
                "LinearAlgebra.AbstractRotation"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n\n```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "J"
            ],
            "arg_types": [
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n\n```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "S"
            ],
            "arg_types": [
                "SymTridiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Union{BitMatrix, BitVector}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Bidiagonal{var\"#s997\", V} where {var\"#s997\"<:Number, V<:AbstractVector{var\"#s997\"}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "Bidiagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "AbstractVecOrMat"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n"
        },
        {
            "name": "transpose",
            "arg_names": [
                "a"
            ],
            "arg_types": [
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose(A)\n```\n\nLazy transpose. Mutating the returned object should appropriately mutate `A`. Often, but not always, yields `Transpose(A)`, where `Transpose` is a lazy transpose wrapper. Note that this operation is recursive.\n\nThis operation is intended for linear algebra usage - for general data manipulation see [`permutedims`](@ref Base.permutedims), which is non-recursive.\n\n# Examples\n\n```jldoctest\njulia> A = [3 2; 0 0]\n2×2 Matrix{Int64}:\n 3  2\n 0  0\n\njulia> B = transpose(A)\n2×2 transpose(::Matrix{Int64}) with eltype Int64:\n 3  0\n 2  0\n\njulia> B isa Transpose\ntrue\n\njulia> transpose(B) === A # the transpose of a transpose unwraps the parent\ntrue\n\njulia> Transpose(B) # however, the constructor always wraps its argument\n2×2 transpose(transpose(::Matrix{Int64})) with eltype Int64:\n 3  2\n 0  0\n\njulia> B[1,2] = 4; # modifying B will modify A automatically\n\njulia> A\n2×2 Matrix{Int64}:\n 3  2\n 4  0\n```\n\nFor complex matrices, the `adjoint` operation is equivalent to a conjugate-transpose.\n\n```jldoctest\njulia> A = reshape([Complex(x, x) for x in 1:4], 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 1+1im  3+3im\n 2+2im  4+4im\n\njulia> adjoint(A) == conj(transpose(A))\ntrue\n```\n\nThe `transpose` of an `AbstractVector` is a row-vector:\n\n```jldoctest\njulia> v = [1,2,3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> transpose(v) # returns a row-vector\n1×3 transpose(::Vector{Int64}) with eltype Int64:\n 1  2  3\n\njulia> transpose(v) * v # compute the dot product\n14\n```\n\nFor a matrix of matrices, the individual blocks are recursively operated on:\n\n```jldoctest\njulia> C = [1 3; 2 4]\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\njulia> D = reshape([C, 2C, 3C, 4C], 2, 2) # construct a block matrix\n2×2 Matrix{Matrix{Int64}}:\n [1 3; 2 4]  [3 9; 6 12]\n [2 6; 4 8]  [4 12; 8 16]\n\njulia> transpose(D) # blocks are recursively transposed\n2×2 transpose(::Matrix{Matrix{Int64}}) with eltype Transpose{Int64, Matrix{Int64}}:\n [1 2; 3 4]   [2 4; 6 8]\n [3 6; 9 12]  [4 8; 12 16]\n```\n\n```\ntranspose(F::Factorization)\n```\n\nLazy transpose of the factorization `F`. By default, returns a [`TransposeFactorization`](@ref), except for `Factorization`s with real `eltype`, in which case returns an [`AdjointFactorization`](@ref).\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "C",
                "B"
            ],
            "arg_types": [
                "BitMatrix",
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n\n```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "B",
                "A"
            ],
            "arg_types": [
                "AbstractMatrix",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(dest,src)\n```\n\nTranspose array `src` and store the result in the preallocated array `dest`, which should have a size corresponding to `(size(src,2),size(src,1))`. No in-place transposition is supported and unexpected results will happen if `src` and `dest` have overlapping memory regions.\n\n# Examples\n\n```jldoctest\njulia> A = [3+2im 9+2im; 8+7im  4+6im]\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n\njulia> B = zeros(Complex{Int64}, 2, 2)\n2×2 Matrix{Complex{Int64}}:\n 0+0im  0+0im\n 0+0im  0+0im\n\njulia> transpose!(B, A);\n\njulia> B\n2×2 Matrix{Complex{Int64}}:\n 3+2im  8+7im\n 9+2im  4+6im\n\njulia> A\n2×2 Matrix{Complex{Int64}}:\n 3+2im  9+2im\n 8+7im  4+6im\n```\n\n```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "transpose!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntranspose!(X::AbstractSparseMatrixCSC{Tv,Ti}, A::AbstractSparseMatrixCSC{Tv,Ti}) where {Tv,Ti}\n```\n\nTranspose the matrix `A` and stores it in the matrix `X`. `size(X)` must be equal to `size(transpose(A))`. No additional memory is allocated other than resizing the rowval and nzval of `X`, if needed.\n\nSee `halfperm!`\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n4×4 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "B",
                "k"
            ],
            "arg_types": [
                "BitMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n4×4 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Hermitian",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Matrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n4×4 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M)\n```\n\nLower triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a)\n4×4 Matrix{Float64}:\n 1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Symmetric",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
        },
        {
            "name": "tril",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> tril(a,-3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "D",
                "k"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UnitLowerTriangular",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UpperTriangular{T, S} where S<:AbstractMatrix{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M)\n```\n\nLower triangle of a matrix, overwriting `M` in the process. See also [`tril`](@ref).\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "LowerTriangular",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "tril!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntril!(M, k::Integer)\n```\n\nReturn the lower triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> tril!(M, 2)\n5×5 Matrix{Int64}:\n 1  2  3  0  0\n 1  2  3  4  0\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "B"
            ],
            "arg_types": [
                "BitMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "B",
                "k"
            ],
            "arg_types": [
                "BitMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Hermitian"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Hermitian",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Matrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M)\n```\n\nUpper triangle of a matrix.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "Symmetric",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "triu",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal.\n\n# Examples\n\n```jldoctest\njulia> a = fill(1.0, (4,4))\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n\njulia> triu(a,3)\n4×4 Matrix{Float64}:\n 0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0\n\njulia> triu(a,-3)\n4×4 Matrix{Float64}:\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Bidiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "Tridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "D",
                "k"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "D"
            ],
            "arg_types": [
                "Diagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitLowerTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "SymTridiagonal{T, V} where V<:AbstractVector{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UnitUpperTriangular",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UnitUpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "UpperTriangular",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "UpperTriangular"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A"
            ],
            "arg_types": [
                "LowerTriangular{T, S} where S<:AbstractMatrix{T}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M"
            ],
            "arg_types": [
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M)\n```\n\nUpper triangle of a matrix, overwriting `M` in the process. See also [`triu`](@ref).\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "A",
                "k"
            ],
            "arg_types": [
                "LowerTriangular{T, S} where S<:AbstractMatrix{T}",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "triu!",
            "arg_names": [
                "M",
                "k"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Integer"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ntriu!(M, k::Integer)\n```\n\nReturn the upper triangle of `M` starting from the `k`th superdiagonal, overwriting `M` in the process.\n\n# Examples\n\n```jldoctest\njulia> M = [1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5; 1 2 3 4 5]\n5×5 Matrix{Int64}:\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n 1  2  3  4  5\n\njulia> triu!(M, 1)\n5×5 Matrix{Int64}:\n 0  2  3  4  5\n 0  0  3  4  5\n 0  0  0  4  5\n 0  0  0  0  5\n 0  0  0  0  0\n```\n"
        },
        {
            "name": "cross",
            "arg_names": [
                "a",
                "b"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ncross(x, y)\n×(x,y)\n```\n\nCompute the cross product of two 3-vectors.\n\n# Examples\n\n```jldoctest\njulia> a = [0;1;0]\n3-element Vector{Int64}:\n 0\n 1\n 0\n\njulia> b = [0;0;1]\n3-element Vector{Int64}:\n 0\n 0\n 1\n\njulia> cross(a,b)\n3-element Vector{Int64}:\n 1\n 0\n 0\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "BitVector",
                "BitVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Hermitian{var\"#s997\", S} where {var\"#s997\"<:Union{Real, Complex}, S<:(AbstractMatrix{<:var\"#s997\"})}",
                "Hermitian{var\"#s128\", S} where {var\"#s128\"<:Union{Real, Complex}, S<:(AbstractMatrix{<:var\"#s128\"})}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Adjoint{<:Union{Real, Complex}}",
                "Adjoint{<:Union{Real, Complex}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "Vector{T}",
                "AbstractRange{TI}",
                "Vector{T}",
                "AbstractRange{TI}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "rx",
                "y",
                "ry"
            ],
            "arg_types": [
                "Vector{T}",
                "AbstractRange{TI}",
                "Vector{T}",
                "AbstractRange{TI}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "D",
                "B"
            ],
            "arg_types": [
                "Diagonal",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "AbstractMatrix",
                "Diagonal"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "J",
                "A"
            ],
            "arg_types": [
                "UniformScaling",
                "AbstractMatrix"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "B"
            ],
            "arg_types": [
                "Symmetric",
                "Symmetric"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Transpose",
                "Transpose"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Number",
                "Number"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "B",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Bidiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Tridiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "adjA",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Adjoint",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "D",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Diagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "J",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UniformScaling",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "A",
                "J"
            ],
            "arg_types": [
                "AbstractMatrix",
                "UniformScaling"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "S",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "SymTridiagonal",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UnitLowerTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "transA",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Transpose{<:Real}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "H",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UpperHessenberg",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "a",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Real, Complex}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "a",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Number",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UnitUpperTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "UpperTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "LowerTriangular",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Hermitian{var\"#s997\", var\"#s128\"}, Symmetric{var\"#s997\", var\"#s128\"}, Symmetric{Complex{var\"#s997\"}, var\"#s128\"}} where {var\"#s997\"<:Real, var\"#s128\"<:Diagonal}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "Union{Hermitian{T, S}, Hermitian{Complex{T}, S}, Symmetric{T, S}} where {T<:Real, S}",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "AbstractVector",
                "AbstractMatrix",
                "AbstractVector"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}",
                "Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, SubArray{T, var\"#s997\", var\"#s128\", I, true} where {var\"#s997\", var\"#s128\"<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, Base.ReshapedArray{T, N, A} where {N, A<:Union{Base.ReinterpretArray{T, N, S, A, IsReshaped} where {T, N, A<:Union{SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}, IsReshaped, S}, SubArray{T, N, A, I, true} where {T, N, A<:DenseArray, I<:Union{Tuple{Vararg{Real}}, Tuple{AbstractUnitRange, Vararg{Any}}}}, DenseArray}}, DenseArray{T}}, I}, DenseArray{T}}"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "AbstractArray",
                "AbstractArray"
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "y"
            ],
            "arg_types": [
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, y)\nx ⋅ y\n```\n\nCompute the dot product between two vectors. For complex vectors, the first vector is conjugated.\n\n`dot` also works on arbitrary iterable objects, including arrays of any dimension, as long as `dot` is defined on the elements.\n\n`dot` is semantically equivalent to `sum(dot(vx,vy) for (vx,vy) in zip(x, y))`, with the added restriction that the arguments must have equal lengths.\n\n`x ⋅ y` (where `⋅` can be typed by tab-completing `\\cdot` in the REPL) is a synonym for `dot(x, y)`.\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [2; 3])\n5\n\njulia> dot([im; im], [1; 1])\n0 - 2im\n\njulia> dot(1:5, 2:6)\n70\n\njulia> x = fill(2., (5,5));\n\njulia> y = fill(3., (5,5));\n\njulia> dot(x, y)\n150.0\n```\n\n```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        },
        {
            "name": "dot",
            "arg_names": [
                "x",
                "A",
                "y"
            ],
            "arg_types": [
                "",
                "",
                ""
            ],
            "kwarg_names": [
            ],
            "module": "LinearAlgebra",
            "doc": "```\ndot(x, A, y)\n```\n\nCompute the generalized dot product `dot(x, A*y)` between two vectors `x` and `y`, without storing the intermediate result of `A*y`. As for the two-argument [`dot(_,_)`](@ref), this acts recursively. Moreover, for complex vectors, the first vector is conjugated.\n\n!!! compat \"Julia 1.4\"\n    Three-argument `dot` requires at least Julia 1.4.\n\n\n# Examples\n\n```jldoctest\njulia> dot([1; 1], [1 2; 3 4], [2; 3])\n26\n\njulia> dot(1:5, reshape(1:25, 5, 5), 2:6)\n4850\n\njulia> ⋅(1:5, reshape(1:25, 5, 5), 2:6) == dot(1:5, reshape(1:25, 5, 5), 2:6)\ntrue\n```\n"
        }
    ]
}